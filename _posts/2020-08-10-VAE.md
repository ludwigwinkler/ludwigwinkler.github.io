---
layout: post
title:  "Variational Autoencoders Derived"
date:   2020-08-10
excerpt: "How to get to the objective function of VAEs ... "
image:
---
<head>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<!-- ### Variational Autoencoders -->

Let $x$ denote a random variable which is generated by a random process.
This random process first samples a random latent variable $z$ and subsequently generates $x|z \sim p(x|z)$ by conditioning the random process $p(x|z)$ on the random variable $z$.
Thus we are dealing with a generative model which can generate valid samples $x$ from some random $z$.
We are interested in the latent distribution of $p(z\||x)$ given the data $x$ and wish to learn it.
Intuitively, we want to know for a given $x$ what the latent variables $z$ were that generated them.
This is akin to observing some observation $x$ and being able to say: I know the $z$ 's that generated that!.

By Bayes rule we now that for a distribution $ p ( x \| z ) $ there also exists the distribution $ p ( z \| x ) $, the distribution we are interested in.

$$
\begin{align}
p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}
\end{align}
$$

The crux of the problem is that we can only observe the data distribution $p(x)$ through a data set $\mathcal{D}= \\{ x_i \\}_{i=0}^N$.
So we neither know what form the data generating process $p(x|z)$ has nor what the true latent distribution $p(z)$ is.
Additionally, the data probability $p(x)$ is even more obscure.
How would you even answer the question of how probable your data set is?

What we do know is the following: We want to find a variational distribution, let's name it $q_\phi(z \|x)$ with the optimizable parameters $\phi$, which we want to be as close as possible to the true distribution $p(z \|x)$.
The motivation behind this formulation is that the true latent conditional distribution $p(z|x)$ could be very complicated, but we will choose a simpler variational distribution $q_\phi(z \|x)$ that we can conveniently work with.
It might not be able to represent all the modes and fat tails that could potentially occur in $p(z \|x)$ but better than nothing, right?

Information theory gives us the right tools to measure the difference between $q_\phi(z|x)$ and $p(z|x)$ through the Kullback-Leibler divergence:
$$
	\mathbb{KL} \left[ q_\phi(z|x) \ || \ p(z|x) \right]
$$
The state of affairs sofar is that we have an easy to work with distribution $q_\phi(z|x)$ with the trainable parameters $\phi$ and that we wish to minimize the divergence to the true latent distribution $p(z|x)$.
We can also rewrite $p(z|x)$ according to Bayes rule to maybe make the computations a bit more tractable.
We can now write out the Kullback-Leibler divergence and inspect the terms that arise from some algebraic manipulation:

$$
\begin{align}
	\mathbb{KL} \left[ q_\phi(z \| x) \ || \ p(z \| x) \right] &= \mathbb{E}_{q_\phi(z \|x)} \left[ \log \frac{q_\phi(z \| x)}{p(z \| x)} \right] \\
	&= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z|x)\right] \\
	&= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log \frac{p(z,x)}{p(x)}\right] \\
	&= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z,x) + \log p(x)\right]
\end{align}
$$
From earlier we know, that the data marginal probability $p(x)$ is almost surely intractable so we might want to avoid working with it directly.
But by applying Bayes' rule we suddenly see that we are working with the joint probability $p(z,x)$.
Given the fact that $0 \leq \mathbb{KL}$ we can deduce
$$
\begin{align}
	0 &\leq \mathbb{KL} \left[ q_\phi(z|x) \ || \ p(z|x) \right] \\
	0 &\leq \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z,x) + \log p(x) \right] \\
	0 &\geq \mathbb{E}_{q_\phi(z|x)} \left[ -\log q_\phi(z|x) + \log p(z,x) - \log p(x) \right] \\
	\log p(x) &\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(z,x)\right] \\
	\log p(x) &\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(x|z) p(z)\right] \\
	\log p(x) &\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(x|z) + \log p(z)\right] \\
	\log p(x) &\geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(z)}{q_\phi(z|x)} + \log p(x|z) \right] \\
	\log p(x) &\geq -\mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{q_\phi(z|x)}{p(z)} \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p(x | z) \right] \\
	\log p(x) &\geq -\mathbb{KL} \left[ q_\phi(z|x) || p(z) \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p(x|z) \right]
\end{align}
$$
What does the inequality above tell us?
It says that if we want to maximize the probability of the data we must minimize the KL divergence in the first term and maximize the probability of the generative model $p(x|z)$.
So for any given $z$, we want the generative model $p(x|z)$.
If we optimize the two terms on the right, we will obtain an inference model $q_\phi(z|x)$ which inverts the generative model $p(x|z)$.

The problem, though, is that we have no clue what either $p(z)$ nor the true generative model $p(x|z)$ actually is.
Here comes the fun part: Let's just assume stuff and parameterize both $p(z)$ and $p(x|z)$ such that we can easily and conveniently work with them.
Since $p(z)$ is a latent distribution we will enforce a strong simplicity by assuming that it follows a standard normal distribution $\mathcal{N}(0, I)$.
We could assume any other family of distributions but the standard normal distribution has lots of nice perks and properties.
This might seem bold but if the generative model $p(x|z)$ is flexible enough it can generate any $x$ from this comparatively simpel $z$.
Now let's turn our attention to $p(x|z)$: We will change the unknown $p(x|z)$ to a parameterized and differentiable $p_\theta(x|z)$ such that we can maximize the probability of the data $x$ for a given $z$.

Now we have the following objective function:

$$
\begin{align}
	\log p(x) \geq -\mathbb{KL} \left[ q_\phi(z|x) || p(z) \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z) \right]
\end{align}
$$

which, upon closer inspection, has a lot of similarities to an autoencoder, except that it's probabilistic!
We use the distribution $q_\phi(z|x)$ to infer some latent code from a given sample.
Through the KL divergence we enforce that the latent representation should be close to the simplified assumption of $p(z) = \mathcal{N}(0,I)$.
The same latent code $z$ should be reconstructed to the true sample $x$ by the generative model $p_\theta(x|z)$.
So we can actually interpret $q_\phi(z|x)$ as a probabilistic encoder and $p_\theta(x|z)$ as a decoder.

It is important to note that the prior and data loglikelihood are not balanced with respect to the data set size as it is done in Bayesian neural network and the parameter prior.
The KL divergence between the latent code $q_\phi(z|x)$ and the prior $p_\theta(z)$ is computed for each data point independently and is equally balanced.
