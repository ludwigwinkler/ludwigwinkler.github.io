---
layout: post
title:  "Annealed Importance Sampling"
category: blog
date:   2025-04-12
excerpt: "... it's giving off the same energy!"
# highlighter: rouge
# image: "/blog/ItoDensityEstimator.png"
---
<head>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
          TeX: {
                equationNumbers: { autoNumber: "all" },
                extensions: ["AMSmath.js", "AMSsymbols.js", "cancel.js"]
            },
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

### Energy

In the context of statistical mechanics, the energy of a system is a measure of its state. In the context of machine learning, we can think of the energy as a measure of how well a model fits the data. The lower the energy, the better the fit.

- absolute energy doesn't matter, only relative energy matters
- potential energy defined with reference to earth surface, just add six feet if you're dead
- we can always add +/-c to the energy of a system, it doesn't change the physics
- plot where two energy functions with different heights result in the same probability distribution
- why not work with the energy directly? after all lower is better, right?
  - because absolute values of energy without an anchor are meaningless, but a probability of 0.000001 or 0.98 is meaningful
  - I give you energy values of 100, 200, 300, and 400, you have no idea what they mean, but I give you probabilities of 0.01, 0.02, 0.03, and 0.04, you know exactly what they mean
  - because we want to work with probabilities, not energies
  - we can convert energy to probability using the Boltzmann distribution
- Boltzmann distribution: $P(x) = \frac{e^{-E(x)}}{Z}$, where $Z$ is the partition function
- in ML, this occurs in logsumexp and softmax, where we can add a constant to the log likelihood without changing the model, used to avoid numerical issues 

### MCMC Sampling

- acceptance rate proof
- show why we can sample along unnormalized distributions as the partitioning function cancels out

Detailed Balance
$$
\begin{align}
\pi(x) q(x'|x) = \pi(x')q(x|x')
\end{align}
$$

Acceptance Ratios
$$
\begin{align}
a(x'|x) &= \min\left[1, \frac{q(x|x')\pi(x')}{q(x'|x)\pi(x)}\right] \\
a(x|x') &= \min\left[1, \frac{q(x'|x)\pi(x)}{q(x|x')\pi(x')}\right]
\end{align}
$$

Multiply detailed balance equation with other factors should preserve DB
$$
\begin{align}
\pi(x) q(x'|x) a(x'|x) &= \pi(x')q(x|x') a(x|x') \\
\pi(x) q(x'|x) \min\left[1, \frac{q(x|x')\pi(x')}{q(x'|x)\pi(x)}\right]
&= \pi(x')q(x|x') \min\left[1, \frac{q(x'|x)\pi(x)}{q(x|x')\pi(x')}\right] \\
 \min\big[\pi(x) q(x'|x), q(x|x')\pi(x')\big] 
&=  \min\big[\pi(x')q(x|x'), q(x'|x)\pi(x)\big] \\
\end{align}
$$


### Importance Sampling

how to estimate $Z$ with importance sampling
$$
\begin{align}
\pi(x) &= \frac{1}{Z} e^{-E(x)} \\
&= \frac{1}{Z} \tilde{\pi}(x) \\
\end{align}
$$
$$
\begin{align}
Z 
&= \int e^{-E(x)}dx \\
&= \int \frac{q(x)}{q(x)}e^{-E(x)}dx \\
&= \int \frac{e^{-E(x)}}{q(x)} q(x)dx \\
&= \int \frac{e^{-E(x)}}{q(x)} dQ(x) \\
&=\mathbb{E}_{x\sim q}\left[ \frac{e^{-E(x)}}{q(x)} \right]
\end{align}
$$

### Annealed Importance Sampling


$$
\begin{align}
\tilde{\pi}_t(x) = \tilde{\pi}_0(x)^{(1-t)} \tilde{\pi}(x)^t \
\end{align}
$$

discretize to $t\in \{0, 0.33, 0.66, 1 \}$.

As the initial distribution $\pi_0$ we can take a standard normal distribution with a large standard deviation $\sigma >>1$
$$
\begin{align}
\pi_0(x) = \mathcal{N}(x; \mu, \sigma^2) = \underbrace{\frac{1}{\sqrt{2\pi \sigma^2}}}_{=Z_0} \overbrace{e^{-\frac{1}{2\sigma^2}(x-\mu)^2}}^{\tilde{\pi}_0}
\end{align}
$$

$$
\begin{align}
Z_{0.33}
&= \int \tilde{\pi}_{0.33} \ dx \\
&= \int  \frac{\pi_{0}}{\pi_{0}}\tilde{\pi}_{0.33} \ dx \\ 
&= \int  \frac{\tilde{\pi}_{0.33}}{\pi_{0}} \ d\pi_{0} \\
&= \mathbb{E}_{x_0 \sim \pi_0} \left[ \frac{\tilde{\pi}_{0.33}}{\pi_{0}} \right] \\
&= \mathbb{E}_{x_0 \sim \pi_0} \left[ \frac{\tilde{\pi}_{0.33}}{\frac{1}{Z_0}\tilde{\pi}_{0}} \right] \\
&= Z_0 \ \mathbb{E}_{x_0 \sim \pi_0} \left[ \frac{\tilde{\pi}_{0.33}}{\tilde{\pi}_{0}} \right] \\
& \downarrow\\
\pi_{0.33}(x)
&= \frac{1}{Z_{0.33}}\tilde{\pi}_{0.33}(x)
\end{align}
$$

proceed to next stage
$$
\begin{align}
\pi_{0.66}(x)
&= \frac{1}{Z_{0.66}}\tilde{\pi}_{0.66}(x)
\end{align}
$$

we repeat the whole procedure
$$
\begin{align}
Z_{0.66}(x)
&= \int \tilde{\pi}_{0.66} \ dx \\
&= \mathbb{E}_{x \sim \pi_{0.33}} \left[ \frac{\tilde{\pi}_{0.66}}{\pi_{0.33}} \right] \\
&= \mathbb{E}_{x \sim \pi_{0.33}} \left[ \frac{\tilde{\pi}_{0.66}}{\frac{1}{Z_{0.33}}\tilde{\pi}_{0.33}} \right] \\
&= Z_{0.33} \ \mathbb{E}_{x \sim \pi_{0.33}} \left[ \frac{\tilde{\pi}_{0.66}}{\tilde{\pi}_{0.33}} \right] \\
&= Z_0 \ \mathbb{E}_{x_0 \sim \pi_0} \left[ \frac{\tilde{\pi}_{0.33}}{\pi_{0}} \right] \mathbb{E}_{x \sim \pi_{0.33}} \left[ \frac{\tilde{\pi}_{0.66}}{\tilde{\pi}_{0.33}} \right] \\
\end{align}
$$

$$
\begin{align}
Z_{1}(x)
&= \int \tilde{\pi}_{1} \ dx \\
&= \mathbb{E}_{x \sim \pi_{0.66}} \left[ \frac{\tilde{\pi}_{1}}{\pi_{0.66}} \right] \\
&= \mathbb{E}_{x \sim \pi_{0.66}} \left[ \frac{\tilde{\pi}_{1}}{\frac{1}{Z_{0.66}}\tilde{\pi}_{0.66}} \right] \\
&= Z_{0.66} \ \mathbb{E}_{x \sim \pi_{0.66}} \left[ \frac{\tilde{\pi}_{1}}{\tilde{\pi}_{0.66}} \right] \\
&= Z_0 \ \mathbb{E}_{x_0 \sim \pi_0} \left[ \frac{\tilde{\pi}_{0.33}}{\pi_{0}} \right] \mathbb{E}_{x \sim \pi_{0.33}} \left[ \frac{\tilde{\pi}_{0.66}}{\tilde{\pi}_{0.33}} \right]  \ \mathbb{E}_{x \sim \pi_{0.66}} \left[ \frac{\tilde{\pi}_{1}}{\tilde{\pi}_{0.66}} \right] \\
\end{align}
$$

Under ideal circumstances, we would be able to sample from the intermediate distribution $\pi_t$, because we would know them with a magic oracle.
Unfortunately, we we will have to draw the samples which we use in the importance sampling somehow.

### Sequential Monte Carlo