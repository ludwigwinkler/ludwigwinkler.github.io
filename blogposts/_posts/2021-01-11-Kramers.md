---
layout: post
title:  "Fokker, Planck & Kolmogorov"
date:   2021-02-04
excerpt: "Distributions as partial differential equations over time "
image:
---
<head>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

At the core of the partial differential equations that will describe the change of a distribution both forward and backward in time lies the Chapman-Kolmogorov equation
$$
\begin{align}
	p(x_{t + \tau}) & = \int p(x_{t + \tau} , x'_{t}) \ dx'_t \\
	& = \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t
\end{align}
$$
which simply expands over an auxiliary variable $x'_t$ while simultaneously marginalizing it out and factorizing the joint distribution $$p(x_{t+\tau}, x'_t)$$ into a conditional distribution.
The conditional distribution above states that we can start from any $$x'_t$$ and by moving to $$x_{t+\tau}$$ with the right transition probability $$ p(x_{t + \tau} | x'_{t})$$ we will obtain the correct marginal distribution $$p(x_{t+\tau})$$.

We will assume a stochastic differential equation the first two orders of which can be estimated with
$$
\begin{align}
	M^{(n)} (x'_t) = \int (x_{t+\tau} - x'_t)^n  p(x_{t + \tau} | x'_t) dx_{t+\tau}
\end{align}
$$
such that the dynamics are described by the Ito drift-diffusion process
$$
\begin{align}
	dX'_t = & M^{(1)}(X'_t) dt + M^{(2)}(X'_t) dW_t \\
	= & \mu(X'_t, t) dt + \sigma(X'_t, t) dW_t \\
\end{align}
$$
with the Wiener process $$W_t$$.
The important part is to note the 'direction' of the differential which is evaluated strictly forward in time.
We take $$ x'_t $$ as a sort of origin point which doesn't change and weight the differential $$x_{t+\tau} - x'_t $$ by the appropriate transition probability $$ p(x_{t+\tau} | x'_t) $$ over every possible $$ x_{t+\tau} $$.

### Forward Equation

The Chapman-Kolmogorov equation fro the forward Kramers-Moyal expansion can be rewritten with the help of an auxilliary variable as
$$
\begin{align}
	p(x_{t + \tau}) = & \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t \\
	= & \int_{X'} \int_{Y} \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} p(x'_t) dx'_t
\end{align}
$$

The main component of the Kramers-Moyal expansions is the use of the Taylor expansion on a shifted function.
The classical Taylor expansion $T_{f,a}(x)$ of a function $f(x)$ around a root point $a$ says that we can reconstruct the function $f(x)$ with an infinite sum consisting of its derivatives

$$
\begin{align}
	T_{f,a}(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n
\end{align}
$$

What happens if we introduce an offset $h$ to the root point $a$ which can take on any value we want?
It turns out that the arbitrary offset $h$ can directly be used as a distance measure to the root point.
The Taylor expansion of the shifted function $f(a + h)$ then becomes

$$
\begin{align}
	T_{f,a}(h) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} h^n
\end{align}
$$

We can then first expand the delta function $\delta ( y_{t+\tau} - x_{t+\tau} )$ with $\pm x_t$ and subsequently expand the Taylor series to obtain
$$
\begin{align}
	\delta(y_{t+\tau} - x_{t+\tau}) = & \delta(\overbrace{y_{t+\tau} - x'_t}^{h} + \overbrace{x'_t - x_{t+\tau}}^{a} ) \\
	= & \sum_{n=0}^\infty \frac{1}{n!} \underbrace{\partial_{x'_t}^{n} \delta(x'_t - x_{t+\tau}) }_{f^{(n)}(a)} \underbrace{( y_{t+\tau} - x'_t)^n}_{h^n}
\end{align}
$$

We can plug the expanded Taylor series back in to get
$$
\begin{align}
p(x_{t + \tau}) 
= & \int_{X'} \int_{Y} \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
= & \int_{X'} \int_{Y} \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \underbrace{ \partial_{x'_t}^{n} \ \delta(x'_t - x_{t+\tau})}_{!} p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) \underbrace{dx'_t}_{!} \\
\end{align}
$$

But now we're in a sort of a pickle, since we're integrating over $$x'_t$$ but the Dirac function $$\delta(x'_t - x_{t+\tau})$$ will serve as a sort of selector for the integral discarding anything for which the value for $$x'_t$$ does not correspond to the future value $$x_{t+\tau}$$.

Since for any subtraction $$x'_{t} - x_{t+\tau}$$ and function $$f(x'_{t} - x_{t+\tau})$$ we can employ the relation $$\partial_{x'_t} f(x'_{t} - x_{t+\tau}) = - \partial_{x_{t}} f(x'_{t} - x_{t+\tau})$$ via the change of variables, we switch the derivative to 
$$
\begin{align}
	\sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'_{t}}^{n} \ \delta(x'_t - x_{t+\tau}) = & \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ (- \partial_{x_{t}})^{n} \ \delta(x'_t - x_{t+\tau})
\end{align}
$$

The question that remains to be answered is what motivates us to do the derivative switch in the first place.
In terms of algebra and calculus, the switch is mathematically valid, yet the holistic reason for it is still a mystery.
It turns out that due to the Chapman-Kolmogorov equation, we will integrate out the variable $$x'_t$$, so a derivative with respect to a latent variable is not of much use.
More holistically, we want to obtain the change in the probability from $$p(x_t)$$ to $$p(x_{t+\tau})$$ for which the values of $$x'_t$$ are not of much use.

<!-- The Taylor expansion of the Dirac impulse above is used for the entire distribution of $p(y_{t+\tau} | x_t)$ since $y$ originates, so to say, from the distribution $p(y_{t + \tau}|x'_t)$ such that we obtain
$$
\begin{align}
	p(x_{t + \tau} | x'_t) = & \int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_{t})^n \ (- \partial_{x_{t+\tau}})^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t+\tau} | x'_t) dy \\
\end{align}
$$ -->

For the special case of $n=0$ where the factorial, powers and derivatives evaluate to 1 and we can marginalize out over $y$, the sum simplifies to 
$$
\begin{align}
	&\int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ (- \partial_{x_{t}})^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t+\tau} | x'_t) dy_{t+\tau} \\
	= & \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} \underbrace{\int ( y_{t+\tau} - x'_{t})^n \ p(y_{t+\tau} | x'_t) \ dy_{t+\tau}}_{M^{(n)}(x'_t)} \Big) \ \delta(x'_t - x_{t+\tau}) \\
	= & \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} M^{(n)}(x'_t) \Big) \ \delta(x'_t - x_{t+\tau})
\end{align}
$$

Plugging the expanded transition probability back into the Chapman-Kolmogorov equation and noting that the Dirac impulse $$\delta(x'_t-x_{t+\tau})$$ eliminates the integral with respect to $$x'_t$$ by eliminating every value of $$x'$$ different from $$x$$ irrespective of time, we get
$$
\begin{align}
	p(x_{t + \tau}) = & \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t \\
	= & \int \int \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) dy_{t+\tau} \ p(x'_t) \ dx'_{t} \\
	= & \int \int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'_t}^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
	= & \int_{X'} \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} M^{(n)}(x'_t) \Big) \ \delta(x'_t - x_{t+\tau}) \ p(x'_t) \ dx'_t \\
	= & p(x_t) + \underbrace{\sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} \left[ M^{(n)}(x_t) \ p(x_t) \right]}_{\lim_{\tau \rightarrow 0} : \partial_t p(x_t) \tau}
\end{align}
$$
where we note that the evaluation of $n=0$ applies in the same way to any value or function that we multiply into the simplified Taylor expansion.
Pulling $$p(x'_t)$$ to the left side and finding that the change between $$p(x_{t+\tau})$$ and $$p(x'_t)$$ should be proportional to $$\partial_t p(x_t) \tau$$ for a small step size $$\tau$$ analogously to th Euler discretization, we obtain
$$
\begin{align}
	p(x_{t+\tau}) - p(x_t) = & \partial_t p(x_t) \tau \\
	\frac{p(x_{t+\tau}) - p(x_t)}{\tau} = & \partial_t p(x_t).
\end{align}
$$

Finally we can note that we can could cut off the Taylor expansion after the second order and realize that Taylor expansion is equivalent to the time derivative in the limit of time, i.e. $$\lim_{\tau \rightarrow 0}$$ and we can proclaim that
$$
\begin{align}
	\partial_t p(x_t) = & - \partial_x \left[ M^{(1)}(x_t) p(x_t) \right] + \frac{1}{2} \partial_x^2 \left[M^{(2)}(x_t) p(x_t) \right] \\
	= & - \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) p(x_t) \right] \\
\end{align}
$$

### Backward Equation

The Kolmogorov backward equation (KBE) can be derived in the same way while paying attention to the derivatives.

Again we start with the Chapman-Kolmogorov equation:
$$
\begin{align}
	p(x_T | x'_t) = \int p(x_T | x''_{t+\tau}) p(x''_{t+\tau} | x'_t) dx''_{t+\tau}
\end{align}
$$

We expand the transition probability $$p(x'_t | x''_{t+\tau})$$ again with a Dirac function 
$$
\begin{align}
	p(x''_{t+\tau} | x'_t) = \int \delta(y_{t+\tau} - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau}
\end{align} 
$$

Then we expand the Dirac function and expand it with the Taylor series to obtain
$$
\begin{align}
	\delta(y_{t+\tau} - x''_{t+\tau}) = & \delta(y_{t+\tau} - x''_{t+\tau} + x'_t - x'_t) \\
	= & \delta (y_{t+\tau} - x'_t)  \delta(x'_t - x_{t + \tau}) \\
	= & \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau})
\end{align}
$$

Plugging the expanded Dirac function back into the transition probability we obtain
$$
\begin{align}
	p(x''_{t+\tau} | x'_t) = & \int \delta(y_{t+\tau} - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau} \\
	= & \int \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau}
\end{align}
$$

When we compare the derivation of the forward Kramers-Moyals expansion with the backwards Kramers-Moyals expansion we should immediately detect that the only difference is the partial derivative.
The subtle but important difference lies in direction to which we differentiate.
For the forward expansion we are interested how the PDF changes with respect to the future values $$x''_{t+\tau}$$ whereas for the backward expansion we want to ultimately know how the PDF changes backward in time, ergo $$\partial_{x'_t}$$ and not $$\partial_{x''_{t+\tau}}$$ since obviously $$t < t + \tau$$. 

This is consequential for whether we include the moments $$M^{(n)}(x'_t)$$ in the differentiation or not.
Remember that the moments are defined with a fixed value at time $$t$$ and that they are defined as as a forward differentiation $$y_{t+\tau} - x'_t$$.
If we now differentiate with respect to time $$t$$, the moments will not be differentiated as the value $$x'_t$$ is assumed fixed in the moments.


Thus substituting the expanded transition probability back into the Chapman-Kolmogorov equation we obtain,
$$
\begin{align}
	p(x_T | x'_t) = & \int p(x_T | x''_{t+\tau}) \ \int \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau} dx''_{t+\tau} \\
	= & \int p(x_T | x''_{t+\tau}) \ \sum_{n=0}^\infty \frac{1}{n!} \underbrace{\int (y_{t + \tau} - x'_t)^n p(y_{t+\tau} | x'_t) dy_{t+\tau}}_{M^{(n)}(x'_t)} \ \partial_{x'_t}^n \ \delta(x'_t - x''_{t+\tau}) dx''_{t+\tau} \\
	= & p(x_T | x'_{t+\tau}) + M^{(1)}(x'_t) \partial_{x'_t} p(x_T | x'_{t+\tau}) + \frac{1}{2} M^{(2)}(x'_t) \partial_{x'_t}^2 p(x_T | x'_{t+\tau})
	% = & p(x_T | x'_{t+\tau}) + \mu(x'_t) \partial_{x'_{t}} p(x_T | x'_t) + \frac{1}{2} \sigma^2(x'_t) \partial^2_{x'_{t}} p(x_T | x'_{t+\tau}).
\end{align}
$$
Dividing both sides by $\tau$ and evaluating in the limit of $\lim_{\tau \rightarrow 0}$, we get 
$$
\begin{align}
	- \partial_t p(x_T| x'_t) = & \mu(x'_t) \partial_{x'_t} p(x_T | x'_t) + \frac{1}{2} \sigma^2(x'_t) \partial^2_{x'_{t}} p(x_T | x'_{t}).
\end{align}
$$

The equation above is a partial differential equation which characterizes how the probability of $x_T$ changes as we go backwards in time.
It essentially answers the question of how much the probability $p(x_T)$ changes after conditioning the process on $x'_t$ and at earlier point in time $t$.
