<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-04-24T21:31:11+02:00</updated><id>/</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">Bayesian Optimization</title><link href="/blog/BayesianOptimization/" rel="alternate" type="text/html" title="Bayesian Optimization" /><published>2018-04-24T00:00:00+02:00</published><updated>2018-04-24T00:00:00+02:00</updated><id>/blog/BayesianOptimization</id><content type="html" xml:base="/blog/BayesianOptimization/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;bayesian-optimization&quot;&gt;Bayesian Optimization&lt;/h2&gt;

&lt;p&gt;As stated above, many problem settings in engineering and science can be formulated as optimization problems of a criterion, commonly called an objective function, $\mathcal{F}(x)$ with respect to some argument $x$.
The goal of any optimization is to find the global optimum of such a function $\mathcal{F}(x)$.
For linear or convex optimization problems, this is usually feasible, yet optimization becomes difficult for non-linear objective functions.
Bayesian optimization tries to tackle such non-linear objective functions by searching for a global optimum in a probabilistical manner.&lt;/p&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;p&gt;In computer science, mathematics and operations research, mathematical optimization aims to find the best value $x^* \in \mathcal{X}$ from a set of feasible values $\mathcal{X}$ with respect to an criterion or objective function $\mathcal{F}(x)$.
Optimization problems can be formulated as either maximization or minimization problems of the objective function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathcal{F}(x^* ) = \min_{x \in \mathcal{X}} \mathcal{F}(x) = \max_{x \in \mathcal{X}} -\mathcal{F}(x)
\end{align}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     x^* = \underset{x \in \mathcal{X}}{\operatorname{argmin}} \mathcal{F}(x) = \underset{x \in \mathcal{X}}{\operatorname{argmax}} -\mathcal{F}(x)
\end{align}&lt;/script&gt;

&lt;p&gt;Since $\mathcal{F}(x)$ is often a complicated, non-linear function the solution is searched for in an iterative manner.
Most optimization algorithms evaluate the objective function $\mathcal{F}(x)$ through a set of succesive queries $x_{1:n}=\{ x_i \}_{i=1}^n \subset \mathcal{X}$ such that the information of the previous evaluations guide the next evaluation $x_{n+1}$ through a utility function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     x_{n+1} = \underset{x \in \mathcal{X}}{\operatorname{argmax}} \ \mathcal{U}(x \ | \ x_1, \ldots, x_n)
\end{align}&lt;/script&gt;

&lt;p&gt;The information contained in the past evaluations $x_{1:n}$ is thus leveraged in a way to make the evaluation $x_{n+1}$ as close as possible to the global optimum.
The utility function $\mathcal{U}$ should balance the exploration of the set of feasible optima $\mathcal{X}$ while simultaneously exploiting existing information in $ x_{1:n}$ to find the globally optimal solution $x^* $.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-optimization-with-gaussian-processes&quot;&gt;Bayesian Optimization with Gaussian Processes&lt;/h2&gt;

&lt;p&gt;In Bayesian optimization a Gaussian process is used to compute a probability distribution over the past evaluations $x_{1:n}$, which guides a subsequent sampling process.
The sampling process uses an acquisition function $\Lambda(x \ | \ x_{1:n})$, which is a utility function on the posterior distribution computed by the Gaussian process.
The acquisition function balances both the exploration as well as the exploitation of the unknown objective function $\mathcal{F}(x)$.
The next evaluation is chosen such that it maximizes the acquisition function, i.e.
\begin{align}
     x_{n+1} = \underset{x \in \mathcal{X}}{\operatorname{argmax}} \ \Lambda(x \ | \ x_{1:n})
\end{align}&lt;/p&gt;

&lt;p&gt;By computing posterior distributions over all predictions at once, Gaussian processes have a powerful property which enables them to search for an optimum globally.
The posterior distributions allow Gaussian processes to balance both exploitation and exploration of the set of feasible solutions by incorporating their uncertainty into optimization task.&lt;/p&gt;

&lt;p&gt;The acqusition function $\Lambda(x \ | \ x_{1:n})$ serves as an improvement criterion for the yet unevaluated feasible solutions.
The improvement is computed relative to the optimal solution $x^+ \in x_{1:n}$ in the set of previous evaluations $x_{1:n}$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     x^+ &amp;= \underset{x \in x_{1:n}}{\operatorname{argmax}} \ \mathcal{F}(x)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A popular acquisition functions is the upper/lower confidence bound \cite{ucb}, which scales the mean with respect to the previously best evaluation.
It then considers a multiple of the standard deviation and adds it for maximization problems or subtracts it for minimization problems.
The hyperparameter $\kappa$ is usually selected as a small integer number, which can be intuitively selected due to its close relationship to confidence values of the Gaussian distribution.
Given the mean $\mu(x)$ and covariance function $\sigma(x)$, the upper confidence bound is computed with the hyperparameter $\kappa$ via&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathbb{UCB}[x] = \mu(x) + \kappa \sigma(x) - \mathcal{F}(x^+)
\end{align}&lt;/script&gt;

&lt;p&gt;A different acquisition function is the expected improvement (EI) \cite{mockus1975bayesian} which considers the expected value at a point $x_{n+1}$ above the currently best value $x^+$.
The expected improvement is the most Bayesian acquisition function as it incorporates the posterior in its entirety including the uncertainty.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{EI}[x] &amp;= \int_{x^+}^{\infty} \left( \frac{f(x) - f(x^+)}{\sigma(x)} \right) \mathcal{N}(x | \mu(x), \sigma(x)) df(x) \\
     &amp;= \int_{x^+}^{\infty} z(x) \ \mathcal{N}(x | \mu(x), \sigma(x)) df(x) \\
     &amp;=\sigma(x) \left( z(x)  \Phi \left( z(x) \right) + \mathcal{N}_{0,1} \left( z(x) \right) \right) \\
     %&amp;=\sigma(x) \left( \frac{\mu(x) - f(x^+)}{\sigma(x)}  \Phi \left( \frac{\mu(x) - f(x^+)}{\sigma(x)} \right) + \mathcal{N}_{0,1} \left( \frac{\mu(x) - f(x^+)}{\sigma(x)} \right) \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the term $z(x)$ represents the z-score for a specific value $x$ in the yet unevaluated feasible set solutions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     z(x) &amp;= \frac{f(x) - f(x^+)}{\sigma(x)}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While the UCB acquisition function has a more straightforward interpretation, it suffers from getting stuck in local minima.
This is due to UCB using a fixed integer multiple $\kappa$ of the variance instead of integrating over it.
The EI acquisition utilizes the uncertainty in a fully Bayesian way and is able to explore the feasible set even after having found an optimum.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;BO_EI0.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI1.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI2.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI3.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI4.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI5.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI6.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI7.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI8.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI9.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI10.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI11.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI12.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI13.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI14.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">Using Gaussian Processes for Optimization</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="" /></entry><entry><title type="html">Gaussian Processes - Extensions</title><link href="/blog/Gaussian-Processes-Extensions/" rel="alternate" type="text/html" title="Gaussian Processes - Extensions" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes-Extensions</id><content type="html" xml:base="/blog/Gaussian-Processes-Extensions/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;In the real world, the observations onto which the Gaussian process is fitted are often influenced and distorted by noise.
This noise is modeled as a independent, identically distributed normal distribution around zero with an error variance $\sigma^2_{\varepsilon}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     y &amp;= f(x) + \varepsilon, \quad \quad \text{i.i.d.} \ \varepsilon \sim \mathcal{N}(0, \sigma^2_{\varepsilon})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix between the respective observations with noise is modified on the diagonal entries.
The linear covariance operator can be applied independently to both the objective function evaluation and the noise, yet the noise variance can only be included for the diagonal entries of the covariance matrix.
This is due to the assumption of independent, identical distributed noise, which is uncorrelated between observations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y,y'] &amp;= k(x,x') + \mathbb{1}_{y=y'}  \mathbb{V}[\varepsilon]\\
     &amp;= k(x,x') + \mathbb{1}_{y=y'}  \sigma^2_{\varepsilon}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Gaussian process without noisy observations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_nonoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noise only on the mean of the posterior distribution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_meannoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noise only on the variance of the posterior distribution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_varnoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noisy observations affecting both the mean and the variance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_bothnoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be realized with the addition of the noise’s variance to the diagonal entries of the covariance matrix of the observation kernel matrix $K_{XX}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K} =
     \begin{bmatrix}
          K_{XX}+\sigma^2_{\varepsilon} \cdot I  &amp; K_{XX_*} \\
          K_{X_*X} &amp; K_{X_*X_*}
     \end{bmatrix} =
     \begin{bmatrix}
          k(X, X) + \sigma_{\varepsilon} \cdot I &amp; k(X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $I$ is an identity matrix $I\in \mathbb{R}^{N \times N}$.
While the noise itself decreases the precision with which we can fit the GP to the observations, it has convenient numerical properties.
The Gramian block matrix $K_{XX}$ has to be inverted during the computation of the mean and covariance function.
Due to possible rank defincencies, $K_{XX}$ can become singular which prohibits its inversion.
Rank definencies in the covariance matrix can arise when two observations are numerically almost identical.
Incorporating the noise variance into the covariance matrix can be thus regarded as a regularization of the Gaussian process.
This opens the possibility of different regularization themes as both the mean and variance can be independently regularized with respect to the inverse of $K_{XX}$ in $\mu(y_* )$ and $\Sigma(y_* )$.&lt;/p&gt;

&lt;h2 id=&quot;model-selection&quot;&gt;Model Selection&lt;/h2&gt;

&lt;p&gt;The optimization of hyperparameters in machine learning is a pivotal process which can influence the performance significantly.
In this regard, Bayesian methods offer a substantial advantage over non-Bayesian methods as the optimal hyperparameters can be automatically recovered from the Bayesian model.
For a supervised learning task, the objective is to maximize the likelihood probability of the targets $p(\mathcal{D})$.&lt;/p&gt;

&lt;p&gt;A central aspect of Bayesian methods is the placement of a prior $p(\theta)$ over possible values of $\theta$ which encodes the prior belief what values of $\theta$ are regarded as probable.
Instead of considering a single value for $\theta$ a probability distribution is used that assigns a different weighting to different values of $\theta$.
This is especially important in tasks with small datasets where the likelihood is sensitive to the variability in the data.&lt;/p&gt;

&lt;p&gt;The prior can be marginalized to evaluate its influence on the data likelihood.
The objective is therefore to find suitable distributions for $\theta$ which increase the likelihood of the data, ie.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     p(\mathcal{D}) = \int p(\mathcal{D}, \theta) \ p(\theta) \ d\theta
\end{align}&lt;/script&gt;

&lt;p&gt;In the case of Gaussian processes with the squared exponential kernel, the hyperparameters are $\theta = \{ \alpha, \sigma \}$ for which we seek values that maximize the probability of the data, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \max_{\theta} \ p(\mathcal{D}; \theta)
     &amp;= \max_{\theta} \ p(y, X ; \theta) \\
     &amp;= \max_{\theta} \ p(y, X  | \theta) \ p(\theta) \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |K_{XX}|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          {K_{XX}}^{-1}
          y
     \right]
     \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |k(X, X; \theta)|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          k(X, X;\theta)^{-1}
          y
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the parameters $\theta$ determine the Gramian matrix $k(XX;\theta)$.
The maximization of the data likelihood is commonly reformulated as a minimzation of the negative log-likelihood.
Working with the log-probability offers a higher numerical stability with respect to floating-point arithmetic of modern computers.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \min_{\theta}  -\log{p(\mathcal{D};\theta)}
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |K_{XX}|\right] + \frac{1}{2} y^TK_{XX}^{-1}y \\
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |k(XX;\theta)|\right] + \frac{1}{2} y^Tk(X,X;\theta)^{-1}y
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The optimization of the log-likelihood can be done with regular optimization algorithms such as limited memory BFGS.&lt;/p&gt;

&lt;h2 id=&quot;derivative-information&quot;&gt;Derivative Information&lt;/h2&gt;

&lt;p&gt;Gaussian processes in their traditional definition are described as a Gaussian distribution over possibly infinite observations.
A Gaussian process computes a predictive distribution for $y_*$ such that predictions are close to observations in their vicinity.
We can expand the Gaussian process by including derivative observations into the set of observations which enforces a similarity in the gradients of the predictions with respect to observations in their vicinity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \begin{bmatrix}
          y \\
          \nabla y \\
          y_*
     \end{bmatrix}
     \sim
     \mathcal{N}\left(\ \cdot \ | \mathbf{0}, \mathbf{K}^{\nabla}\right)
\end{align}&lt;/script&gt;

&lt;p&gt;The joint distribution over predictions, derivative observations and observations can be modeled as a Gaussian over all three types of observations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(y_*, \nabla y, y, X_*, X)
&amp;\propto
\exp \left[
-\frac{1}{2}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}^T
\left[
\begin{array}{c c | c}
     K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
     K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
     \hline
     K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
\end{array}
\right]^{-1}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}
\right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the expanded covariance matrix which now includes similarity measures between predictions, observations and derivative observations:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K}^{\nabla}
     &amp;=
     \left[
     \begin{array}{c | c}
          K^{\nabla, \nabla \nabla}_{X X} &amp; K^{\nabla}_{X X_*} \\
          \hline
          K^{\nabla}_{X_*X} &amp; K_{X_* X_*}
     \end{array}
     \right]\\
     &amp;=
     \left[
          \begin{array}{c c | c}
               K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
               K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
               \hline
               K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
          \end{array}
     \right] \\
     &amp;=
     \left[
     \renewcommand*{\array_*tretch}{1.5}
          \begin{array}{c c | c}
               k_{y,y}(X, X) &amp; k_{y, \nabla y}(X, X) &amp; k_{y,y_*}(X, X_*) \\
               k_{\nabla y, y}(X, X) &amp; k_{ \nabla y, \nabla y}(X, X) &amp; k_{\nabla y, y_*}(X, X_*) \\
               \hline
               k_{y_*, y}(X_*, X) &amp; k_{y_*, \nabla y}(X_*, X) &amp; k_{y_*, y_*}(X_*, X_*)
          \end{array}
     \right] \label{eq:derivobs_kernelmatrix-1}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The posterior distribution including derivative observations can be derived from the joint distribution with the matrix inversion lemma in the same manner as seen above.
The mean and covariance of the posterior distribution with derivative observations can be computed with the expanded kernel matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
     p(y_* | \nabla y, y, X_*, X) &amp;= \mathcal{N} \big( K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} y, K_{X_*X_*} - K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} K^{\nabla}_{XX_*} \big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Gramian block matrices between predictions, observations and derivative observations can be computed with updated kernels with incorporate the derivative observations.
More precisely, the covariance between two any entries in the observation respectively prediction vector are defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= k_{y, y'}(x, x') \\
     \mathbb{C}[y, \nabla y'] &amp;= k_{y, \nabla y'}(x, x') \\
     \mathbb{C}[\nabla y, y'] &amp;= k_{\nabla y, y'}(x, x') \\
     \mathbb{C}[\nabla y, \nabla y'] &amp;= k_{\nabla y, \nabla y'}(x, x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;These updated kernels can be derived in a fairly straightforward manner since the covariance with the zero mean assumption is a linear operator.
In order to expand the Gaussian process with derivative observations we have to take the derivative of the kernel and expand the covariance matrix with the respective entries:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= k(x,x') \\
     \mathbb{C}[y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x'} \frac{1}{N} \sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x'} \mathbb{C}[y,y'] \\
          &amp;= \nabla_{x'} k(x,x')\\
     \mathbb{C}[\nabla_{x}y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N \nabla_{x} y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x} \nabla_{x'} \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x}\nabla_{x'} \mathbb{C}[y, y'] \\
          &amp;= \nabla_{x}\nabla_{x'} k(x,x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While derivative observations themselves are usually hard to come by for computationally expensive functions $f(x)$, derivative observations are of numerical advantage in cases where observations lie very close to each other.
In these cases the inversion can become unstable or even impossible due to the rank definciency.
Derivative observations pose a useful way to circumvent such rank definciencies for very close observations by combining two observations into one observation and a derivative observation.&lt;/p&gt;

&lt;p&gt;A Gaussian process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;GP_4Obs.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same Gaussian process with derivative observations. The GP is able to fit the true function considerably better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;GP_4Obs_Deriv.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">Extensions to Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_4Obs_Deriv.png" /></entry><entry><title type="html">Gaussian Processes - Basics</title><link href="/blog/Gaussian-Processes/" rel="alternate" type="text/html" title="Gaussian Processes - Basics" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes</id><content type="html" xml:base="/blog/Gaussian-Processes/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Many problems in science and engineering can be formulated as a mathematical optimization problem in which an optimal solution is sought, either locally or globally.
The field of global optimization is the application of applied mathematics and numerical analysis towards finding the overall optimal solution in a set of candidate solutions.
Local optimization is considered an easier problem, in which it suffices to find an optimum which is optimal with respect to its immediate vicinity.
Such a local optimum is obviously a suboptimal solution and, while harder to find, global optima are more preferred.&lt;/p&gt;

&lt;p&gt;Generally, optimization problems are formulated as finding the optimal solution which minimizes, respectively maximizes, a criterion, which is commonly referred to as the objective function.
Further constraints on the the set of solutions can be formulated, such that only a subset of solutions are permissible as candidates for the optimum.&lt;/p&gt;

&lt;p&gt;Optimization is commonly done in an iterative manner where the objective function is evaluated for multiple candidate solutions.
Due to the iterative nature, it becomes desirable to evaluate this function as few times as possible over the course of the entire optimization, which becomes even more crucial when the evaluation of the objective function itself is costly.
Therefore, it would be advantageous to infer information about the objective function beyond the evaluations themselves, which only provide punctual information.&lt;/p&gt;

&lt;p&gt;Bayesian inference models provide such advantages since they compute predictive distributions instead of punctual evaluations.
One class of Bayesian inference models are Gaussian processes (GP), which can be applied to model previous evaluations of the objective function as a multi-variate Gaussian distribution.
Given such a Gaussian distribution over the previous evaluations, information can be inferred over all candidate solutions in the feasible set at once.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-processes&quot;&gt;Gaussian Processes&lt;/h2&gt;

&lt;p&gt;In most situations where observations have many small independent components, their distribution tends towards the Gaussian distribution.
Compared to other probability distributions, the Gaussian distribution is tractable and it’s parameters have intuitive meaning.
The theory of the central limit theorem (CLT) makes the Gaussian distribution a versatile distribution which is used in numerous situations in science and engineering.&lt;/p&gt;

&lt;p&gt;A convenient property of the Gaussian distribution for a random variable $X$ is its complete characterization by its mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance $\Sigma$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu &amp;= \mathbb{E}[X] \\
     \Sigma &amp;= \mathbb{E}[(X-\mu)^T(X-\mu)]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Mathematically, a multivariate Gaussian for a vector $x \in \mathbb{R}^d$ is defined by its mean $\mu \in \mathbb{R}^d$ and covariance function $\Sigma \in \mathbb{R}^{d \times d}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
          \mathcal{N}(x | \mu, \Sigma) &amp;=
               \frac{1}{\sqrt{(2 \pi)^d |\Sigma|^2}}
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right] \\
               &amp;\propto
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A useful property of the Gaussian distribution is that its shape is determined by its mean and covariance in the exponential term.
This allows us to omitt the normalization constant and determine the relevant mean and covariance terms from the exponential term.&lt;/p&gt;

&lt;p&gt;Let $y=f(x)$, where $x \in \mathbb{R}^d$ and $y \in \mathbb{R}$ be the function which we want to estimate with a Gaussian Process.
Furthermore, let $\mathcal{D} = (X, y) = \{(x_i, y_i)\}_{i=0}^N$
with $X \in$ $\mathbb{R}^{N \times d}$
and $y \in \mathbb{R}^{N}$,
be our training observations of the function $f$.&lt;/p&gt;

&lt;p&gt;Lastly, let $ \mathcal{D}_* = ( X_* , y_* ) = \{ ( X_{ * j } , y_{ * j } ) \} _{j=0}^{ N_* } $ with $ X_* \in \mathbb{R}^{N_* \times d} $ and $ y_* \in \mathbb{R}^{ N_* } $ ,
be the test observations at which we want to compute the predictive distributions of $ y_* =f( X_* ) $
for the function $ f $.&lt;/p&gt;

&lt;p&gt;A Gaussian process is defined as a stochastic process, such that every finite collection of realizations
$ X = \{ x_i \}_{ i=0 }^N , x_i \in \mathbb{R}^d$ of the random variables
$ X \sim \mathcal{N}( \cdot  |  \mu, \Sigma),  X \in \mathbb{R}^d $
is a multivariate distribution.&lt;/p&gt;

&lt;p&gt;A constraint of Gaussian processes as they are used in machine learning, which can be relaxed in specific cases, is that they are assumed to have a zero mean.
In order to compute a predictive distribution over $ y_* $ we initially construct the joint distribution over the training observations $\mathcal{D} = (X,y) $ and test observations $ \mathcal{D}_* = ( X_* ,y_* ) $:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_*, y, X_* , X) &amp;= \frac{1}{\sqrt{(2 \pi)^{ N+N_* } |K|^2}}
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ XX } &amp; K_{ X X_* } \\
          K_{ X_* X } &amp; K_{ X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ X X } &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_*}
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \mathcal{N}
     \left(
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix} \middle|
     \mathbf{0}, K
     \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the covariance matrix of the joint Gaussian distribution is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K=\begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_* }
     \end{bmatrix}
     =
     \begin{bmatrix}
          k( X, X) &amp; k( X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $ k(x,x’) $ is an kernel function $ k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ that measures the similarity between two vectors $ x, x’ \in \mathcal{X}$.
We can observe from \eqref{eq:covariance1} that the covariance between any two observations in the distribution is determined by the similarity through the kernel function $k(x, x’)$, namely&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathbb{C}[y, y'] = k(x, x')
\end{align}&lt;/script&gt;

&lt;p&gt;An essential component of a GP is the kernel function with which the covariances is computed.
Often the kernels are engineered to incorporate prior knowledge.
A commonly used kernel is the squared exponential kernel&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     k(x, x' \ ; \ \theta) = \alpha \exp \left[ - \frac{|| x - x'||^2}{2 \sigma^2}\right], \quad \theta = \{ \alpha, \sigma \}
\end{align}&lt;/script&gt;

&lt;p&gt;where $\theta$ corresponds to the hyperparameters of the Gaussian process which can be independently optimized with respect to the observations $(X, y)$.&lt;/p&gt;

&lt;p&gt;Gaussian Processes can be readily extended to multiple dimensions by simply adjusting the kernel to incorporate multiple dimensions.
The individual variances $\sigma_i$ of the dimensions $\mathbb{R}^d$ in the exponential kernel can be independently adjusted, or optimized with the maximization of the marginal probability of the data.
The expanded kernel for multidimensional input is defined as followed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     k(x, x'; \ \theta) &amp;= \alpha \exp \left[ - \frac{1}{2} (x-x') \Sigma^{-1} (x-x')     \right], \quad \theta=\{ \alpha, \Sigma \} \\
     \Sigma &amp;= \text{diag}(\sigma^2_0, \sigma^2_1, \ldots, \sigma^2_d)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The block matrices $k(X,X) \in \mathbb{R}^{N \times N}$
$ k(X, X_* ) \in \mathbb{R}^{N \times N_* }, $
$k( X_* , X ) \in \mathbb{R}^{ N_* \times N }$ and
$k(X_* , X_* ) \in \mathbb{R}^{N_* \times N_* }$ are the Gramian matrices of the training and test observations with respect to the kernel $k(x, x’)$.&lt;/p&gt;

&lt;p&gt;Furthermore both $k(X,X)$ and $k( X_* , X_* )$ are symmetric matrices and $k( X, X_* )$ and $k( X_* ,X)$ are each others mutually transposed.&lt;/p&gt;

&lt;p&gt;Given the joint distribution $ p(y_* , y, X_* , X) $, the aim for modeling the training and test observations with a GP is to derive the posterior distribution $ p( y_*  | y, X_* , X ) $ .
In order to derive the mean and covariance function of the posterior distribution, the block matrix inversion lemma is used to compute the inverse of the covariance matrix.&lt;/p&gt;

&lt;p&gt;For ease of reading and brevity the respective block matrices were replaced by more easily readible variables in the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K^{-1}&amp;= \begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1} \label{eq:blockmatrixinversionlemma1} \\
     &amp; =\begin{bmatrix}
          A &amp; B \\
          C &amp; D
     \end{bmatrix}^{-1} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\
          -(D-CA^{-1}B)^{-1}CA^{-1} &amp; (D-CA^{-1}B)^{-1}
     \end{bmatrix} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B\Sigma^{-1}CA^{-1} &amp; -A^{-1}B\Sigma^{-1} \\
          -\Sigma^{-1}CA^{-1} &amp; \Sigma^{-1}
     \end{bmatrix} \label{eq:Sigma^-1Identity} \\
     &amp;= \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix} \label{eq:blockmatrixinversionlemma-1} \\
     \Sigma &amp;= D-CA^{-1}B = K_{X_* X_* } - K_{ X_* X}{K_{ X_* X_* }}^{-1}K_{X X_* }
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Instead of computing the inverse of the entire matrix $K$, which can be computationally expensive for large covariance matrices, the precision matrix $K^{-1}$ can be computed block-wise with the block matrix inversion lemma.
Given the precision matrix in block matrix notation, the inner product in the exponential term of the Gaussian distribution can be computed as a sum over the inner products with the independent block matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* , y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{XX} &amp; K_{X X_* } \\
          K_{X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( y^TPy + y^TQ y_* + y_*^TRy + y_* ^TS y_*
     \right)
     \right] \label{eq:jointdist_innersumoverblockmatrices}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since we are only interested in the posterior distribution $p(y_*  | y, X_* , X )$, terms which do not include $ y_* $ can be moved into the normalization term.
The conditional distribution can thus be simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* |  y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -y^TQy_* - y_*^TRy + y_*^TS y_*
     \right)
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( -y^TA^{-1}B\Sigma^{-1} y_* -y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}K_{X_* X}{K_{ X X }}^{-1} y + y_*^T\Sigma^{-1}y_*
     \right)
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the matrices $\Sigma$ being a symmetric matrix by construction, and $B$ and $C$ being each other transposed, namely $C^T=B$, which gives rise to the identity:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     (y^TA^{-1}B\Sigma^{-1}y_*)^T
          &amp;= y_*^T(\Sigma^{-1})^TB^T(A^{-1})^Ty \\
          &amp;= y_*^T\Sigma^{-1}CA^{-1}y
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Alternatively one would argue that the result of both inner products yields the same scalar value due to $B=C^T$.
With the derivations above we obtain a posterior distribution $p(y_*  |  y, X_* , X )$ with the mean and covariance function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu(y_*)       &amp;= K_{ X_* X}{K_{XX}}^{-1}y \\
     \Sigma(y_*)    &amp;= K_{ X_* X_* } - K_{ X_* X}{K_{ X X }}^{-1}K_{ X X_*}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;It should be noted that during plotting only the diagonal entries of the covariance matrix are of interest since the diagonal entries of the covariance matrix denote the variances at the evaluated points.
Given the computation of both the mean and variance of the posterior distribution we obtain a Gaussian distribution:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* | y, X_*, X) &amp;= \mathcal{N} \big( \underbrace{K_{X_* X} {K_{XX}}^{-1} y}_{\mu}, \underbrace{K_{X_* X_*} - K_{X_* X}{K_{X X}}^{-1}K_{X X_*}}_{\Sigma} \big)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here is an image of a Gaussian Process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ludwigwinkler/BayesianOptimization/gh-pages/docs/GP_2Obs.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">A Tutorial for Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_2Obs.png" /></entry><entry><title type="html">RL in Challenging State Spaces and Sparse Reward Signals</title><link href="/blog/AdvRL/" rel="alternate" type="text/html" title="RL in Challenging State Spaces and Sparse Reward Signals" /><published>2017-11-06T00:00:00+01:00</published><updated>2017-11-06T00:00:00+01:00</updated><id>/blog/AdvRL</id><content type="html" xml:base="/blog/AdvRL/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Meetup on Nov 6, 2017.
It is somewhat more high level than usual since the audience came from a broad range of backgrounds.&lt;/p&gt;

&lt;p&gt;The content of the talk were ‘AlphaGo’, it’s successor ‘AlphaGo Zero’ and ‘Feudal Networks’ all published by DeepMind.
The talk can be downloaded &lt;a href=&quot;BML-AdvRL.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><summary type="html">Talk on AlphaGo and Hierarchical RL</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/AdvRL_Cover.png" /></entry><entry><title type="html">Introduction to Reinforcement Learning</title><link href="/blog/IntroRL/" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning" /><published>2017-05-03T00:00:00+02:00</published><updated>2017-05-03T00:00:00+02:00</updated><id>/blog/IntroRL</id><content type="html" xml:base="/blog/IntroRL/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Seminar hosted by Ben in the offices of &lt;a href=&quot;https://lateral.io&quot;&gt;Lateral&lt;/a&gt;.
It serves as an introduction to reinforcement learning covering the usual suspects like on- and off-policy algorithms, TD-learning and shows how neural networks were used as function approximators in deep reinforcement learning.&lt;/p&gt;

&lt;p&gt;The talk can be downloaded &lt;a href=&quot;BML-RL.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><summary type="html">A Tutorial Talk on RL</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/IntroRL_Cover.png" /></entry></feed>
