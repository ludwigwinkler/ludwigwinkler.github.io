<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-04-24T19:47:39+02:00</updated><id>/</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">Gaussian Processes - Extensions</title><link href="/blog/Gaussian-Processes-Extensions/" rel="alternate" type="text/html" title="Gaussian Processes - Extensions" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes-Extensions</id><content type="html" xml:base="/blog/Gaussian-Processes-Extensions/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;In the real world, the observations onto which the Gaussian process is fitted are often influenced and distorted by noise.
This noise is modeled as a independent, identically distributed normal distribution around zero with an error variance $\sigma^2_{\varepsilon}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     y &amp;= f(x) + \varepsilon, \quad \quad \text{i.i.d.} \ \varepsilon \sim \mathcal{N}(0, \sigma^2_{\varepsilon})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix between the respective observations with noise is modified on the diagonal entries.
The linear covariance operator can be applied independently to both the objective function evaluation and the noise, yet the noise variance can only be included for the diagonal entries of the covariance matrix.
This is due to the assumption of independent, identical distributed noise, which is uncorrelated between observations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y,y'] &amp;= k(x,x') + \mathbb{1}_{y=y'}  \mathbb{V}[\varepsilon]\\
     &amp;= k(x,x') + \mathbb{1}_{y=y'}  \sigma^2_{\varepsilon}
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{figure}[tb]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
          \includegraphics[width=\textwidth]{Noise_nonoise.png}
          \captionsetup{width=.8\linewidth}
          \caption{Gaussian process without noisy observations.}
          \label{fig:gull}
     \end{subfigure}
     \begin{subfigure}[t]{0.45\textwidth}
          \includegraphics[width=\textwidth]{Noise_bothnoise.png}
          \captionsetup{width=.8\linewidth}
          \caption{Gaussian process with noisy observations}
          \label{fig:tiger}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.45\textwidth}
          \includegraphics[width=\textwidth]{Noise_meannoise.png}
          \captionsetup{width=.8\linewidth}
          \caption{Gaussian process with noise only on the mean of the posterior distribution.}
          \label{fig:gull}
     \end{subfigure}
     \begin{subfigure}[t]{0.45\textwidth}
          \includegraphics[width=\textwidth]{Noise_varnoise.png}
          \captionsetup{width=.8\linewidth}
          \caption{Gaussian process with noise only on the variance of the posterior distribution}
          \label{fig:tiger}
     \end{subfigure}
     \caption{The influence of noisy observations on the posterior distribution of the Gaussian process.
     Especially (d) would be interesting in practice as it assumes the mean as undisturbed but includes the noise in the variance of the observation.}
     \label{fig:noise}
\end{figure}&lt;/script&gt;

&lt;p&gt;This can be realized with the addition of the noise’s variance to the diagonal entries of the covariance matrix of the observation kernel matrix $K_{XX}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K} =
     \begin{bmatrix}
          K_{XX}+\sigma^2_{\varepsilon} \cdot I  &amp; K_{XX_*} \\
          K_{X_*X} &amp; K_{X_*X_*}
     \end{bmatrix} =
     \begin{bmatrix}
          k(X, X) + \sigma_{\varepsilon} \cdot I &amp; k(X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $I$ is an identity matrix $I\in \mathbb{R}^{N \times N}$.
While the noise itself decreases the precision with which we can fit the GP to the observations, it has convenient numerical properties.
The Gramian block matrix $K_{XX}$ has to be inverted during the computation of the mean and covariance function.
Due to possible rank defincencies, $K_{XX}$ can become singular which prohibits its inversion.
Rank definencies in the covariance matrix can arise when two observations are numerically almost identical.
Incorporating the noise variance into the covariance matrix can be thus regarded as a regularization of the Gaussian process.
This opens the possibility of different regularization themes as both the mean and variance can be independently regularized with respect to the inverse of $K_{XX}$ in $\mu(y_* )$ and $\Sigma(y_* )$.&lt;/p&gt;

&lt;h2 id=&quot;model-selection&quot;&gt;Model Selection&lt;/h2&gt;

&lt;p&gt;The optimization of hyperparameters in machine learning is a pivotal process which can influence the performance significantly.
In this regard, Bayesian methods offer a substantial advantage over non-Bayesian methods as the optimal hyperparameters can be automatically recovered from the Bayesian model.
For a supervised learning task, the objective is to maximize the likelihood probability of the targets $p(\mathcal{D})$.&lt;/p&gt;

&lt;p&gt;A central aspect of Bayesian methods is the placement of a prior $p(\theta)$ over possible values of $\theta$ which encodes the prior belief what values of $\theta$ are regarded as probable.
Instead of considering a single value for $\theta$ a probability distribution is used that assigns a different weighting to different values of $\theta$.
This is especially important in tasks with small datasets where the likelihood is sensitive to the variability in the data.&lt;/p&gt;

&lt;p&gt;The prior can be marginalized to evaluate its influence on the data likelihood.
The objective is therefore to find suitable distributions for $\theta$ which increase the likelihood of the data, ie.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     p(\mathcal{D}) = \int p(\mathcal{D}, \theta) \ p(\theta) \ d\theta
\end{align}&lt;/script&gt;

&lt;p&gt;In the case of Gaussian processes with the squared exponential kernel, the hyperparameters are $\theta = \{ \alpha, \sigma \}$ for which we seek values that maximize the probability of the data, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \max_{\theta} \ p(\mathcal{D}; \theta)
     &amp;= \max_{\theta} \ p(y, X ; \theta) \\
     &amp;= \max_{\theta} \ p(y, X  | \theta) \ p(\theta) \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |K_{XX}|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          {K_{XX}}^{-1}
          y
     \right]
     \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |k(X, X; \theta)|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          k(X, X;\theta)^{-1}
          y
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the parameters $\theta$ determine the Gramian matrix $k(XX;\theta)$.
The maximization of the data likelihood is commonly reformulated as a minimzation of the negative log-likelihood.
Working with the log-probability offers a higher numerical stability with respect to floating-point arithmetic of modern computers.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \min_{\theta}  -\log{p(\mathcal{D};\theta)}
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |K_{XX}|\right] + \frac{1}{2} y^TK_{XX}^{-1}y \\
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |k(XX;\theta)|\right] + \frac{1}{2} y^Tk(X,X;\theta)^{-1}y
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The optimization of the log-likelihood can be done with regular optimization algorithms such as limited memory BFGS.&lt;/p&gt;

&lt;p&gt;\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
      \captionsetup{width=.8\linewidth}
        \includegraphics[width=\textwidth]{LL_Unoptimized.png}
        \caption{Gaussian process before hyperparameter optimization.}
        \label{fig:gull}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
      \captionsetup{width=.8\linewidth}
        \includegraphics[width=\textwidth]{LL_Optimized.png}
        \caption{Optimized hyperparameters using log-likelihood minimization. The kernel parameters have been fitted to the marginal data distribution.}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Example of the model selection for a Gaussian process. for which the hyperparameters $\theta$ have been optimized.}
\end{figure}&lt;/p&gt;

&lt;h2 id=&quot;derivative-information&quot;&gt;Derivative Information&lt;/h2&gt;

&lt;p&gt;Gaussian processes in their traditional definition are described as a Gaussian distribution over possibly infinite observations.
A Gaussian process computes a predictive distribution for $y_*$ such that predictions are close to observations in their vicinity.
We can expand the Gaussian process by including derivative observations into the set of observations which enforces a similarity in the gradients of the predictions with respect to observations in their vicinity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \begin{bmatrix}
          y \\
          \nabla y \\
          y_*
     \end{bmatrix}
     \sim
     \mathcal{N}\left(\ \cdot \ | \mathbf{0}, \mathbf{K}^{\nabla}\right)
\end{align}&lt;/script&gt;

&lt;p&gt;The joint distribution over predictions, derivative observations and observations can be modeled as a Gaussian over all three types of observations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(y_*, \nabla y, y, X_*, X)
&amp;\propto
\exp \left[
-\frac{1}{2}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}^T
\left[
\begin{array}{c c | c}
     K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
     K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
     \hline
     K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
\end{array}
\right]^{-1}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}
\right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the expanded covariance matrix which now includes similarity measures between predictions, observations and derivative observations:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K}^{\nabla}
     &amp;=
     \left[
     \begin{array}{c | c}
          K^{\nabla, \nabla \nabla}_{X X} &amp; K^{\nabla}_{X X_*} \\
          \hline
          K^{\nabla}_{X_*X} &amp; K_{X_* X_*}
     \end{array}
     \right]\\
     &amp;=
     \left[
          \begin{array}{c c | c}
               K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
               K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
               \hline
               K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
          \end{array}
     \right] \\
     &amp;=
     \left[
     \renewcommand*{\array_*tretch}{1.5}
          \begin{array}{c c | c}
               k_{y,y}(X, X) &amp; k_{y, \nabla y}(X, X) &amp; k_{y,y_*}(X, X_*) \\
               k_{\nabla y, y}(X, X) &amp; k_{ \nabla y, \nabla y}(X, X) &amp; k_{\nabla y, y_*}(X, X_*) \\
               \hline
               k_{y_*, y}(X_*, X) &amp; k_{y_*, \nabla y}(X_*, X) &amp; k_{y_*, y_*}(X_*, X_*)
          \end{array}
     \right] \label{eq:derivobs_kernelmatrix-1}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The posterior distribution including derivative observations can be derived from the joint distribution with the matrix inversion lemma in the same manner as seen above.
The mean and covariance of the posterior distribution with derivative observations can be computed with the expanded kernel matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
     p(y_* | \nabla y, y, X_*, X) &amp;= \mathcal{N} \big( K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} y, K_{X_*X_*} - K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} K^{\nabla}_{XX_*} \big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Gramian block matrices between predictions, observations and derivative observations can be computed with updated kernels with incorporate the derivative observations.
More precisely, the covariance between two any entries in the observation respectively prediction vector are defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= k_{y, y'}(x, x') \\
     \mathbb{C}[y, \nabla y'] &amp;= k_{y, \nabla y'}(x, x') \\
     \mathbb{C}[\nabla y, y'] &amp;= k_{\nabla y, y'}(x, x') \\
     \mathbb{C}[\nabla y, \nabla y'] &amp;= k_{\nabla y, \nabla y'}(x, x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;These updated kernels can be derived in a fairly straightforward manner since the covariance with the zero mean assumption is a linear operator.
In order to expand the Gaussian process with derivative observations we have to take the derivative of the kernel and expand the covariance matrix with the respective entries:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= k(x,x') \\
     \mathbb{C}[y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x'} \frac{1}{N} \sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x'} \mathbb{C}[y,y'] \\
          &amp;= \nabla_{x'} k(x,x')\\
     \mathbb{C}[\nabla_{x}y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N \nabla_{x} y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x} \nabla_{x'} \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x}\nabla_{x'} \mathbb{C}[y, y'] \\
          &amp;= \nabla_{x}\nabla_{x'} k(x,x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While derivative observations themselves are usually hard to come by for computationally expensive functions $f(x)$, derivative observations are of numerical advantage in cases where observations lie very close to each other.
In these cases the inversion can become unstable or even impossible due to the rank definciency.
Derivative observations pose a useful way to circumvent such rank definciencies for very close observations by combining two observations into one observation and a derivative observation.
\begin{figure}[htb]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{GP_4Obs.png}
        \captionsetup{width=.8\linewidth}
        \caption{GP without derivative observations. Since the GP does not take the derivative of the function into concern and assumes a zero mean, the slope of the approximated function does not correspond to the slope of the objective function at the observations.}
        \label{fig:gull}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \captionsetup{width=.8\linewidth}
        \includegraphics[width=\textwidth]{GP_4Obs_Deriv.png}
        \caption{The same GP with derivative observations. The true function $f(x)$ can now be modeled with a limited number of observations and derivative observations.}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Example for a Gaussian process with additional derivative observations.}
    \label{fig:derivative}
\end{figure}&lt;/p&gt;</content><summary type="html">Extensions to Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_4Obs_Deriv.png" /></entry><entry><title type="html">Gaussian Processes - Basics</title><link href="/blog/Gaussian-Processes/" rel="alternate" type="text/html" title="Gaussian Processes - Basics" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes</id><content type="html" xml:base="/blog/Gaussian-Processes/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Many problems in science and engineering can be formulated as a mathematical optimization problem in which an optimal solution is sought, either locally or globally.
The field of global optimization is the application of applied mathematics and numerical analysis towards finding the overall optimal solution in a set of candidate solutions.
Local optimization is considered an easier problem, in which it suffices to find an optimum which is optimal with respect to its immediate vicinity.
Such a local optimum is obviously a suboptimal solution and, while harder to find, global optima are more preferred.&lt;/p&gt;

&lt;p&gt;Generally, optimization problems are formulated as finding the optimal solution which minimizes, respectively maximizes, a criterion, which is commonly referred to as the objective function.
Further constraints on the the set of solutions can be formulated, such that only a subset of solutions are permissible as candidates for the optimum.&lt;/p&gt;

&lt;p&gt;Optimization is commonly done in an iterative manner where the objective function is evaluated for multiple candidate solutions.
Due to the iterative nature, it becomes desirable to evaluate this function as few times as possible over the course of the entire optimization, which becomes even more crucial when the evaluation of the objective function itself is costly.
Therefore, it would be advantageous to infer information about the objective function beyond the evaluations themselves, which only provide punctual information.&lt;/p&gt;

&lt;p&gt;Bayesian inference models provide such advantages since they compute predictive distributions instead of punctual evaluations.
One class of Bayesian inference models are Gaussian processes (GP), which can be applied to model previous evaluations of the objective function as a multi-variate Gaussian distribution.
Given such a Gaussian distribution over the previous evaluations, information can be inferred over all candidate solutions in the feasible set at once.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-processes&quot;&gt;Gaussian Processes&lt;/h2&gt;

&lt;p&gt;In most situations where observations have many small independent components, their distribution tends towards the Gaussian distribution.
Compared to other probability distributions, the Gaussian distribution is tractable and it’s parameters have intuitive meaning.
The theory of the central limit theorem (CLT) makes the Gaussian distribution a versatile distribution which is used in numerous situations in science and engineering.&lt;/p&gt;

&lt;p&gt;A convenient property of the Gaussian distribution for a random variable $X$ is its complete characterization by its mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance $\Sigma$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu &amp;= \mathbb{E}[X] \\
     \Sigma &amp;= \mathbb{E}[(X-\mu)^T(X-\mu)]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Mathematically, a multivariate Gaussian for a vector $x \in \mathbb{R}^d$ is defined by its mean $\mu \in \mathbb{R}^d$ and covariance function $\Sigma \in \mathbb{R}^{d \times d}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
          \mathcal{N}(x | \mu, \Sigma) &amp;=
               \frac{1}{\sqrt{(2 \pi)^d |\Sigma|^2}}
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right] \\
               &amp;\propto
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A useful property of the Gaussian distribution is that its shape is determined by its mean and covariance in the exponential term.
This allows us to omitt the normalization constant and determine the relevant mean and covariance terms from the exponential term.&lt;/p&gt;

&lt;p&gt;Let $y=f(x)$, where $x \in \mathbb{R}^d$ and $y \in \mathbb{R}$ be the function which we want to estimate with a Gaussian Process.
Furthermore, let $\mathcal{D} = (X, y) = \{(x_i, y_i)\}_{i=0}^N$
with $X \in$ $\mathbb{R}^{N \times d}$
and $y \in \mathbb{R}^{N}$,
be our training observations of the function $f$.&lt;/p&gt;

&lt;p&gt;Lastly, let $ \mathcal{D}_* = ( X_* , y_* ) = \{ ( X_{ * j } , y_{ * j } ) \} _{j=0}^{ N_* } $ with $ X_* \in \mathbb{R}^{N_* \times d} $ and $ y_* \in \mathbb{R}^{ N_* } $ ,
be the test observations at which we want to compute the predictive distributions of $ y_* =f( X_* ) $
for the function $ f $.&lt;/p&gt;

&lt;p&gt;A Gaussian process is defined as a stochastic process, such that every finite collection of realizations
$ X = \{ x_i \}_{ i=0 }^N , x_i \in \mathbb{R}^d$ of the random variables
$ X \sim \mathcal{N}( \cdot  |  \mu, \Sigma),  X \in \mathbb{R}^d $
is a multivariate distribution.&lt;/p&gt;

&lt;p&gt;A constraint of Gaussian processes as they are used in machine learning, which can be relaxed in specific cases, is that they are assumed to have a zero mean.
In order to compute a predictive distribution over $ y_* $ we initially construct the joint distribution over the training observations $\mathcal{D} = (X,y) $ and test observations $ \mathcal{D}_* = ( X_* ,y_* ) $:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_*, y, X_* , X) &amp;= \frac{1}{\sqrt{(2 \pi)^{ N+N_* } |K|^2}}
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ XX } &amp; K_{ X X_* } \\
          K_{ X_* X } &amp; K_{ X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ X X } &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_*}
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \mathcal{N}
     \left(
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix} \middle|
     \mathbf{0}, K
     \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the covariance matrix of the joint Gaussian distribution is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K=\begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_* }
     \end{bmatrix}
     =
     \begin{bmatrix}
          k( X, X) &amp; k( X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $ k(x,x’) $ is an kernel function $ k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ that measures the similarity between two vectors $ x, x’ \in \mathcal{X}$.
We can observe from \eqref{eq:covariance1} that the covariance between any two observations in the distribution is determined by the similarity through the kernel function $k(x, x’)$, namely&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathbb{C}[y, y'] = k(x, x')
\end{align}&lt;/script&gt;

&lt;p&gt;An essential component of a GP is the kernel function with which the covariances is computed.
Often the kernels are engineered to incorporate prior knowledge.
A commonly used kernel is the squared exponential kernel&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     k(x, x' \ ; \ \theta) = \alpha \exp \left[ - \frac{|| x - x'||^2}{2 \sigma^2}\right], \quad \theta = \{ \alpha, \sigma \}
\end{align}&lt;/script&gt;

&lt;p&gt;where $\theta$ corresponds to the hyperparameters of the Gaussian process which can be independently optimized with respect to the observations $(X, y)$.&lt;/p&gt;

&lt;p&gt;Gaussian Processes can be readily extended to multiple dimensions by simply adjusting the kernel to incorporate multiple dimensions.
The individual variances $\sigma_i$ of the dimensions $\mathbb{R}^d$ in the exponential kernel can be independently adjusted, or optimized with the maximization of the marginal probability of the data.
The expanded kernel for multidimensional input is defined as followed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     k(x, x'; \ \theta) &amp;= \alpha \exp \left[ - \frac{1}{2} (x-x') \Sigma^{-1} (x-x')     \right], \quad \theta=\{ \alpha, \Sigma \} \\
     \Sigma &amp;= \text{diag}(\sigma^2_0, \sigma^2_1, \ldots, \sigma^2_d)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The block matrices $k(X,X) \in \mathbb{R}^{N \times N}$
$ k(X, X_* ) \in \mathbb{R}^{N \times N_* }, $
$k( X_* , X ) \in \mathbb{R}^{ N_* \times N }$ and
$k(X_* , X_* ) \in \mathbb{R}^{N_* \times N_* }$ are the Gramian matrices of the training and test observations with respect to the kernel $k(x, x’)$.&lt;/p&gt;

&lt;p&gt;Furthermore both $k(X,X)$ and $k( X_* , X_* )$ are symmetric matrices and $k( X, X_* )$ and $k( X_* ,X)$ are each others mutually transposed.&lt;/p&gt;

&lt;p&gt;Given the joint distribution $ p(y_* , y, X_* , X) $, the aim for modeling the training and test observations with a GP is to derive the posterior distribution $ p( y_*  | y, X_* , X ) $ .
In order to derive the mean and covariance function of the posterior distribution, the block matrix inversion lemma is used to compute the inverse of the covariance matrix.&lt;/p&gt;

&lt;p&gt;For ease of reading and brevity the respective block matrices were replaced by more easily readible variables in the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K^{-1}&amp;= \begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1} \label{eq:blockmatrixinversionlemma1} \\
     &amp; =\begin{bmatrix}
          A &amp; B \\
          C &amp; D
     \end{bmatrix}^{-1} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\
          -(D-CA^{-1}B)^{-1}CA^{-1} &amp; (D-CA^{-1}B)^{-1}
     \end{bmatrix} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B\Sigma^{-1}CA^{-1} &amp; -A^{-1}B\Sigma^{-1} \\
          -\Sigma^{-1}CA^{-1} &amp; \Sigma^{-1}
     \end{bmatrix} \label{eq:Sigma^-1Identity} \\
     &amp;= \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix} \label{eq:blockmatrixinversionlemma-1} \\
     \Sigma &amp;= D-CA^{-1}B = K_{X_* X_* } - K_{ X_* X}{K_{ X_* X_* }}^{-1}K_{X X_* }
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Instead of computing the inverse of the entire matrix $K$, which can be computationally expensive for large covariance matrices, the precision matrix $K^{-1}$ can be computed block-wise with the block matrix inversion lemma.
Given the precision matrix in block matrix notation, the inner product in the exponential term of the Gaussian distribution can be computed as a sum over the inner products with the independent block matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* , y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{XX} &amp; K_{X X_* } \\
          K_{X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( y^TPy + y^TQ y_* + y_*^TRy + y_* ^TS y_*
     \right)
     \right] \label{eq:jointdist_innersumoverblockmatrices}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since we are only interested in the posterior distribution $p(y_*  | y, X_* , X )$, terms which do not include $ y_* $ can be moved into the normalization term.
The conditional distribution can thus be simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* |  y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -y^TQy_* - y_*^TRy + y_*^TS y_*
     \right)
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( -y^TA^{-1}B\Sigma^{-1} y_* -y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}K_{X_* X}{K_{ X X }}^{-1} y + y_*^T\Sigma^{-1}y_*
     \right)
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the matrices $\Sigma$ being a symmetric matrix by construction, and $B$ and $C$ being each other transposed, namely $C^T=B$, which gives rise to the identity:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     (y^TA^{-1}B\Sigma^{-1}y_*)^T
          &amp;= y_*^T(\Sigma^{-1})^TB^T(A^{-1})^Ty \\
          &amp;= y_*^T\Sigma^{-1}CA^{-1}y
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Alternatively one would argue that the result of both inner products yields the same scalar value due to $B=C^T$.
With the derivations above we obtain a posterior distribution $p(y_*  |  y, X_* , X )$ with the mean and covariance function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu(y_*)       &amp;= K_{ X_* X}{K_{XX}}^{-1}y \\
     \Sigma(y_*)    &amp;= K_{ X_* X_* } - K_{ X_* X}{K_{ X X }}^{-1}K_{ X X_*}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;It should be noted that during plotting only the diagonal entries of the covariance matrix are of interest since the diagonal entries of the covariance matrix denote the variances at the evaluated points.
Given the computation of both the mean and variance of the posterior distribution we obtain a Gaussian distribution:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* | y, X_*, X) &amp;= \mathcal{N} \big( \underbrace{K_{X_* X} {K_{XX}}^{-1} y}_{\mu}, \underbrace{K_{X_* X_*} - K_{X_* X}{K_{X X}}^{-1}K_{X X_*}}_{\Sigma} \big)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here is an image of a Gaussian Process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ludwigwinkler/BayesianOptimization/gh-pages/docs/GP_2Obs.png&quot; alt=&quot;Hi&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">A Tutorial for Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_2Obs.png" /></entry><entry><title type="html">RL in Challenging State Spaces and Sparse Reward Signals</title><link href="/blog/AdvRL/" rel="alternate" type="text/html" title="RL in Challenging State Spaces and Sparse Reward Signals" /><published>2017-11-06T00:00:00+01:00</published><updated>2017-11-06T00:00:00+01:00</updated><id>/blog/AdvRL</id><content type="html" xml:base="/blog/AdvRL/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Meetup on Nov 6, 2017.
It is somewhat more high level than usual since the audience came from a broad range of backgrounds.&lt;/p&gt;

&lt;p&gt;The content of the talk were ‘AlphaGo’, it’s successor ‘AlphaGo Zero’ and ‘Feudal Networks’ all published by DeepMind.
The talk can be downloaded &lt;a href=&quot;BML-AdvRL.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><summary type="html">Talk on AlphaGo and Hierarchical RL</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/AdvRL_Cover.png" /></entry><entry><title type="html">Introduction to Reinforcement Learning</title><link href="/blog/IntroRL/" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning" /><published>2017-05-03T00:00:00+02:00</published><updated>2017-05-03T00:00:00+02:00</updated><id>/blog/IntroRL</id><content type="html" xml:base="/blog/IntroRL/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Seminar hosted by Ben in the offices of &lt;a href=&quot;https://lateral.io&quot;&gt;Lateral&lt;/a&gt;.
It serves as an introduction to reinforcement learning covering the usual suspects like on- and off-policy algorithms, TD-learning and shows how neural networks were used as function approximators in deep reinforcement learning.&lt;/p&gt;

&lt;p&gt;The talk can be downloaded &lt;a href=&quot;BML-RL.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><summary type="html">A Tutorial Talk on RL</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/IntroRL_Cover.png" /></entry></feed>
