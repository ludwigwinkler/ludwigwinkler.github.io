<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-09T11:27:05+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">Tuscany &amp;amp; Rome</title><link href="http://localhost:4000/blog/Italy24/" rel="alternate" type="text/html" title="Tuscany &amp; Rome" /><published>2024-04-12T00:00:00+02:00</published><updated>2024-04-12T00:00:00+02:00</updated><id>http://localhost:4000/blog/Italy24</id><content type="html" xml:base="http://localhost:4000/blog/Italy24/">&lt;!-- ## Berlin Over The Years --&gt;

&lt;style&gt;
    .image-gallery {
        overflow: auto;
        margin-left: -1% !important;
    }

    .image-gallery li {
        float: left;
        float: top;
        display: block;
        margin: 0 0 1% 1%;
        width: 99%;
    }

    .image-gallery li a {
        text-align: top;
        text-decoration: none !important;
        color: #777;
    }

    .image-gallery li a span {
        display: block;
        text-overflow: ellipsis;
        overflow: hidden;
        white-space: nowrap;
        padding: 3px 0;
    }

    .image-gallery li a img {
        width: 100%;
        height: 100%;
        display: flex;
        vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240330-_DSC5526.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240330-_DSC5526.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240330-_DSC5536.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240330-_DSC5536.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240401-_DSC5707.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240401-_DSC5707.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240402-_DSC5761.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240402-_DSC5761.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240402-_DSC5768.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240402-_DSC5768.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240403-_DSC5933.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240403-_DSC5933.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240403-_DSC6063.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240403-_DSC6063.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240403-_DSC6082.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240403-_DSC6082.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240405-_DSC6454.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240405-_DSC6454.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240405-_DSC6476.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240405-_DSC6476.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240405-_DSC6514.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240405-_DSC6514.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240406-_DSC6582.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240406-_DSC6582.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy24/20240406-_DSC6592.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy24/20240406-_DSC6592.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;

&lt;!--  --&gt;

&lt;!--  --&gt;
&lt;p&gt;&lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;&lt;/p&gt;</content><author><name></name></author><category term="photography" /><summary type="html">Dolce Vita in a Fiat 500</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/italy24/20240406-_DSC6582.jpg" /></entry><entry><title type="html">Black-Scholes Equation for Options Pricing</title><link href="http://localhost:4000/blog/BlackScholes/" rel="alternate" type="text/html" title="Black-Scholes Equation for Options Pricing" /><published>2024-02-28T00:00:00+01:00</published><updated>2024-02-28T00:00:00+01:00</updated><id>http://localhost:4000/blog/BlackScholes</id><content type="html" xml:base="http://localhost:4000/blog/BlackScholes/">&lt;head&gt;
&lt;!-- &lt;script type=&quot;text/x-mathjax-config&quot;&gt;  --&gt;
  &lt;!-- MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt; --&gt;
&lt;!-- uncomment two lines above and remove the html css to svg lines --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: &quot;all&quot; } },
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
      displayMath: [['$$','$$'], ['\[' , '\]'], ['\\[', '\\]']],
      processEscapes: true
    },
    &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
    CommonHTML: { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
    });
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\Efunc}[1]{\mathbb{E}\left[ #1\right]}
\newcommand{\Vfunc}[1]{\mathbb{V}\left[ #1\right]}
\newcommand{\KL}[2]{\text{KL}\left[ #1 \ || \ #2 \right]}
\newcommand{\denom}[1]{\frac{1}{#1}}
\newcommand{\drift}{\mu(X_t, t)}
\newcommand{\diff}{\sigma(X_t, t)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The Black-Scholes equation is heralded as one rather important equations in finance, as it allows putting a precise price on an option &lt;em&gt;within the Black-Scholes framework&lt;/em&gt;.
It’s from economics, so as usual everything is assumed to be Gaussian distributed, statistically independent and linear. ;-)&lt;/p&gt;

&lt;p&gt;But first, let’s try to frame the problem we’re dealing with properly.
Namely,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;What is an Option?&lt;/li&gt;
  &lt;li&gt;Why are they of relevance?&lt;/li&gt;
  &lt;li&gt;Can we put a price tag on them like in Aldi?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-option&quot;&gt;The Option&lt;/h3&gt;

&lt;p&gt;An option in finance is a type of derivative contract that gives the buyer the right, but not the obligation, to buy or sell an underlying asset or instrument at a specified strike price prior to or on a specified date, depending on the form of the option. 
Derivatives in general are financial products which &lt;em&gt;derive&lt;/em&gt; their value from another, usual more basic financial product.
For example, you could take a kilo of gold and construct a derivative on the price of gold or the volatility of gold or inverse correlation to some interest rate or the correlation to another financial product.
Since there is no limit to the creativity how exactly your derivative contract &lt;em&gt;derives&lt;/em&gt; its value from the &lt;em&gt;underlying&lt;/em&gt; asset, it’s no wonder that Warren Buffet called them ‘financial weapons of mass destruction’.
Derivatives on housing prices in 2008 come to mind.
Options are widely used for various purposes, including hedging, speculation, and acquiring or disposing of assets at favorable prices. They can apply to numerous assets, like stocks, bonds, commodities, and currencies.&lt;/p&gt;

&lt;p&gt;There are two main types of options:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Call Options: Grant the holder the right to buy the underlying asset at the strike price within a specified period. Investors buy call options if they anticipate an increase in the asset’s price.&lt;/li&gt;
  &lt;li&gt;Put Options: Grant the holder the right to sell the underlying asset at the strike price within a specified period. Investors buy put options if they anticipate a decrease in the asset’s price.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The three important features of an option are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Premium: The price the option buyer pays to the seller for the rights granted by the option. After all you’re locking in preferential treatment at a later point in time.&lt;/li&gt;
  &lt;li&gt;Strike Price: The predetermined price at which the option buyer can buy (call) or sell (put) the underlying asset.&lt;/li&gt;
  &lt;li&gt;Expiration Date: The date by which the option must be exercised or it expires worthless.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-relevance-of-options&quot;&gt;The Relevance of Options&lt;/h3&gt;

&lt;p&gt;So let’s assume that a stock $S$ is worth $ \$ 100 $.&lt;/p&gt;

&lt;p&gt;Further assume that for the time being we price a call option at $ \$ 10 $ in one years time.
Thus if the stock rises above $ \$110 $, let’s say $ \$ 125$, we will make a profit of $ \$ 15 $ as we can buy the stock for $ \$ 100$ and immediately sell it for the $\$ 125$ and after subtracting the price of $\$ 10$ of our option, we have a profit of $\$15$. Nice, free money.
If the stock $S$ stays below $\$110$, let’s say $\$ 108$ dollars it doesn’t really make sense to buy the stock at all to quickly sell it again, as we would buy high and sell low which is exactly the opposite of making a profit. In that case we lost the $\$ 10$ we paid for the option in the beginning.&lt;/p&gt;

&lt;p&gt;But was the price of $\$ 10$ actually the correct price of the option?
Let’s establish the value of the option as $V(S, t)$ which depends on the price of the underlying stock (naturally) and the time index $t$ which usually measures the time until expiration of the option.
Next, we define our portfolio as $\Pi=\$110$ which is everything you have, be it stocks or cold, hard cash.&lt;/p&gt;

&lt;p&gt;We can assume that in a world of ever increasing stock prices (thanks, Fed) we want to buy a stock $S$ and protect it against loosing its value.
We can achieve this by buying a put option to sell the stock $S$ in one years time for the $\$100$ dollars we bought it for.
Therefore, if the stock crashes to $\$1$, we can still demand from the seller of the put option to buy the stock for the full $\$100$ dollars and make us whole.
For this peace of mind, we pay the seller of the put option the initial $\$ 10$.&lt;/p&gt;

&lt;p&gt;But the option itself has a value as well as they are tradeable and can be sold and bought on specialized markets.
Let a third person own the same stock and suddenly the stock crashes to $\$1$.
If the third person could get to own our option, he could buy our option for its value $V(S, t)$ and force the seller of the option to buy his basically worthless stock for the full $\$100$.
Thus it works as a sort of insurance to &lt;em&gt;hedge&lt;/em&gt; a portfolio against sudden movements.
The nice things about options is that we &lt;em&gt;can&lt;/em&gt; sell respectively buy the stock for a predetermined price, but we &lt;em&gt;don’t have to&lt;/em&gt;.
That is how we can hedge our portfolio, and the overarching framework is called dynamic hedging.
The dynamic part comes from us constantly selling and buying stocks $S$ and their options $V(S, t)$ in our portfolio $\Pi$ based on the behavior of the stock and our options.&lt;/p&gt;

&lt;p&gt;We will now introduce the term $\Delta = \partial_S V(S, t)$ to express the sensitivity of the worth of the option with regards to changes in the price of the underlying stock $S$.&lt;/p&gt;

&lt;p&gt;For a put option, this sensitivity is negative, $\Delta &amp;lt; 0$, since if the value of the stock $S$ is very low for an option with a high strike price, the put option itself becomes very valuable as it allows you to sell the worthless stock for the full agreed-upon price of $\$100$.
So $V(S, t)$ and $S$ are negatively linked with $\Delta &amp;lt; 0$ as any drop in $S$ increases $V(S, t)$.&lt;/p&gt;

&lt;p&gt;This allows us to offset the stock position $S$ in our portoflio $\Pi$ with put options $V(S, t)$ against possible drops in the value of the stocks!
Namely, we have a hedged portfolio
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\Pi 	&amp;= V(S, t) - \Delta S \\
	&amp;= V(S, t) - \partial_S V(S, t) \cdot S
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Let’s exemplify the behavior of $\Pi$.&lt;/p&gt;

&lt;p&gt;We have bought the stock $S$ for $\$100$ and hedged it with a put option $V(S, t)$ to be able to sell it in one years time for the same $\$100$.
Empirically, for put options for which the price of the underlying stock has dropped significantly from the strike price (the option is deep in the money), we can assume a $\Delta=-1$.
Now a year has passed and the price of the stock $S$ is at a paltry $\$1$.
The value of the option is the difference of the buy price of the underlying stock $S=\$1$ and the strike price of $\$100$ such that $V(S, t) = \$99$, which is the maximum profit you can make in this constellation of stock price and option value.
In cases where owning an option provides a significant advantage, for example the stock is very low for a put option with high strike price or the stock price is very high for a call option with low strike price, an option is called ‘in the money’.&lt;/p&gt;

&lt;p&gt;Since $S= \$1$ and $\Delta=-1$, we have
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\Pi 
&amp;= V(S, t) - \partial_S V(S, t) \cdot S \\
&amp;= \$99 - (-1) \cdot \$ 1 \\
&amp;= \$ 100
\end{align} %]]&gt;&lt;/script&gt;
and we see that our portfolio as a whole has not budged from the initial $\$100$ dollars (we omitted the option premium and transaction costs and the like to strengthen the intuition).&lt;/p&gt;

&lt;p&gt;Through the sensitivity of $\Delta = \partial_S V(S, t)$ we can construct a portfolio which is risk neutral.
Come hell or high water in the financial markets, by properly adapting the ratio of stocks and options in your portfolio you can whether the storm and keep your money.
Of course, this approach relies on an accurate estimation of the relevant sensitivity $\Delta$.
This approach is named &lt;em&gt;delta hedging&lt;/em&gt; after its main ingredient, $\Delta = \partial_S V(S, t)$.&lt;/p&gt;

&lt;h3 id=&quot;bob-the-financier-can-we-price-vs-t-yes-we-can--with-stochastic-calculus&quot;&gt;Bob, the financier: Can we price $V(S, t)$? Yes we can! … with stochastic calculus&lt;/h3&gt;

&lt;p&gt;Naturally, the value of the option and consequently the portfolio $\Pi$ is driven by the behavior of the underlying stock $S$.
The question thus arises of how exactly the price of the stock influences the price of the option and thus the portfolio itself.
Therefore we are interested in the change in the portfolio
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
d\Pi = dV(S, t) - \Delta \cdot dS
\end{align}&lt;/script&gt;
which is a differential equation which in turn is ‘driven’ by the differential $dV(S, t)$ and $dS$.
Importantly, the portfolio makes only sense to maintain if equals a return rate on the safest instruments, US treasury bonds.
If you’re making less with the fancy modelling than the US treasury bonds, this whole idea is not even worth pursuing.
A useful and truly hedged portfolio needs to return the same amount, as if we were to invest all the cash in $\Pi$ into Uncle Sams bonds.
Thus we have the following requirement
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
d\Pi = r \Pi dt
\end{align}&lt;/script&gt;
which is the compounding interest of the whole portfolio invested with US treasury bonds.&lt;/p&gt;

&lt;p&gt;One assumption of Black-Scholes model is that the stock price follows a geometric Brownian motion
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
dS_t = \mu S dt + \sigma S dW_t
\end{align}&lt;/script&gt;
such that large stock prices incur larger movements in both drift and diffusion.&lt;/p&gt;

&lt;p&gt;We are now interested in the dynamics of $dV(S, t)$ which hinges on the behavior of $S$ caputred by the SDE above.&lt;/p&gt;

&lt;p&gt;Ito’s Lemma, derived and explained in more detail &lt;a href=&quot;https://ludwigwinkler.github.io/blog/ItosLemma/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://ludwigwinkler.github.io/blog/FokkerPlanck/&quot;&gt;used for the derivation of the FPE here&lt;/a&gt;, gives us a straight forward equation for this:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
dV(S, t) = \left(\partial_t V(S, t) + \mu S \partial_S V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt + \sigma S \partial_S V(S, t) dW_t.
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Now, we can plug in both the terms of $dV(S, t)$ and $dS$ to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
d\Pi = &amp; \left(\partial_t V(S, t) + \mu S \partial_S V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt + \sigma S \partial_S V(S, t) dW_t \\
&amp; - \Delta \cdot \left( \mu S dt + \sigma S dW_t\right) \\
= &amp; \left(\partial_t V(S, t) + \mu S \partial_S V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt + \sigma S \partial_S V(S, t) dW_t \\
&amp; - \partial_S V(S, t) \cdot \left( \mu S dt + \sigma S dW_t\right) \\
= &amp; \left(\partial_t V(S, t) + \color{blue}{\mu S \partial_S V(S, t)} + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt + \color{red}{\sigma \partial_S V(S, t) dW_t} \\
&amp; - \color{blue}{\partial_S V(S, t) \mu S dt} - \color{red}{\partial_S V(S, t)\sigma S dW_t} \\
= &amp; \left(\partial_t V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt
\end{align} %]]&gt;&lt;/script&gt;
which is particularly interested as there is no diffusion term which indicates absolut deterministic dynamics as far as the portfolio dynamics are concerned.
Since we equated the portfolio dynamics at a minimum to the risk free returns of government bonds, we equate another time to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
r ( V(S, t) - \partial_S V(S, t) \cdot S) dt
= &amp; \left(\partial_t V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t) \right) dt \\
r V(S, t) - r \partial_S V(S, t) \cdot S
= &amp; \partial_t V(S, t) + \frac{1}{2} \sigma^2 S^2 \partial^2_S V(S, t)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Thus, if we find a function $V(S, t)$ which for any value of $S$ and at any time $t$ fulfills the partial differential equation above, we will have a portfolio that generates at a minimum the risk free return and is hedged at all times.
Consequentially, this also tells us the price of the specific option $V(S, t)$ given a specific stock price $S$.&lt;/p&gt;</content><author><name></name></author><summary type="html">Optional Derivatives</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/blackscholes.png" /></entry><entry><title type="html">Simple Reverse-Time SDE Derivation for Diffusion Models</title><link href="http://localhost:4000/blog/SimpleReverseSDE/" rel="alternate" type="text/html" title="Simple Reverse-Time SDE Derivation for Diffusion Models" /><published>2024-01-13T00:00:00+01:00</published><updated>2024-01-13T00:00:00+01:00</updated><id>http://localhost:4000/blog/SimpleReverseSDE</id><content type="html" xml:base="http://localhost:4000/blog/SimpleReverseSDE/">&lt;head&gt;
&lt;!-- &lt;script type=&quot;text/x-mathjax-config&quot;&gt;  --&gt;
  &lt;!-- MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt; --&gt;
&lt;!-- uncomment two lines above and remove the html css to svg lines --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: &quot;all&quot; } },
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
      displayMath: [['$$','$$'], ['\[' , '\]'], ['\\[', '\\]']],
      processEscapes: true
    },
    &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
    CommonHTML: { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
    });
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\Efunc}[1]{\mathbb{E}\left[ #1\right]}
\newcommand{\Vfunc}[1]{\mathbb{V}\left[ #1\right]}
\newcommand{\KL}[2]{\text{KL}\left[ #1 \ || \ #2 \right]}
\newcommand{\denom}[1]{\frac{1}{#1}}
\newcommand{\drift}{\mu(X_t, t)}
\newcommand{\diff}{\sigma(X_t, t)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We start out with the Fokker-Planck equation (FPE) which relates the change over time for the probability for a specific value of $x$ with a diffusion term $\sigma(t)$ which is only dependent on the time,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_t \ p(x,t) = &amp; - \partial_x \left[ \drift \ p(x, t) \right] +  \partial_x^2 \left[ \denom{2} \sigma(t)^2 \ p(x, t) \right]  \\
	=                     &amp; - \partial_x \left[ \drift \ p(x, t) \right] + \denom{2} \sigma(t)^2 \partial_x^2 \left[ \ p(x, t) \right].
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The FPE describes the evolution of the entire probability distribution of a stochastic process.
We can simulate a single particle by defining the stochastic differential equation (SDE),
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
dX_t = \mu(X_t, t) dt + \sigma(t) dW_t
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The importance of the FPE is its holistic approach of modelling the change of an basically infinite large ensemble of particles governed by the SDE above.
Thus whereas simulating a single particle is nice and good, solving the FPE gives us the distribution of trajectories in a single go.
So obtaining the FPE is of the harder, but more rewarding task as it gives us the underlying probability distribution over time and space instead of a bunch of trajectories.&lt;/p&gt;

&lt;p&gt;Now we consider a time reversion $\tau(t) = 1 - t$ and are interested in what the change of the probability distribution is under this reversed time index,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_{t} \ p(x,\tau(t)) = &amp; - \partial_x \left[ \mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t)) \right]       \\
	                              &amp; + \denom{2} \partial_x^2 \left[ \sigma(\tau(t))^2 \ p(x, \tau(t)) \right].
\end{align} %]]&gt;&lt;/script&gt;
With the time transformation $\tau(t)$, we apply the chain rule on the time transformation on the left hand side to obtain
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\frac{\partial p(x,\tau(t))}{\partial t}
	= \frac{ \partial p(x,\tau(t))}{\partial \tau} \ \frac{\partial \tau(t)}{\partial t}
	= \frac{ \partial p(x,\tau(t))}{\partial \tau} \
	\underbrace{ \frac{\partial \tau(t)}{\partial t}}_{-1}
	= -\frac{ \partial p(x,\tau(t))}{\partial \tau}.
\end{align}&lt;/script&gt;
Then, we pull the negative factor from the chain rule to the right hand side and combine the drift and diffusion term into a single derivative via the distributive property of the partial derivative,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{ \partial p(x,\tau(t))}{\partial \tau} = &amp; \partial_x \left[ \mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t)) \right] - \denom{2} \sigma(\tau(t))^2 \partial_x^2 \left[ \ p(x, \tau(t)) \right]  \label{eq:app_reversetime_derivation1} \\
	=                                              &amp; - \partial_x \Bigg[ -\mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t)) + \denom{2} \sigma(\tau(t))^2 \partial_x \left[ \ p(x, \tau(t)) \right] \Bigg]
\end{align} %]]&gt;&lt;/script&gt;
Applying the log derivative identity $\partial_x \log p(x) = \frac{1}{p(x)} \partial_x p(x)$, which rearranged yields $ \partial_x p(x) = p(x) \partial_x \log p(x)$, we obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{ \partial p(x,\tau(t))}{\partial \tau} = &amp; - \partial_x \Bigg[ -\mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t))                            \nonumber                                                                             \\
	                                               &amp; \qquad \quad + \denom{2} \sigma(\tau(t))^2 \partial_x \log p(x, \tau(t)) p(x, \tau(t) ) \Bigg]                                                                                 \\
	=                                              &amp; - \partial_x \Bigg[ \Big( \underbrace{-\mu(X(\tau(t)), \tau(t)) + \denom{2} \sigma(\tau(t))^2 \partial_x \log p(x, \tau(t))}_{\text{reverse drift}} \Big) p(x, \tau(t))\Bigg].
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The equation above states that the inverted drift with an additional scaled score term of the forward distribution will invert the stochastic process.
If we read off the corresponding SDE we get
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
dX(\tau) = \left\{-\mu(X(\tau(t)), \tau(t)) + \denom{2} \sigma(\tau(t))^2 \partial_x \log p(x, \tau(t)) \right\} dt
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, there is no diffusion term occuring in this formulation of the reverse FPE (loud, surprised gasp! How shocking, dear!).
We can in fact derive a more flexible reverse drift by returning to a slightly rearranged equation \ref{eq:app_reversetime_derivation1} with an additional, ‘neutral’ (because &lt;script type=&quot;math/tex&quot;&gt;\pm&lt;/script&gt;) scaling factor $\alpha^2$.
In the equations below, we incorporate the negative and the positive side of the additional diffusion in separate ways:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{ \partial p(x,\tau(t))}{\partial \tau} = &amp; - \partial_x \left[ - \mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t)) \right]  - \denom{2} \sigma(\tau(t))^2 \partial_x^2 \left[ \ p(x, \tau(t)) \right]        \nonumber                                                                                                            \\
	                                               &amp; \underbrace{\pm \ \frac{\alpha^2}{2} \ \sigma(\tau(t))^2 \partial_x^2 \left[ \ p(x, \tau(t)) \right]}_{\text{additional diffusion}} \\
	=                                              &amp; - \partial_x \left[ - \mu(X(\tau(t)), \tau(t)) \ p(x, \tau(t)) \right]  
								- \denom{2} \sigma(\tau(t))^2 \left( 1 + \alpha^2 \right) \partial_x^2 \left[ \ p(x, \tau(t)) \right]                                                                                           \\
	                                               &amp; \underbrace{ + \ \frac{\alpha^2}{2} \ \sigma(\tau(t))^2 \partial_x^2 \left[ \ p(x, \tau(t)) \right]}_{\text{additional diffusion}}                                                                  \\
	=                                              &amp; - \partial_x \Bigg[ \Big( - \mu(X(\tau(t)), \tau(t))                                                                                                                                                \\
	                                               &amp; \qquad \qquad + \denom{2} \sigma(\tau(t))^2 \left( 1 + \alpha^2 \right) \partial_x \log p(x, \tau(t)) \Big) \ p(x, \tau(t)) \Bigg]         \nonumber                                            \\
	                                               &amp; + \ \partial_x^2 \left[ \frac{\alpha^2}{2} \ \sigma(\tau(t))^2 \ p(x, \tau(t)) \right].
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;from which we can infer the reverse drift consisting of the inverted original drift with the additionally scaled score with $\alpha$ and the additional diffusion,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	dX(\tau) = &amp; \left\{ - \mu(X(\tau(t)), \tau(t)) + \denom{2} \sigma(\tau(t))^2 \left( 1 + \alpha^2 \right) \partial_x \log p(x, \tau(t)) \Big) \ p(x, \tau(t)) \right\} dt \\
	           &amp; + \alpha \ \sigma(\tau(t)) dW(\tau)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In the intuition is quite clear: If we add extra diffusion to our forward process, we will ‘diffuse’ more and the probability mass will be distributed over a larger, more spread out area.
Therefore, if we want to invert this particular stochastic process, we need to increase the score term dependent on the &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; which pushes the particles back into the high probability region.&lt;/p&gt;</content><author><name></name></author><summary type="html">Save yourself a lot of Bayes with a linear function</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/simple_reverse_sde.png" /></entry><entry><title type="html">Fokker, Planck &amp;amp; Ito</title><link href="http://localhost:4000/blog/FokkerPlanck/" rel="alternate" type="text/html" title="Fokker, Planck &amp; Ito" /><published>2023-10-21T00:00:00+02:00</published><updated>2023-10-21T00:00:00+02:00</updated><id>http://localhost:4000/blog/FokkerPlanck</id><content type="html" xml:base="http://localhost:4000/blog/FokkerPlanck/">&lt;head&gt;
&lt;!-- &lt;script type=&quot;text/x-mathjax-config&quot;&gt;  --&gt;
  &lt;!-- MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt; --&gt;
&lt;!-- uncomment two lines above and remove the html css to svg lines --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: &quot;all&quot; } },
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
      displayMath: [['$$','$$'], ['\[' , '\]'], ['\\[', '\\]']],
      processEscapes: true
    },
    &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
    CommonHTML: { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
    });
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\Efunc}[1]{\mathbb{E}\left[ #1\right]}
\newcommand{\Vfunc}[1]{\mathbb{V}\left[ #1\right]}
\newcommand{\KL}[2]{\text{KL}\left[ #1 \ || \ #2 \right]}
\newcommand{\denom}[1]{\frac{1}{#1}}
\newcommand{\drift}{\mu(X_t, t)}
\newcommand{\diff}{\sigma(X_t, t)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;A Dutch, a German and a Japanese walk into a bar …&lt;/p&gt;

&lt;p&gt;Let us consider the random variable &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt; that follows an Ito drift-diffusion process of the form
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	dX_t = \drift dt + \diff dW_t
\end{align}&lt;/script&gt;
where &lt;script type=&quot;math/tex&quot;&gt;W_t&lt;/script&gt; is a Wiener process with &lt;script type=&quot;math/tex&quot;&gt;W_t \sim \mathcal{N}(0, t)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We want to study an arbitrary function &lt;script type=&quot;math/tex&quot;&gt;f(X_t)&lt;/script&gt; with a compact support, meaning that &lt;script type=&quot;math/tex&quot;&gt;f(X_t)=0, X_t \in \{ -\infty, \infty \}&lt;/script&gt;.
Intuitively, this means that for the extreme values of &lt;script type=&quot;math/tex&quot;&gt;\pm \infty&lt;/script&gt; the function &lt;script type=&quot;math/tex&quot;&gt;f(X_t)&lt;/script&gt; evaluates to zero. 
The function &lt;script type=&quot;math/tex&quot;&gt;f(X_t)&lt;/script&gt; should be twice differentiable in its argument &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt; such that we can use the Taylor expansion up to the second order, giving us 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	df = \partial_x f(X_t) dX_t + \denom{2} \partial_x^2 f(X_t) dX_t^2.
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For the infinitissimal values &lt;script type=&quot;math/tex&quot;&gt;dt&lt;/script&gt;, any term with an exponent higher than one will go towards zero at a faster rate.
Thus the terms &lt;script type=&quot;math/tex&quot;&gt;dt^2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;dt dW_t = dt^{1.5}&lt;/script&gt; will evaluate to zero at the limit.
We can then plug in the dynamics of &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt; to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	df(X_t) = &amp; \partial_x f(X_t) dX_t + \denom{2} \partial_x^2 f(X_t) dX_t^2 \\
	= &amp; \partial_x f(X_t) \left( \drift dt + \diff dW_t \right) + \denom{2} \partial_x^2 f(X_t) \left(\drift dt + \diff dW_t \right)^2 \\
	= &amp; \partial_x f(X_t) \left( \drift dt + \diff dW_t \right) \\
	&amp; + \denom{2} \partial_x^2 f(X_t) \big( \drift^2 \underbrace{dt^2}_{=0} + \drift \diff \underbrace{ dt \ dW_t}_{=0} + \diff^2 \underbrace{dW_t^2}_{=dt} \big) \\
	= &amp;\left(\drift \partial_x f(X_t) + \denom{2} \diff^2 \partial_x^2 f(X_t) \right) dt + \diff \partial_x f(X_t) dW_t 
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can abbreviate the notation to enable a higher degree of notational brevity and write
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	df = \left( \mu \partial_x f + \denom{2} \sigma^2 \partial_x^2 f \right) dt + \sigma \partial_x f dW_t
\end{align}&lt;/script&gt;
which is identical to the line above but shorter and less cluttered.&lt;/p&gt;

&lt;p&gt;We can easily see that the differential &lt;script type=&quot;math/tex&quot;&gt;df&lt;/script&gt; follows an Ito drift-diffusion process, although with modified drift and diffusion terms in direct comparison to &lt;script type=&quot;math/tex&quot;&gt;dX_t&lt;/script&gt;.
Naturally we can take the expectation of to isolate the drift of &lt;script type=&quot;math/tex&quot;&gt;df&lt;/script&gt; since &lt;script type=&quot;math/tex&quot;&gt;\Efunc{dW_t}=0&lt;/script&gt; by definition,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\Efunc{df} &amp; = \Efunc{ \mu \partial_x f + \denom{2} \sigma^2 \partial_x^2 f } dt \\
	\frac{d}{dt} \Efunc{f} &amp; = \Efunc{ \mu \partial_x f + \denom{2} \sigma^2 \partial_x^2 f }
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Since the Wiener process &lt;script type=&quot;math/tex&quot;&gt;W_t&lt;/script&gt; introduces stochasticity into the evolution of &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;, we are in fact dealing with a distribution &lt;script type=&quot;math/tex&quot;&gt;p(x, t)&lt;/script&gt;.
We can then proceed by plugging in the distribution &lt;script type=&quot;math/tex&quot;&gt;p(x, t)&lt;/script&gt; into the expectation and writing it out in its full glory,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{d}{dt} \Efunc{f} &amp; = \Efunc{ \mu \partial_x f + \denom{2} \sigma^2 \partial_x^2 f } \\
	&amp;= \int_{-\infty}^\infty \left( \mu \partial_x f + \denom{2} \sigma^2 \partial_x^2 f \right) p(x, t) dx \\
	&amp;= \int_{-\infty}^\infty \mu \ \partial_x f \ p(x, t) dx + \denom{2} \int_{-\infty}^\infty \sigma^2 \ \partial_x^2 f \ p(x, t) dx
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The state so far is that we reduced the expected change in &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; to two integrals which we now have to solve.
For this we can utilize integration by parts which is the sort of the anti derivative of the product rule.
Remember that
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\partial_x \left[ u(x) v(x) \right] = \partial_x \left[  u(x) \right] v(x) + u(x) \partial_x \left[ v(x) \right]
\end{align}&lt;/script&gt;
or in a easier form
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\left( u(x) v(x) \right)' = u'(x) v(x) + u(x) v'(x)
\end{align}&lt;/script&gt;
The integration by parts rule states that for a range &lt;script type=&quot;math/tex&quot;&gt;x \in [ a, b ]&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\left[ u(x) v(x) \right]_a^b = \int_a^b u'(x) v(x) dx + \int_a^b u(x) v'(x) dx
\end{align}&lt;/script&gt;
or alternatively
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\int_a^b u(x) v'(x) dx = \left[ u(x) v(x) \right]_a^b - \int_a^b u'(x) v(x) dx + 
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can now proceed to identify the relevant terms &lt;script type=&quot;math/tex&quot;&gt;u(x)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;v(x)&lt;/script&gt; in the two integrals,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{d}{dt} \Efunc{f} = &amp; \int_{-\infty}^\infty \underbrace{\mu \ p(x, t)}_{u(x)} \ \underbrace{\partial_x f}_{v'(x)}  dx + \denom{2} \int_{-\infty}^\infty \underbrace{ \sigma^2 \ p(x, t)}_{u(x)} \ \underbrace{\partial_x^2 f}_{v
	(x)} dx \\
	= &amp; \underbrace{\left[  \mu \ p(x, t)  \  f \right]_{-\infty}^\infty}_{=0} - \int_{-\infty}^\infty  \partial_x \left[ \mu \ p(x, t) \right] \ f \ dx \\
	&amp; + \denom{2} \underbrace{\left[  \sigma^2 \ p(x, t)  \  \partial_x f \right]_{-\infty}^\infty}_{=0} - \denom{2} \int_{-\infty}^\infty  \partial_x \left[ \sigma^2 \ p(x, t) \right] \ \partial_x f \ dx
\end{align} %]]&gt;&lt;/script&gt;
For any reasonable probability distribution, evaluating &lt;script type=&quot;math/tex&quot;&gt;p(x,t)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;\pm \infty&lt;/script&gt; evaluates to zero such that the evaluation brackets &lt;script type=&quot;math/tex&quot;&gt;\left[ p(x,t) \ldots \right]_{-\infty}^\infty = 0&lt;/script&gt;.
We can then apply the integration by parts a second time on the second integral to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{d}{dt} \Efunc{f} = &amp; - \int_{-\infty}^\infty  \partial_x \left[ \mu \ p(x, t) \right] \ f \ dx - \denom{2} \int_{-\infty}^\infty  \underbrace{\partial_x \left[ \sigma^2 \ p(x, t) \right]}_{u(x)} \ \underbrace{\partial_x f}_{v'(x)} \ dx \\
	= &amp; \int_{-\infty}^\infty  \partial_x \left[ \mu \ p(x, t) \right] \ f \ dx \\
	&amp; - \denom{2} \underbrace{\left[ \partial_x \left[ \sigma^2 \ p(x, t) \right] \ f \right]_{-\infty}^\infty}_{=0} + \denom{2} \int_{-\infty}^\infty  \partial_x^2 \left[ \sigma^2 \ p(x, t) \right] \ f \ dx \\
	= &amp; \int_{-\infty}^\infty f \left( - \partial_x \left[ \mu \ p(x, t) \right] + \denom{2} \partial_x^2 \left[ \sigma^2 \ p(x, t) \right] \right) dx
\end{align} %]]&gt;&lt;/script&gt;
With Leibniz’ rule we can pull in the time derivative on the left hand side to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{d}{dt} \Efunc{f} = &amp; \frac{d}{dt} \int_{-\infty}^\infty f(x) p(x,t) dx \\
	=&amp; \int_{-\infty}^\infty f(x) \ \partial_t \ p(x,t) dx
\end{align} %]]&gt;&lt;/script&gt;
which gives us
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\int_{-\infty}^\infty f(x) \ \partial_t \ p(x,t) dx = \int_{-\infty}^\infty f \left( - \partial_x \left[ \mu \ p(x, t) \right] + \denom{2} \partial_x^2 \left[ \sigma^2 \ p(x, t) \right] \right) dx
\end{align}&lt;/script&gt;
The last step to obtain the Fokker-Planck equation is to observe that the function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; which is integrated over occurs both on the left and the right hand side.
Since the integrals &lt;script type=&quot;math/tex&quot;&gt;\int f(x) \ldots dx&lt;/script&gt; is identical on both sides we can equate the derivatives directly to obtain
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\partial_t \ p(x,t) = - \partial_x \left[ \mu \ p(x, t) \right] + \denom{2} \partial_x^2 \left[ \sigma^2 \ p(x, t) \right]
\end{align}&lt;/script&gt;
which is the Fokker-Planck equation!&lt;/p&gt;</content><author><name></name></author><summary type="html">Fokker-Planck Equation Via Ito Calculus</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/fokkerplanckito.png" /></entry><entry><title type="html">Israel to Jordan</title><link href="http://localhost:4000/blog/IsraelJordan23/" rel="alternate" type="text/html" title="Israel to Jordan" /><published>2023-10-14T00:00:00+02:00</published><updated>2023-10-14T00:00:00+02:00</updated><id>http://localhost:4000/blog/IsraelJordan23</id><content type="html" xml:base="http://localhost:4000/blog/IsraelJordan23/">&lt;!-- ## Berlin Over The Years --&gt;
&lt;p&gt;For a compilation of (legally unauthorized) 4K drone footage of the beautiful Wadi Rum check out this &lt;a href=&quot;https://www.youtube.com/watch?v=9-UedQjFBew&quot;&gt;Youtube link&lt;/a&gt;.&lt;/p&gt;

&lt;style&gt;
    .image-gallery {
        overflow: auto;
        margin-left: -1% !important;
    }

    .image-gallery li {
        float: left;
        float: top;
        display: block;
        margin: 0 0 1% 1%;
        width: 99%;
    }

    .image-gallery li a {
        text-align: top;
        text-decoration: none !important;
        color: #777;
    }

    .image-gallery li a span {
        display: block;
        text-overflow: ellipsis;
        overflow: hidden;
        white-space: nowrap;
        padding: 3px 0;
    }

    .image-gallery li a img {
        width: 100%;
        height: 100%;
        display: flex;
        vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231001-_DSC3720.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231001-_DSC3720.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231002-_DSC3839.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231002-_DSC3839.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231003-_DSC3857.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231003-_DSC3857.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231003-_DSC3925.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231003-_DSC3925.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231003-_DSC3962.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231003-_DSC3962.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231005-_DSC4279.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231005-_DSC4279.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231006-_DSC4356.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231006-_DSC4356.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4371.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4371.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4395.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4395.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4508.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4508.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4594.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4594.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4802.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4802.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231008-_DSC4948.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231008-_DSC4948.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231009-_DSC5257.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231009-_DSC5257.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231009-_DSC5294.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231009-_DSC5294.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/israeljordan23/20231009-_DSC5325.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/israeljordan23/20231009-_DSC5325.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;

&lt;!--  --&gt;

&lt;!--  --&gt;
&lt;p&gt;&lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;
  &lt;!-- 
 --&gt;&lt;/p&gt;</content><author><name></name></author><category term="photography" /><summary type="html">Tel Aviv, Jerusalem and Wadi Rum</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/israeljordan23/20231009-_DSC5294.jpg" /></entry><entry><title type="html">Fokker, Planck &amp;amp; Kolmogorov Revisited</title><link href="http://localhost:4000/blog/Kramers/" rel="alternate" type="text/html" title="Fokker, Planck &amp; Kolmogorov Revisited" /><published>2023-07-21T00:00:00+02:00</published><updated>2023-07-21T00:00:00+02:00</updated><id>http://localhost:4000/blog/Kramers</id><content type="html" xml:base="http://localhost:4000/blog/Kramers/">&lt;head&gt;
&lt;!-- &lt;script type=&quot;text/x-mathjax-config&quot;&gt;  --&gt;
  &lt;!-- MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt; --&gt;
&lt;!-- uncomment two lines above and remove the html css to svg lines --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: &quot;all&quot; } },
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
      displayMath: [['$$','$$'], ['\[' , '\]'], ['\\[', '\\]']],
      processEscapes: true
    },
    &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
    CommonHTML: { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
    });
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;A Dutch, a German and a Russian walk into a bar …&lt;/p&gt;

&lt;p&gt;At the core of the partial differential equations that will describe the change of a distribution both forward and backward in time lies the Chapman-Kolmogorov equation
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t + \tau}) &amp; = \int p(x_{t + \tau} , x'_{t}) \ dx'_t \\
	&amp; = \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t
\end{align} %]]&gt;&lt;/script&gt;
which simply expands over an auxiliary variable $x’_t$ while simultaneously marginalizing it out and factorizing the joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau}, x'_t)&lt;/script&gt; into a conditional distribution.
The conditional distribution above states that we can start from any &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; and by moving to &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt; with the right transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x_{t + \tau} | x'_{t})&lt;/script&gt; we will obtain the correct marginal distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We will assume a stochastic differential equation the first two orders of which can be estimated with
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	M^{(n)} (x'_t) = \int (x_{t+\tau} - x'_t)^n  p(x_{t + \tau} | x'_t) dx_{t+\tau}
\end{align}&lt;/script&gt;
such that the dynamics are described by the Ito drift-diffusion process
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	dX'_t = &amp; M^{(1)}(X'_t) dt + M^{(2)}(X'_t) dW_t \\
	= &amp; \mu(X'_t, t) dt + \sigma(X'_t, t) dW_t \\
\end{align} %]]&gt;&lt;/script&gt;
with the Wiener process &lt;script type=&quot;math/tex&quot;&gt;W_t&lt;/script&gt;.
The important part is to note the ‘direction’ of the differential which is evaluated strictly forward in time.
We take &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; as a sort of origin point which doesn’t change and weight the differential &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau} - x'_t&lt;/script&gt; by the appropriate transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau} | x'_t)&lt;/script&gt; over every possible &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;forward-equation&quot;&gt;Forward Equation&lt;/h3&gt;

&lt;p&gt;The Chapman-Kolmogorov equation fro the forward Kramers-Moyal expansion can be rewritten with the help of an auxilliary variable as
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t + \tau}) = &amp; \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t \\
	= &amp; \int_{X'} \int_{Y} \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} p(x'_t) dx'_t
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The main component of the Kramers-Moyal expansions is the use of the Taylor expansion on a shifted function.
The classical Taylor expansion $T_{f,a}(x)$ of a function $f(x)$ around a root point $a$ says that we can reconstruct the function $f(x)$ with an infinite sum consisting of its derivatives
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	T_{f,a}(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;What happens if we introduce an offset $h$ to the root point $a$ which can take on any value we want?
It turns out that the arbitrary offset $h$ can directly be used as a distance measure to the root point.
The Taylor expansion of the shifted function $f(a + h)$ then becomes
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	T_{f,a}(h) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} h^n
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can then first expand the delta function $\delta ( y_{t+\tau} - x_{t+\tau} )$ with $\pm x_t$ and subsequently expand the Taylor series to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\delta(y_{t+\tau} - x_{t+\tau}) = &amp; \delta(\overbrace{y_{t+\tau} - x'_t}^{h} + \overbrace{x'_t - x_{t+\tau}}^{a} ) \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} \underbrace{\partial_{x'}^{n} \delta(x'_t - x_{t+\tau}) }_{f^{(n)}(a)} \underbrace{( y_{t+\tau} - x'_t)^n}_{h^n}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can plug the expanded Taylor series back in to get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t + \tau}) 
= &amp; \int_{X'} \int_{Y} \overbrace{\delta(y_{t+\tau} - x_{t+\tau})}^{\text{Taylor Expansion}} p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
= &amp; \int_{X'} \int_{Y} \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'}^{n} \left[ \delta(x'_t - x_{t+\tau}) \right] p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
= &amp; \sum_{n=0}^\infty \frac{1}{n!} \int_{X'} \int_{Y} ( y_{t+\tau} - x'_t)^n \ \underbrace{ \partial_{x'}^{n} \left[ \delta(x'_t - x_{t+\tau})\right]}_{?} p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Now we need to find out, how to deal with the $n$’th derivative of Dirac delta function inside an integral.&lt;/p&gt;

&lt;h3 id=&quot;a-quick-intermezzo-on-delta-functions-and-integration-by-parts&quot;&gt;A Quick Intermezzo on Delta Functions and Integration by Parts&lt;/h3&gt;

&lt;p&gt;In my work so far, I got the feeling that two mathematical tricks provide the most joy to physicists when they don’t know how to proceed: 1) Taylor Expansions and 2) Integration by Parts.
We already encountered the Taylor expansion, so get ready to observe the second trick, integration by parts (IbP).&lt;/p&gt;

&lt;p&gt;It should be first stated that the delta is a weird beast which is more or less only defined where it’s argument is zero.
For $\delta(0)=1$ and $\delta(x)=0, x \neq 0$.
For an integral of the product of $\int \delta(x=x’) f(x) dx = f(x’)$, the Dirac delta function serves as a sort of ‘selector’ which ignores any contribution of the integral except where $x=x’$ because only when $x=x’$ will it be one, otherwise zero.&lt;/p&gt;

&lt;p&gt;Fortunately, we can yield the physicists favourite magic spell when dealing with probabilities: Integration by Parts.
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\int_{x=-\infty}^\infty u'(x) v(x) dx = \left[ u(x) v(x) \right]_{x=-\infty}^\infty - \int_{x=-\infty}^\infty u(x) v'(x) dx
\end{align}&lt;/script&gt;
which is the just integrating a rearranged product rule $(u(x) v(x))’ = u’(x) v(x) + u(x) v’(x)$ over the domain of $x$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\int_a^b (u(x) v(x))' dx &amp;= \int_a^b u'(x) v(x) dx + \int_a^b u(x) v'(x) dx \\
&amp; \Updownarrow \\
\int u'(x) v(x) dx &amp;= \left[ u(x) v(x) \right]_{x=a}^b - \int_a^b u(x) v'(x) dx
\end{align} %]]&gt;&lt;/script&gt;
where the integration cancels only the derivative of the product as the others are a product of a derivative and another function.&lt;/p&gt;

&lt;p&gt;If the function $f(x)$ has compact support, meaning that ${f(x)=0 | x = \pm \infty }$, the the evaluation of the derivative-free component in integration by parts vanishes.
So if we’re dealing with a probability distribution which is assumed to be zero at the far ends, we obtain the simplified term
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\int_{x=-\infty}^\infty \delta'(x) f(x) dx = \underbrace{\left[ \delta(x) f(x) \right]_{x=-\infty}^\infty}_{=0} - \int_{x=-\infty}^\infty \delta(x) f'(x) dx
\end{align}&lt;/script&gt;
For higher order derivatives this generalizes to
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\int_{x=-\infty}^\infty \delta^{(n)} (x) f(x) dx = \sum_k^{n-1} \left[ (-1)^k \delta^{(k)}(x) f^{(n-k)}(x) \right]_{x=-\infty}^\infty + (-1)^n \int_{x=-\infty}^\infty \delta(x) f^{(n)}(x) dx
\end{align}&lt;/script&gt;
which more or less moves the all the derivatives from the Dirac delta function over to $f(x)$ which is often called the ‘test function’ when inside an integral over the full domain.
Since all the derivates and the function $f$ itself are zero at $x=\pm \infty$, $f^{(n)}(\pm \infty) = f(\pm \infty)=0 \ \forall n \in \mathbb{N}$, the term simplifies to
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\int_{x=-\infty}^\infty \delta^{(n)} (x) f(x) dx &amp;= \underbrace{\sum_k^{n-1} \left[ (-1)^k \delta^{(k)}(x) f^{(n-k)}(x) \right]_{x=-\infty}^\infty}_{f^{(n)}(\pm \infty) = f(\pm \infty)=0} + (-1)^n \int_{x=-\infty}^\infty \delta(x) f^{(n)}(x) dx \\
&amp;= (-1)^n \int_{x=-\infty}^\infty \delta(x) f^{(n)}(x) dx
\end{align} %]]&gt;&lt;/script&gt;
which we will utilize in our further derivation.&lt;/p&gt;

&lt;p&gt;Intermezzo is over.&lt;/p&gt;

&lt;h3 id=&quot;continuing-with-kramers-moyal&quot;&gt;Continuing with Kramers-Moyal&lt;/h3&gt;

&lt;p&gt;We now have 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t + \tau}) 
= &amp; \sum_{n=0}^\infty \frac{1}{n!} \int_{X'} \int_{Y} ( y_{t+\tau} - x'_t)^n \ \underbrace{ \partial_{x'}^{n} \left[ \delta(x'_t - x_{t+\tau})\right]}_{?} p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
\end{align} %]]&gt;&lt;/script&gt;
which we rearrange to
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t + \tau}) 
= &amp; \sum_{n=0}^\infty \frac{1}{n!} \int_{X'} \partial_{x'}^{n} \left[ \delta(x'_t - x_{t+\tau})\right] \underbrace{\int_{Y} ( y_{t+\tau} - x'_t)^n \ p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau}}_{M^{(n)}(x'_t)} \ p(x'_t) dx'_t \\
= &amp; \sum_{n=0}^\infty \frac{1}{n!} \underbrace{ \int_{X'} \underbrace{ \partial_{x'}^{n} \left[ \delta(x'_t - x_{t+\tau})\right]}_{u'(x)} \underbrace{M^{(n)}(x'_t) \ p(x'_t)}_{v(x)} dx'_t }_{\text{Integration by Parts with Dirac delta function}} \\
= &amp; \sum_{n=0}^\infty \frac{1}{n!} (-1)^n \int_{X'} \delta(x'_t - x_{t+\tau}) \partial_{x'}^{n}  \left[ M^{(n)}(x'_t) \ p(x'_t) \right] dx'_t\\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The delta function &lt;script type=&quot;math/tex&quot;&gt;\delta(x'_t - x_{t+\tau})&lt;/script&gt; is only one if the values of the two &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;’s is the same &lt;em&gt;irrespective of time&lt;/em&gt;.
It thus serves as a selector which reduces to the integral over the domain &lt;script type=&quot;math/tex&quot;&gt;X'&lt;/script&gt; to a single evaluation at the numerical value of &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt;.
So we get the following sum which we expand up to the second order
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t + \tau}) 
= &amp; \sum_{n=0}^\infty \frac{1}{n!} (-1)^n \partial_{x}^{n}  \left[ M^{(n)}(x_t) \ p(x_t) \right]\\
= &amp; p(x_t) - \partial_{x} \left[ M^{(1)}(x_t) \ p(x_t) \right] + \frac{1}{2} \partial_{x}^2  \left[ M^{(2)}(x_t) \ p(x_t) \right] + \mathcal{O}(n^3)\\
\end{align} %]]&gt;&lt;/script&gt;
where the $n=0$ eliminates most of the operators in the first summand.
Pulling &lt;script type=&quot;math/tex&quot;&gt;p(x_t)&lt;/script&gt; to the left side and finding that the change between &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(x_t)&lt;/script&gt; should be proportional to &lt;script type=&quot;math/tex&quot;&gt;\partial_t p(x_t) \tau&lt;/script&gt; for a small step size &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; analogously to an Euler discretization, we obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t+\tau}) - p(x_t) = &amp; \partial_t p(x_t) \tau \\
	\frac{p(x_{t+\tau}) - p(x_t)}{\tau} = &amp; \partial_t p(x_t).
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Finally we can note that we can could cut off the Taylor expansion after the second order and realize that Taylor expansion is equivalent to the time derivative in the limit of time, i.e. &lt;script type=&quot;math/tex&quot;&gt;\lim_{\tau \rightarrow 0}&lt;/script&gt; and we can proclaim that
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_t p(x_t) = &amp; - \partial_{x_t} \left[ M^{(1)}(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[M^{(2)}(x_t) p(x_t) \right] \\
	= &amp; - \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) p(x_t) \right] \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;!-- $$
\begin{align}
	\partial_t p(x_t) = &amp; - \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) p(x_t) \right] \\
\end{align}
$$ --&gt;

&lt;h3 id=&quot;backward-equation&quot;&gt;Backward Equation&lt;/h3&gt;

&lt;p&gt;The Kolmogorov backward equation (KBE) can be derived in the same way while paying attention to the derivatives.&lt;/p&gt;

&lt;p&gt;In the Kolmogorov forward equation the differential operators &lt;script type=&quot;math/tex&quot;&gt;M^{(n)}&lt;/script&gt; were defined for for stochastic variables following the natural arrow of time.
For the backward expansion of &lt;script type=&quot;math/tex&quot;&gt;p(x_t | x'_{t'})&lt;/script&gt; we will use differential operators on &lt;script type=&quot;math/tex&quot;&gt;x'_{t'}&lt;/script&gt;.
We thus have the ordering of time &lt;script type=&quot;math/tex&quot;&gt;t' \leq t' + \tau \leq t&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Again we start with the Chapman-Kolmogorov equation and insert an intermediate variable &lt;script type=&quot;math/tex&quot;&gt;x''_{t+\tau}&lt;/script&gt;:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	p(x_t | x'_t) = \int p(x_t | x''_{t'+\tau}) p(x''_{t'+\tau} | x'_{t'}) dx''_{t'+\tau}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We expand the transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x''_{t'+\tau} | x'_{t'})&lt;/script&gt; again with a Dirac function 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	p(x''_{t'+\tau} | x'_{t'}) = \int \delta(y_{t'+\tau} - x''_{t'+\tau}) p(y_{t'+\tau} | x'_{t'}) dy_{t'+\tau}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then we expand the Dirac function with the Taylor series just as in the Forward Kolmogorov equation to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\delta(y_{t'+\tau} - x''_{t'+\tau})
	= &amp; \delta( y_{t'+\tau} - x'_{t'} + x'_{t'} - x''_{t'+\tau}) \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} (y_{t' + \tau} - x'_{t'})^n \ \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau})
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Plugging the expanded Dirac function back into the transition probability we obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x''_{t'+\tau} | x'_{t'}) = &amp; \int \delta(y_{t'+\tau} - x''_{t'+\tau}) p(y_{t'+\tau} | x_{t'}) dy_{t+\tau} \\
	= &amp; \int \sum_{n=0}^\infty \frac{1}{n!} (y_{t' + \tau} - x'_{t'})^n \ \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau}) p(y_{t'+\tau} | x_{t}) dy_{t'+\tau} \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau}) \int (y_{t' + \tau} - x'_t)^n  p(y_{t'+\tau} | x_{t'}) dy_{t'+\tau} \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau}) M^{(n)}(x'_{t'}) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We now plug in the Taylor expansion to substitute &lt;script type=&quot;math/tex&quot;&gt;p(x''_{t'+\tau} | x'_{t'})&lt;/script&gt; in our original master equation and get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_t | x'_t) 
	&amp;= \int p(x_t | x''_{t'+\tau}) p(x''_{t'+\tau} | x'_{t'}) dx''_{t'+\tau} \\
	&amp;= \int p(x_t | x''_{t'+\tau}) \sum_{n=0}^\infty \frac{1}{n!} \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau}) M^{(n)}(x'_{t'}) dx''_{t'+\tau} \\
	&amp;= \sum_{n=0}^\infty \frac{1}{n!} M^{(n)}(x'_{t'}) \int p(x_t | x''_{t'+\tau}) \partial_{x'}^n \ \delta(x'_{t'} - x''_{t'+\tau}) dx''_{t'+\tau} \\
\end{align} %]]&gt;&lt;/script&gt;
where we pulled out the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and the &lt;script type=&quot;math/tex&quot;&gt;M^{(n)}(x'_{t'})&lt;/script&gt; as they are independent of the integral over &lt;script type=&quot;math/tex&quot;&gt;x''_{t'+\tau}&lt;/script&gt;.
Given our derivation of the KFE, we would be quick to reapply integration by parts, but it turns out that the distribution inside the integral doesn’t contain &lt;script type=&quot;math/tex&quot;&gt;x'_{t'}&lt;/script&gt; and therefore we can pull the derivative out of the integral.
Since a Dirac delta function inside an integral is just a selector, we eliminate the integral and obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_t | x'_t) 
	&amp;= \sum_{n=0}^\infty \frac{1}{n!} M^{(n)}(x'_{t'}) \ \partial_{x'}^n \int p(x_t | x''_{t'+\tau}) \ \delta(x'_{t'} - x''_{t'+\tau}) dx''_{t'+\tau} \\
	&amp;= \sum_{n=0}^\infty \frac{1}{n!} M^{(n)}(x'_{t'}) \ \partial_{x'}^n p(x_t | x'_{t'+\tau}) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Expanding the sum up to the second order 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_t | x'_t) 
	&amp;= p(x_t | x'_{t'+\tau}) + M^{(1)}(x'_{t'}) \ \partial_{x'} p(x_t | x'_{t'+\tau}) + \frac{1}{2} M^{(2)}(x'_{t'}) \ \partial_{x'}^2 p(x_t | x'_{t'+\tau}) \\
\end{align} %]]&gt;&lt;/script&gt;
and taking the limit of &lt;script type=&quot;math/tex&quot;&gt;\tau \rightarrow 0&lt;/script&gt; while discretizing it a la Euler, we get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_t | x'_t) - p(x_t | x'_{t'+\tau})
	&amp;= M^{(1)}(x'_{t'}) \ \partial_{x'} p(x_t | x'_{t'+\tau}) + \frac{1}{2} M^{(2)}(x'_{t'}) \ \partial_{x'}^2 p(x_t | x'_{t'+\tau}) \\
	- (p(x_t | x'_{t'+\tau}) - p(x_t | x'_t))
	&amp;= M^{(1)}(x'_{t'}) \ \partial_{x'} p(x_t | x'_{t'}) \ \tau + \frac{1}{2} M^{(2)}(x'_{t'}) \ \partial_{x'}^2 p(x_t | x'_{t'}) \ \tau \\
\end{align} %]]&gt;&lt;/script&gt;
and by dividing by &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;, we finally get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	- \frac{p(x_t | x'_{t'+\tau}) - p(x_t | x'_t)}{\tau}
	&amp;= - \partial_{x'} p(x_t | x'_{t'}) \\
	&amp;= M^{(1)}(x'_{t'}) \ \partial_{x'} p(x_t | x'_{t'}) + \frac{1}{2} M^{(2)}(x'_{t'}) \ \partial_{x'}^2 p(x_t | x'_{t'}) \\
	&amp;= \mu(x'_{t'}) \ \partial_{x'} p(x_t | x'_{t'}) + \frac{1}{2} \sigma^2(x'_{t'}) \ \partial_{x'}^2 p(x_t | x'_{t'}) \\
\end{align} %]]&gt;&lt;/script&gt;
which tells us how the probability distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_t | x'_{t'})&lt;/script&gt; changes as we move further backwards in time.&lt;/p&gt;</content><author><name></name></author><summary type="html">Distributions as partial differential equations over time</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/kramersmoyal.png" /></entry><entry><title type="html">Fast Fourier Transform</title><link href="http://localhost:4000/blog/FastFourierTransform/" rel="alternate" type="text/html" title="Fast Fourier Transform" /><published>2023-04-11T00:00:00+02:00</published><updated>2023-04-11T00:00:00+02:00</updated><id>http://localhost:4000/blog/FastFourierTransform</id><content type="html" xml:base="http://localhost:4000/blog/FastFourierTransform/">&lt;head&gt;
&lt;!-- &lt;script type=&quot;text/x-mathjax-config&quot;&gt;  --&gt;
  &lt;!-- MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt; --&gt;
&lt;!-- uncomment two lines above and remove the html css to svg lines --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: &quot;all&quot; } },
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
      displayMath: [['$$','$$'], ['\[' , '\]'], ['\\[', '\\]']],
      processEscapes: true
    },
    &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
    CommonHTML: { linebreaks: { automatic: true } },
    SVG: { linebreaks: { automatic: true } }
    });
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\def\tr#1{\text{Tr}\left[ #1 \right]}
 \def\Efunc#1{\mathPbb{E}\left[ #1\right]}
 \def\Efuncc#1#2{\mathbb{E}_{#1}\left[ #2 \right]}
 \def\red#1{\textcolor{red}{#1}}
 \def\blue#1{\textcolor{blue}{#1}}
 \newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;During the Easter holiday at my parents house, I discovered a 800 page digital signal processings book in my dad’s book shelf.
Leafing through the pages and examining the table of contents, I saw a whole section on the Fast Fourier Transform (FFT) which has been called by many scientists one of the most important algorithms of our time.&lt;/p&gt;

&lt;p&gt;To quote ‘What a Wonderful World’ by Louis Armstrong:&lt;/p&gt;

&lt;p&gt;🎼 &lt;em&gt;‘And I think to myself, what a wonderful (world) algorithm …‘&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But how does this apparently ‘super-duper important algorithm’ actually work?
That’s when I went down a rabbit whole full of complex exponentials, recursion and nifty tricks for periodic functions.&lt;/p&gt;

&lt;h3 id=&quot;why-were-going-to-what-were-going-to-do&quot;&gt;Why we’re going to what we’re going to do&lt;/h3&gt;

&lt;p&gt;If you went looking for an explanation of the Fast Fourier Transform (FFT) you already know how frequencies, Fourier transformations and complex numbers relate to each other.
One thing to point out is that FFT is an algorithm to compute the Discrete Fourier Transform (DFT) efficiently.
So I’m going to skip the introduction on the continuous Fourier transformation and the like.&lt;/p&gt;

&lt;h3 id=&quot;eulers-identity&quot;&gt;Euler’s Identity&lt;/h3&gt;

&lt;p&gt;It always amazes me how a few mathematicians 200-300 years ago laid the theoretical groundwork for so many ideas that we use today on a daily basis.
The names of Gauss, Euler, Fourier and Laplace have appeared so many times in my studies and work that you tend to forgot that those people did their work with a candle on their desk because the use of electricity developed yet, while the fundamental basics of electrical engineering stands on the shoulders of their enormous mathematical contributions.&lt;/p&gt;

&lt;p&gt;My understanding of the FFT started out with the innocuous equation known as Euler’s Identity:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
  e^{\pm i x} = \cos x \pm i sin x
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We’re fairly familiar with the standard &lt;script type=&quot;math/tex&quot;&gt;e^x&lt;/script&gt; which is just the exponential function but once we introduce the complex number $i$ into the fray, things get really, not exactly weird, but circly and trigonometric.&lt;/p&gt;

&lt;p&gt;The real exponential can be rewritten as
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
  e^{x} = \sum_{k=0}^\infty \frac{x^k}{k!}
\end{align}&lt;/script&gt;
and if we add the complex number $i$ as an argument modifier, it starts to modify the entire equation:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  e^{x} 
  &amp;= \sum_{k=0}^\infty \frac{(ix)^k}{k!} \\
  &amp;= \sum_{k=0}^\infty i^k \frac{x^k}{k!}.
\end{align} %]]&gt;&lt;/script&gt;
Now we have to deal with infinite power of the complex number $i$ but it turns out it reduces to just four numbers,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
  i^0 &amp;= 1                  &amp;&amp;  i^4 = i^3 \cdot i = -i \cdot i  &amp;&amp; = 1  &amp;&amp; \ldots\\
  i^1 &amp;= i                  &amp;&amp;  i^5 = i^4 \cdot i = 1 \cdot i   &amp;&amp; = i   &amp;&amp; \ldots\\
  i^2 &amp;= -1                 &amp;&amp;  i^6 = i^5 \cdot i = i \cdot i   &amp;&amp; = -1  &amp;&amp; \ldots\\
  i^3 &amp;= i \cdot i^2 = -i   &amp;&amp;  i^7 = i^6 \cdot i = -1 \cdot i  &amp;&amp; = -i &amp;&amp; \ldots \\
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;the computation of which we can recycle every fourth power with the modulo operator $k \% 4$.&lt;/p&gt;

&lt;p&gt;We can now write out the sum for a couple of terms and see whether a convenient structure reveals itself (spoiler alert: it does):
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  e^{ix} 
  &amp;= 1 + i x - \frac{x^2}{2!} - i \frac{x^3}{3!} + \frac{x^4}{4!} + i \frac{x^5}{5!} - \frac{x^6}{6!} - i \frac{x^7}{7!} + \ldots \\
  &amp;= \underbrace{1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \ldots}_{\text{Real}} + \underbrace{i \left( x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} \ldots \right)}_{\text{Imaginary}} \\
  &amp;= \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n} + i \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $i$’th power switches the sign of the terms and whether they’re complex or not according to the table a few lines above.&lt;/p&gt;

&lt;p&gt;Fortunately for us, it just so happens that the MacLaurin series (Taylor series when the root is zero) for the sine and cosine are precisely the power series that occured in the equation above, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  e^{ix} 
  &amp;= 1 + i x - \frac{x^2}{2!} - i \frac{x^3}{3!} + \frac{x^4}{4!} + i \frac{x^5}{5!} - \frac{x^6}{6!} - i \frac{x^7}{7!} + \ldots \\
  &amp;= \underbrace{1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \ldots}_{\text{Real}} + \underbrace{i \left( x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} \ldots \right)}_{\text{Imaginary}} \\
  &amp;= \underbrace{\sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n}}_{\text{MacLaurin Series: } \cos(x)} + i \underbrace{\sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1}}_{\text{MacLaurin Series:} \sin(x)} \\
  &amp;= \cos(x) + i \sin(x)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;I like to think of complex numbers as endowing a linear term with an additional, magic dimension which can interact with the real dimension in convenient ways.
While complex numbers live in a ‘2D’ real and imaginary space with two dimensions, quaternions take it to a whole new level and allow working with multiple dimensions, all enclosed in linear terms.
The advantage is that a possibly complex rotation around the origin is just a simple addition for complex numbers.
So instead of constructing complicated rotation matrices all you have to do is multiply two linear terms where the imaginary respectively quaternions take care of the cross-dimensional interactions.&lt;/p&gt;

&lt;h3 id=&quot;the-nth-root-of-unity-or-walking-around-a-circle-in-n-steps&quot;&gt;The n’th Root of Unity (or walking around a circle in N steps)&lt;/h3&gt;

&lt;p&gt;The second ingredient to the Discrete Fourier Transform (DFT) is walking in a circle in the complex plane.
Let us define a function $w(n)$ with $0 \leq n \leq N$
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
w(n) 
&amp;= e^{-i 2 \pi \frac{n}{N}} \\
&amp;= \cos\left(2 \pi \frac{n}{N} \right) - i \sin \left(2 \pi \frac{n}{N} \right)
\end{align} %]]&gt;&lt;/script&gt;
where we can conclude that the ratio lies in $0 \leq \frac{n}{N} \leq 1$ since $0 \leq n \leq N$.
We know that a full rotation of a sine and cosine function in terms of radians is defined in the range $[0, 2 \pi]$.
If we plug the fraction $\frac{n}{N}$ as multiplicative factor in front of the $2\pi$ then we will move from $0$ to $2\pi$ in exactly $n$ steps.
Plugging $2 \pi \frac{n}{N}$ into the sine and cosine function lets us do a full rotation in the complex plane in precisely $n$ steps.&lt;/p&gt;

&lt;p&gt;The term $e^{-i 2 \pi \frac{n}{N}}$ is called the n’th root of unity, although I prefer to think of it as doing a full rotation with a radius of one in the complex plane in $n$ steps.&lt;/p&gt;

&lt;h3 id=&quot;the-star-of-the-show-the-discrete-fourier-transform&quot;&gt;The Star of the Show: The Discrete Fourier Transform&lt;/h3&gt;

&lt;p&gt;Let’s get the horse out of the barn and define the Discrete Fourier Transform (DFT) as
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \sum_{n=0}^{N-1} x_n e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}} \\
\end{align} %]]&gt;&lt;/script&gt;
which for a signal of length $N$ computes $0 \leq k \leq N-1$ frequency components (where the $N-1$ arises from zero indexing which we use to denote the offset of the signal at $k=0$).&lt;/p&gt;

&lt;p&gt;In my undergraduate signals and systems courses, I liked to think of Fourier analysis algorithms as computing the ratio between a signal and a composite frequency by dropping the complex part below the main signal:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \sum_{n=0}^{N-1} x_n e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}} \\
  &amp;= \sum_{n=0}^{N-1} \frac{x_n}{e^{i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}}} \\
\big( \text{contribution of frequency} &amp;= \frac{\text{signal}}{\text{frequency}} \big)
\end{align} %]]&gt;&lt;/script&gt;
An alternative intuition for me is the that the Fourier transform computes the correlation of a signal with a series of sinusoids with increasing frequency.
If a signal aligns perfectly with a particular sinusoid, it will yield a high correlation score, whereas if the signal is completely orthogonal, it has zero correlation and will the correlation score is zero.&lt;/p&gt;

&lt;p&gt;One technical peculiarity, which is determined by the Nyquist-Shannon sampling theorem, is that we can have only half the number of frequency bins vis-a-vis the signal length.
The discrete signal $x_n$ is by definition a sampling frequency, as the underlying continuous signal is sampled/measured at discrete steps.
Nyquist-Shannon says that you need twice the sampling frequency for a signal in order to be sure that you have a ‘definite’ representation of the signal.
For a signal of length N, we can only have $N/2$ present frequencies which we can accurately measure.&lt;/p&gt;

&lt;p&gt;I spend a good hour figuring out why the DFT of a real signal is always mirrored in the frequency bins.
Most explanations on the internet only plot the real part of the spectrum, which is precisely where that detail matters.
Unsurprisingly for a real signal, this equates to plotting
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
X_k 
&amp;= \text{Re}\left( \sum_{n=0}^{N-1} x_n e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}} \right) \\
&amp;= \text{Re} \left( \sum_{n=0}^{N-1} x_n \left( \cos\left(2 \pi k \frac{n}{N} \right) - i \sin \left(2 \pi k \frac{n}{N} \right) \right) \right) \\
&amp;= \sum_{n=0}^{N-1} x_n \cos \left(2 \pi k \frac{n}{N} \right).
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Considering that the cosine function is periodic by definition it is mirrored at points $(… -2\pi, -\pi, 0, \pi, 2\pi, …)$.
Said differently, 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\cos(x) = \cos(2\pi - x), \quad 0 \leq x \leq 2\pi
\end{align}&lt;/script&gt;
which repeats for arbitrary integer multiples of 2 in either directions as $\cos$ is an even, periodic function from $0$ to $2\pi$.
So we have
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{Re}\left( X_k \right) 
&amp;\stackrel{!}{=} \text{Re}\left( X_{N-k} \right) \\
\sum_{n=0}^{N-1} x_n \cos \left(2 \pi k \frac{n}{N} \right) 
&amp; = \sum_{n=0}^{N-1} x_n \cos \left(2 \pi (N-k) \frac{n}{N} \right) \\
&amp; = \sum_{n=0}^{N-1} x_n \cos \left(2 \pi n -2 \pi k \frac{n}{N} \right) \\
&amp; = \sum_{n=0}^{N-1} x_n \cos \left( - 2 \pi k \frac{n}{N} \right) \\
&amp; = \sum_{n=0}^{N-1} x_n \cos \left(2 \pi k \frac{n}{N} \right) \\
\end{align} \\ %]]&gt;&lt;/script&gt;
where since $n$ is an integer, the $2 \pi n$ term is just a full period in the cosine function and the for an even function such as the cosine, $f(x)=f(-x)$, the negative sign is in the RHS cosine function is identical to its positive sign.&lt;/p&gt;

&lt;p&gt;If the signal $x_n$ is complex, this feature does not hold anymore, as complex $i \sin$ terms would interact with complex parts of the signal $x_n$ resulting in ‘spill overs’ into the real dimension as the multiplication of two imaginary numbers results in a real number.&lt;/p&gt;

&lt;h3 id=&quot;the-cooler-sibling-of-the-dft-the-fast-fourier-transform&quot;&gt;The cooler sibling of the DFT: The Fast Fourier Transform&lt;/h3&gt;

&lt;p&gt;One big drawback of the DFT is that it scales quadratically with the signal length, requiring $N^2$ operations for a signal of length $N$.
Remember that 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \sum_{n=0}^{N-1} x_n e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}} \\
\end{align} %]]&gt;&lt;/script&gt;
which implies that for each $0 \leq k \leq N$ frequencies we need to add $N$ terms in the summation with additional multiplications of complex numbers.
For a real signal, we can save the second half, $N/2 \leq k \leq N$ as its a symmetric evaluation, but that doesn’t hold for complex signals.&lt;/p&gt;

&lt;p&gt;The solution is a divide and conquer algorithm which heavily exploits the specific structure of the $n$’th root of unities in the complex exponential.
Remember that the naive implementation would cost us $O(N^2)$ computations for a signal of length $N$.
We won’t get around examining every entry $n$ in the signal so one $N$ has to stay, because otherwise we would skip possibly essential data in the signal.
But we can exploit the cyclic structure in the $n$’th root of unity (aka the complex exponential) and reuse computations to reduce the $N$ in the frequencies to a $\log N$ to get an $O(N \log N)$ algorithm.&lt;/p&gt;

&lt;h3 id=&quot;an-algebraic-approach&quot;&gt;An Algebraic Approach&lt;/h3&gt;

&lt;p&gt;Let’s consider the DFT for a particular frequency $X_k$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \sum_{n=0}^{N-1} x_n e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{n}{N}} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which we can rewrite to equivalently by dividing the even and odd numbered entries in the signal $x_n$ to
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \sum_{m=0}^{N/2-1} \underbrace{x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m}{N}} }_{\text{even DFT computations of $x_n$}} + \sum_{m=0}^{N/2-1} \underbrace{ x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m+1}{N}} }_{\text{odd DFT computations of $x_n$}} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This split into even and odd entries is valid, as we might only go from $[0, …, m, …, N/2]$ but we compensate for that by scaling the index from $m$ to $2m$.&lt;/p&gt;

&lt;p&gt;Next we split off the $+1$ in the complex exponential in the odd DFT computations and drop the $2$ below the fraction in the complex exponential to get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
X_k 
&amp;= \sum_{m=0}^{N/2-1} \underbrace{x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m}{N}} }_{\text{even DFT computations of $x_n$}}  + \sum_{m=0}^{N/2-1} \underbrace{ x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m+1}{N}} }_{\text{odd DFT computations of $x_n$}} \\
&amp;= \sum_{m=0}^{N/2-1}x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m}{N}} + e^{-i 2 \pi \frac{k}{N}} \sum_{m=0}^{N/2-1} x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{2m}{N}} \\
&amp;= \sum_{m=0}^{N/2-1}x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} + e^{-i 2 \pi \frac{k}{N}} \sum_{m=0}^{N/2-1} x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} \\
&amp;= \text{DFT}(\text{even}(x_n), k, N/2) + e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(x_n), k, N/2) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and right here is the key insight that each summand has the exact same functional form of a DFT that we defined earlier, except for a multiplication for the odd entries and the shortened index $m$.&lt;/p&gt;

&lt;p&gt;But we furthermore exploit the periodicity of the sinusoids, which we already encounter in the frequency spectrum of real signals above.
An even function is more or less defined as $f(x) = f(-x)$ and an odd function as $f(x) = - f(-x)$.&lt;/p&gt;

&lt;p&gt;Now it just so happens that the cosine is an even function and the sinus is an odd function for a single period.
But since we defined our signal of length $N$ over a single period of $2 \pi$ (for specific integer multiple $k$, but that’s not important for the intuition), we now that for a cosine we only have to compute ${0, \ldots, k, \ldots, N/2}$  since ${N/2, \ldots, k, \ldots, N}$ is handed to us on a platter due to the even property of the cosine function.&lt;/p&gt;

&lt;p&gt;The same applies to the sinus function which we can mirror for all always of ${N/2, \ldots, k, \ldots, N}$ by adjusting it with the complex exponential.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_k 
  &amp;= \text{DFT}(\text{even}(x_n), k, N/2) + e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(x_n), k, N/2) \\
  X_{k + \frac{N}{2}}
  &amp;= \text{DFT}(\text{even}(x_n), k, N/2) - e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(x_n), k, N/2) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can show this more rigorously by writing out $X_{k + \frac{N}{2}}$ to get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_{k + \frac{N}{2}} 
  &amp;= \sum_{m=0}^{N/2-1}x_{2m} e^{-i 2 \pi \ \cdot \ (k + \frac{N}{2}) \ \cdot \ \frac{m}{N/2}} + e^{-i 2 \pi \frac{(k+ \frac{N}{2})}{N}} \sum_{m=0}^{N/2-1} x_{2m+1} e^{-i 2 \pi \ \cdot \ (k + \frac{N}{2}) \ \cdot \ \frac{m}{N/2}} \\
  &amp;= \sum_{m=0}^{N/2-1}x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} \underbrace{\ e^{-i2\pi \ m}}_{=1 + i 0 = 1} + e^{-i 2 \pi \frac{k}{N}} \underbrace{e^{-i\pi}}_{=-1} \sum_{m=0}^{N/2-1} x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} \ \underbrace{e^{-i2\pi \ m}}_{= 1 + i0 = 1} \\
  &amp;= \sum_{m=0}^{N/2-1}x_{2m} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} - e^{-i 2 \pi \frac{k}{N}} \sum_{m=0}^{N/2-1} x_{2m+1} e^{-i 2 \pi \ \cdot \ k \ \cdot \ \frac{m}{N/2}} \\
  &amp;= \text{DFT}(\text{even}(x_n), k, N/2) - e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(x_n), k, N/2) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where the complex exponentials evaluate accordingly due to $m$ being integers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The log scaling of the FFT algorithm stems from the fact that we can split the computation into even and odd terms which are again DFT’s, and that each application of a DFT gives us the second half of the frequency bins for for free (with a simple array addition and one complex multiplication).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I said earlier that we can’t save ourselves from going through our signal at least once, but by exploiting the symmetry properties of the sinusoids, we already halved the computations for the $k$ frequency bins in half.
So while we have to do a full ‘spatial’ pass of $n$ over the $x_n$’s, we can save ourselves half the time by mirroring in the ‘frequency’ pass for the index $k$.&lt;/p&gt;

&lt;p&gt;We can play this game again to get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  \text{DFT}(\text{even}(x_n), k, N/2) 
  &amp;= \text{DFT}(\text{even}(\text{even}(x_n)), k, N/4)+ e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(\text{even}(x_n)), k, N/4) \\
  &amp; + e^{-i 2 \pi \frac{k}{N}} \left( \text{DFT}(\text{even}(\text{odd}(x_n)), k, N/4)+ e^{-i 2 \pi \frac{k}{N}} \ \text{DFT}(\text{odd}(\text{odd}(x_n)), k, N/4) \right) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where we realize with &lt;em&gt;a sudden burst of clarity&lt;/em&gt; that for every recursive DFT split, we only have to compute the first half of the frequencies, as the second half can be reconstructed from the already computed values in the first half.&lt;/p&gt;

&lt;!-- I find it especially nice to observe that for every subsampling/splitting into even and odd terms we effectively half the sampling frequency.
While we nominally compute the frequency bin $k$, by doing steps, we reduce the frequency by a factor of two.
But we observe that for each split into an even lower sampling frequency ( like from $N/2$ to $N/4$ in the second application of the DFT), we multiply the split terms by what was originally called the 'twiddle' factor $\exp(-i2 \pi k/N)$.
As we half the sampling resolution by taking every second entry, we have to keep multiplying these complex exponentials, which double the frequency, thus correcting for the reduced sampling frequency.

For the example above, we have $\exp(-i 2\pi k/N) \cdot \exp(-i 2\pi k/N) = \exp(-i2 \pi \ 2 k/N)$ after the second application of the  ra --&gt;

&lt;h3 id=&quot;a-linear-algebraic-approach&quot;&gt;A (Linear) Algebraic Approach&lt;/h3&gt;

&lt;p&gt;Things are always better when visualized and fortunately we can write out the DFT as a matrix-vector multiplication.
For this efficient algorithn, we require that the length $N$ is a root of 2, such that we can repeatedly divide the length $N$ by two until we arrive at a DFT length of one.&lt;/p&gt;

&lt;p&gt;First we will define the $n$’th root of unity as
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
w_N = e^{-\frac{i 2 \pi}{N}} \qquad w_N^{n \cdot k} = e^{-i 2 \pi \ k \ \frac{n}{N}}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can now construct the full $K \times N$ matrix where each row of the matrix corresponds to a particular frequency $k$ and where naturally $K=N$.
For a signal of length $N=8$ this gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
0 \leq k \leq N/2: \\ \\ \\ \\ N/2 \leq k \leq N: \\
\end{matrix} \begin{bmatrix}
w^0 &amp; w^0 &amp; w^0 &amp; w^0 &amp; w^0 &amp; w^0 &amp; w^0 &amp; w^0\\
w^0 &amp; w^1 &amp; w^2 &amp; w^3 &amp; w^4 &amp; w^5 &amp; w^6 &amp; w^7 \\
w^0 &amp; w^2 &amp; w^4 &amp; w^6 &amp; w^8 &amp; w^{10} &amp; w^{12} &amp; w^{14} \\
w^0 &amp; w^3 &amp; w^6 &amp; w^9 &amp; w^{12} &amp; w^{15} &amp; w^{18} &amp; w^{21} \\
\hline
w^0 &amp; w^4 &amp; w^8 &amp; w^{12} &amp; w^{16} &amp; w^{20} &amp; w^{24} &amp; w^{28} \\
w^0 &amp; w^5 &amp; w^{10} &amp; w^{15} &amp; w^{20} &amp; w^{25} &amp; w^{30} &amp; w^{35} \\
w^0 &amp; w^6 &amp; w^{12} &amp; w^{18} &amp; w^{24} &amp; w^{30} &amp; w^{36} &amp; w^{42} \\
w^0 &amp; w^7 &amp; w^{14} &amp; w^{21} &amp; w^{28} &amp; w^{35} &amp; w^{42} &amp; w^{49} \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;We use this matrix as a projection for our signal $x_n$ to obtain the frequency bins.
Doing this naively would cost us exactly $8 \times 8 = 64$ operations&lt;/p&gt;

&lt;p&gt;But what do know about the roots of unity that we can exploit?
While it may appear that the roots of unity are just increasing haphazardly, there are many duplicates &lt;em&gt;as the complex exponentials are circular&lt;/em&gt;.
This means that for example $w_8^2=w_8^{10}$ as with a length of $N=8$, $w_8^{10}$ does a full circle back to $w_8^2$ (it requires ‘8 steps’ to do a full circle and has ‘2 steps’ left to end up where $w^2$ is already, so the modulo operator respectively the periodic property of the complex exponential).
Similarly $w_8^4=w_8^{12}$.&lt;/p&gt;

&lt;p&gt;Additionally, our astute obeservation of earlier tells that we don’t need to compute the second half of the matrix, but can instead reconstruct it from the results in the upper half.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix} 0 \leq k \leq N/2: \\ \\ \\ \\ N/2 \leq k \leq N: \\ \end{matrix} 
\begin{bmatrix}
w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0\\
w_8^0 &amp; w_8^1 &amp; w_8^2 &amp; w_8^3 &amp; w_8^4 &amp; w_8^5 &amp; w_8^6 &amp; w_8^7 \\
w_8^0 &amp; w_8^2 &amp; w_8^4 &amp; w_8^6 &amp; w_8^8 &amp; w_8^{10} &amp; w_8^{12} &amp; w_8^{14} \\
w_8^0 &amp; w_8^3 &amp; w_8^6 &amp; w_8^9 &amp; w_8^{12} &amp; w_8^{15} &amp; w_8^{18} &amp; w_8^{21} \\
\hline
w_8^0 &amp; w_8^4 &amp; w_8^8 &amp; w_8^{12} &amp; w_8^{16} &amp; w_8^{20} &amp; w_8^{24} &amp; w_8^{28} \\
w_8^0 &amp; w_8^5 &amp; w_8^{10} &amp; w_8^{15} &amp; w_8^{20} &amp; w_8^{25} &amp; w_8^{30} &amp; w_8^{35} \\
w_8^0 &amp; w_8^6 &amp; w_8^{12} &amp; w_8^{18} &amp; w_8^{24} &amp; w_8^{30} &amp; w_8^{36} &amp; w_8^{42} \\
w_8^0 &amp; w_8^7 &amp; w_8^{14} &amp; w_8^{21} &amp; w_8^{28} &amp; w_8^{35} &amp; w_8^{42} &amp; w_8^{49} \\
\end{bmatrix} \\
\\
\qquad \qquad \qquad \Big \downarrow \\
\\
\begin{matrix}
0 \leq k \leq N/2: \\
\\
\\
\\
N/2 \leq k \leq N: \\
\end{matrix} \left[ 
\begin{array}{cccccccc}
w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0\\
w_8^0 &amp; w_8^1 &amp; w_8^2 &amp; w_8^3 &amp; w_8^4 &amp; w_8^5 &amp; w_8^6 &amp; w_8^7 \\
w_8^0 &amp; w_8^2 &amp; w_8^4 &amp; w_8^6 &amp; w_8^8 &amp; w_8^{10} &amp; w_8^{12} &amp; w_8^{14} \\
w_8^0 &amp; w_8^3 &amp; w_8^6 &amp; w_8^9 &amp; w_8^{12} &amp; w_8^{15} &amp; w_8^{18} &amp; w_8^{21} \\
\hline
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
\end{array} \right] %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is to split the matrices into even and odd entries of $x_n$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[ \begin{array}{cccc cccc}
w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0\\
w_8^0 &amp; w_8^1 &amp; w_8^2 &amp; w_8^3 &amp; w_8^4 &amp; w_8^5 &amp; w_8^6 &amp; w_8^7 \\
w_8^0 &amp; w_8^2 &amp; w_8^4 &amp; w_8^6 &amp; w_8^8 &amp; w_8^{10} &amp; w_8^{12} &amp; w_8^{14} \\
w_8^0 &amp; w_8^3 &amp; w_8^6 &amp; w_8^9 &amp; w_8^{12} &amp; w_8^{15} &amp; w_8^{18} &amp; w_8^{21} \\
\hline
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
\end{array} \right] \\
\qquad \Big \downarrow \quad \text{Split}\\
\begin{matrix}
\underbrace{\begin{bmatrix}
w_8^0 &amp; - &amp; w_8^0 &amp; - &amp; w_8^0 &amp; - &amp; w_8^0 &amp; - \\
w_8^0 &amp; - &amp; w_8^2 &amp; - &amp; w_8^4 &amp; - &amp; w_8^6 &amp; - \\
w_8^0 &amp; - &amp; w_8^4 &amp; - &amp; w_8^8 &amp; - &amp; w_8^{12} &amp; - \\
w_8^0 &amp; - &amp; w_8^6 &amp; - &amp; w_8^{12} &amp; - &amp; w_8^{18} &amp; - \\
\hline
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
\end{bmatrix}
}_{\text{even entries of $x_n$}} 
&amp;, 
\underbrace{ 
\begin{bmatrix}
- &amp; w_8^0 &amp; - &amp; w_8^0 &amp; - &amp; w_8^0 &amp; - &amp; w_8^0\\
- &amp; w_8^1 &amp; - &amp; w_8^3 &amp; - &amp; w_8^5 &amp; - &amp; w_8^7 \\
- &amp; w_8^2 &amp; - &amp; w_8^6 &amp; - &amp; w_8^{10} &amp; - &amp; w_8^{14} \\
- &amp; w_8^3 &amp; - &amp; w_8^9 &amp; - &amp; w_8^{15} &amp; - &amp; w_8^{21} \\
\hline
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
- &amp; - &amp; - &amp; - &amp; - &amp; - &amp; -  &amp; - \\
\end{bmatrix}
}_{\text{odd entries of $x_n$}} \\
\big \downarrow &amp; \qquad \qquad \qquad \big \downarrow \text{Extract} \ e^{-i2\pi \frac{k}{N}} \\
\begin{bmatrix}
w_8^0&amp; w_8^0 &amp; w_8^0 &amp; w_8^0\\
w_8^0&amp; w_8^2 &amp; w_8^4 &amp; w_8^6 \\
w_8^0&amp; w_8^4 &amp; w_8^{8} &amp; w_8^{12} \\
w_8^0&amp; w_8^6 &amp; w_8^{12} &amp; w_8^{18} \\
\end{bmatrix}
&amp; 
\begin{bmatrix}
w_8^0 &amp;     &amp;     &amp; \\
    &amp; w_8^1 &amp;     &amp; \\
    &amp;     &amp; w_8^2 &amp; \\
    &amp;     &amp;     &amp; w_8^3 \\
\end{bmatrix}
\begin{bmatrix}
w_8^0 &amp; w_8^0 &amp; w_8^0 &amp; w_8^0\\
w_8^0 &amp; w_8^2 &amp; w_8^4 &amp; w_8^6 \\
w_8^0 &amp; w_8^4 &amp; w_8^{8} &amp; w_8^{12} \\
w_8^0 &amp; w_8^6 &amp; w_8^{12} &amp; w_8^{18} \\
\end{bmatrix} \\
\qquad \big \downarrow \text{DIT} &amp; \qquad \big \downarrow \text{DIT} \\
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
&amp; \begin{bmatrix}
w_8^0 &amp;     &amp;     &amp; \\
    &amp; w_8^1 &amp;     &amp; \\
    &amp;     &amp; w_8^2 &amp; \\
    &amp;     &amp;     &amp; w_8^3 \\
\end{bmatrix}
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix} \\
\end{matrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the DIT stands for ‘Decimation in Time’ where we flip the $2$ in the complex exponential $\frac{2m}{N}$ down to $\frac{m}{N/2}$ which halves both the nominator and denominator, i.e.
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
w_8^6 = e^{-i2\pi \ k \ \frac{2m}{N}} |_{m=1, k=3, N=8} = e^{-i2\pi \ k \ \frac{m}{N/2}} |_{m=1, k=3, N=8} = w_4^3 
\end{align}&lt;/script&gt;
Besides being mathematically correct, it makes intuitively sense, as doing $6$ steps with $1/8$’th of a stepsize is the same as doing $3$ steps with $1/4$’th of a step size on a circle.&lt;/p&gt;

&lt;p&gt;So we obtain two DFT’s of shape $4 \times 4$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
X_0 \\ X_1 \\ X_2 \\ X_3 \\
\end{bmatrix} 
=
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_2 \\ x_4 \\ x_6 \\
\end{bmatrix}
+ 
\underbrace{
\begin{bmatrix}
w_8^0 &amp;     &amp;     &amp; \\
    &amp; w_8^1 &amp;     &amp; \\
    &amp;     &amp; w_8^2 &amp; \\
    &amp;     &amp;     &amp; w_8^3 \\
\end{bmatrix}}_{e^{-i 2 \pi \frac{k}{N}}}
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_3 \\ x_5 \\ x_7 \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The magic periodicity reuses the computations in the first half of the DFT to give the second half $X_{k+N/2}$ frequency bins with minimal overhead:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[ \begin{array}{} X_4 \\ X_5 \\ X_6 \\ X_7 \\ \end{array} \right] =
\underbrace{
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_2 \\ x_4 \\ x_6 \\
\end{bmatrix}
}_{\text{already computed above}}
-
\underbrace{
\begin{bmatrix}
w_8^4 &amp;     &amp;     &amp; \\
    &amp; w_8^5 &amp;     &amp; \\
    &amp;     &amp; w_8^6 &amp; \\
    &amp;     &amp;     &amp; w_8^7 \\
\end{bmatrix}}_{e^{-i 2 \pi \frac{k}{N}}}
\underbrace{
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_3 \\ x_5 \\ x_7 \\
\end{bmatrix}
}_{\text{already been computed}} %]]&gt;&lt;/script&gt;

&lt;p&gt;Not using the structure of the matrix respectively the complex periodicity would have left us with a $ 8 \times 8 = 64$ matrix multiplication.
Using the tricks laid out above costs us two $4 \times 4$ matrix multplications, two $4$ additions and two $4$ multiplications for a grand total of $2 \cdot (4 \times 4) + 2 \cdot 4 + 2 \cdot 4 = 48$ operations&lt;/p&gt;

&lt;p&gt;Upon closer inspection of the first matrix being multiplied with $[ x_0, x_2, x_4, x_6]$ we can notice that matrix exhibits the same properties and structure as the original DFT matrix.
This is were the recursion kicks in in the linear algebra formulation.&lt;/p&gt;

&lt;p&gt;For that to happen, we take the even of the evens, $[x_0, x_4]$ and the odds of the even $[x_2, x_6]$ and split them as before:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\text{DFT}
\left( \left[
\begin{array}{}
x_0 \\ x_2 \\ x_4 \\ x_6 \\
\end{array}
\right]
\right)
=
\left[
  \begin{array}{}
X_0 \\ X_1 \\ X_2 \\ X_3 \\
\end{array}
\right]
= 
\underbrace{
\begin{bmatrix}
w_4^0 &amp; w_4^0 &amp; w_4^0 &amp; w_4^0\\
w_4^0 &amp; w_4^1 &amp; w_4^2 &amp; w_4^3 \\
w_4^0 &amp; w_4^2 &amp; w_4^{4} &amp; w_4^{6} \\
w_4^0 &amp; w_4^3 &amp; w_4^6 &amp; w_4^{9} \\
\end{bmatrix}
}_{\text{yet again a DFT matrix}}
\begin{bmatrix}
x_0 \\ x_2 \\ x_4 \\ x_6 \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Again using the key insight that we can reconstruct the second half of the DFT bins from the first half, we can zero out a lot of computations to essentially break down the original DFT from a $4 \times 4$ to two $2 \times 2$ matrix multiplications:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
X_0 \\ X_1 \\ - \\ - \\
\end{bmatrix}
=
\begin{bmatrix}
w_4^0 &amp; - &amp; w_4^0 &amp; - \\
w_4^0 &amp; - &amp; w_4^2 &amp; - \\
- &amp; - &amp; - &amp; -  \\
- &amp; - &amp; - &amp; -  \\
\end{bmatrix}
\begin{bmatrix}
x_0 \\ - \\ x_4 \\ - \\
\end{bmatrix}
+ 
\begin{bmatrix}
w_4^0 &amp;     &amp;     &amp; \\
    &amp; w_4^1 &amp;     &amp; \\
    &amp;     &amp; - &amp; \\
    &amp;     &amp;     &amp; - \\
\end{bmatrix}
\begin{bmatrix}
- &amp; w_4^0 &amp; - &amp; w_4^0\\
- &amp; w_4^0 &amp; - &amp; w_4^2 \\
- &amp; - &amp; - &amp; -  \\
- &amp; - &amp; - &amp; -  \\
\end{bmatrix}
\begin{bmatrix}
- \\ x_2 \\ - \\ x_6 \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;and we can reconfigure the already computed $2 \times 2$ matrices to get the second half of the frequency bins&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
X_3 \\ X_4 \\
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
w_2^0 &amp; w_2^0 \\
w_2^0 &amp; w_2^1 \\
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_4 \\
\end{bmatrix}
}_{\text{already computed}} 
- 
\begin{bmatrix}
w_4^2 &amp;     \\
    &amp; w_4^3 \\
\end{bmatrix}
\begin{bmatrix}
w_2^0 &amp; w_2^0\\
w_2^0 &amp; w_2^1 \\
\end{bmatrix}
\begin{bmatrix}
x_2 \\ x_6 \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;And we can do it yet again by observing that the $2 \times 2$ matrix can be broken up again once more (but at size 1 the recursion naturally stops).
Similarly this recursion applies equally to the original odd entry terms in the upper most recursion layer.&lt;/p&gt;

&lt;p&gt;This recursive breaking up matrices and saving half the computations (via reconstruction of the second half of the frequency bins) gives the FFT it’s highly useful $O(N \log N)$ complexity.&lt;/p&gt;

&lt;p&gt;The efficiency stems from us retracing the recursive matrix break ups with our results once we arrived at the DFT with length of one.
Each evaluation in the recursion allows us to reconstruct frequency bins twice our original size (thanks to periodicity) with minimal overhead, which goes from $1 \rightarrow 2 \rightarrow 4 \rightarrow 8$.
So for signals of length $8$, we need 4 recursive step to arrive at a DFT length of one, from which we can reconstruct the frequency bins efficiently.
For a signal of length $16$ we need 5 recursions, for $32$ just one more, namely 6 recursions, and for $64$ just seven recursions.
While the speed up might be small for short signals, audio people with 20.000 samplings steps should seriously rejoice and feel blessed by the $O(N \log N)$ complexity.&lt;/p&gt;

&lt;h3 id=&quot;the-inverse-dft&quot;&gt;The inverse DFT&lt;/h3&gt;

&lt;p&gt;In order to reconstruct a signal from its Fourier coefficients $X_k$, we multiply the frequency amplitude $X_k$ with a complex exponential which is a periodic function with a specific frequency $k$.
We have the reconstruction
&lt;script type=&quot;math/tex&quot;&gt;x_n = \frac{1}{N} \sum_{k=0}^K X_k e^{i 2\pi k \frac{n}{N}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where the only difference to the DFT is the minus sign in the exponential and a scaling factor $1/N$.
The cool thing is that the minus sign has no influence on the core structure of the transformation, so we can employ the entire algorithm again for the reconstruction from frequency to time domain.&lt;/p&gt;

&lt;p&gt;Thus we have a $O(N \log N)$ algorithm both for the transformation in both directions. Sweet …&lt;/p&gt;</content><author><name></name></author><summary type="html">From Complex Exponentials to Frequencies in O(N log N)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/FFT.png" /></entry><entry><title type="html">Continuous Time Markov Chains in Jax</title><link href="http://localhost:4000/blog/ContTimeMarkovChain/" rel="alternate" type="text/html" title="Continuous Time Markov Chains in Jax" /><published>2023-01-09T00:00:00+01:00</published><updated>2023-01-09T00:00:00+01:00</updated><id>http://localhost:4000/blog/ContTimeMarkovChain</id><content type="html" xml:base="http://localhost:4000/blog/ContTimeMarkovChain/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\def\tr#1{\text{Tr}\left[ #1 \right]}
 \def\Efunc#1{\mathbb{E}\left[ #1\right]}
 \def\Efuncc#1#2{\mathbb{E}_{#1}\left[ #2 \right]}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;continuous-time-markov-chains&quot;&gt;Continuous Time Markov Chains&lt;/h3&gt;

&lt;p&gt;A continuous-time Markov chain (CTMC) is a mathematical model that describes the evolution of a system over time, where the state of the system changes according to a set of probabilities at each point in time. 
Like a discrete-time Markov chain, a CTMC consists of a set of states and a set of transitions between those states. 
However, in a CTMC, the transitions between states can occur at any point in time, rather than only at discrete time steps.&lt;/p&gt;

&lt;p&gt;A CTMC is defined by its generator matrix, which describes the rate at which the system transitions between states. 
The elements of the generator matrix are typically denoted by $Q_{ij}$, where $Q_{ij}$ is the rate at which the system transitions from state $i$ to state $j$.
These rates denote the intensity $\lambda$ of exponentially distributed random variables.
The diagonal elements $Q_{ii}$ denote the rate with which we will stay in state $i$, whereas $Q_{ij}, i \neq j$ denotes the rate of changing from state $i$ to $j$.
Whereas discrete time markov chains have transition probabilities in form of the matrix $P_{ij}$ for discrete time steps, CTMC are a bit more involved by changing continuously in time $t$ and change states according to $P(t)$.&lt;/p&gt;

&lt;p&gt;But how are the rates $Q_{ij}$ and the transition matrix $P_{ij}(t)$ connected?
The ‘generator’ in generator matrix $Q$ comes from $Q$’s property of containing the instantaneous rate of change for an infinitesimally small change in time.
The rate describes the instantaneous propensity or affinity for the process to transition between its states.
If we know how the instantaneous behaviour of the stochastic process is, we can ‘generate’ entire trajectories of it.&lt;/p&gt;

&lt;p&gt;Each entry $P_ij(t)$ denotes the probability of transitioning from state $i$ to state $j$ dependent on the time $t$ that has passed.
This ergodicity is due to the Markov property that treats each transition independently from all previous.
For $t=0$ we get $P(0)=I$ as we will surely stay in the same state $i$ since no time has passed.
If no time has passed, we concurrently didn’t have time to transition.&lt;/p&gt;

&lt;p&gt;Thus $Q$ and $P(t)$ are connected via
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
  Q = \lim_{t \rightarrow 0^+} \frac{P(t)- I}{t} = \lim_{t \rightarrow 0^+} \frac{P(t)- P(0)}{t}.
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Each row of $P(t)$ has to sum up to 1, namely $\sum_j P_{ij}(t) = 1$. 
Another intriguing property of the generator matrix $Q$ is that the diagonal terms are negative.
To see why we can construct a simple example by looking at the first row $i=1$ after one unit of time has passed, $P_{i=1}(t=1) = [0.8, 0.1, 0.1]$.
Plugging it into the limit (albeit be it with $t=1$), we get 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
  Q = P(1) - I = [0.8, 0.1, 0.1] - [1, 0, 0] = [-0.2, 0.1, 0.1].
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Taking this to the limit of $\lim t \rightarrow 0^+$ and seeing that the identity matrix only subtracts from the diagonal elements, we can conclude that the diagonal elements $Q_{ii}$ are in effect negative and the negative sum of the off-diagonal terms in each row, $Q_{ii} = - \sum_{i \neq j} Q_{ij}$.
We can thus interpret the absolute value of the diagonal terms $Q_{ii}$ as the rate of staying in the state $i$, whereas all other rates determine the rate of changing to a different state $j$.&lt;/p&gt;

&lt;p&gt;Computing the limit $\lim t \rightarrow 0^+$ gives us the rates of the exponentially distributed holding times in each state.
The exponential distribution models the time between events in a Poisson point process, explained on the blog &lt;a href=&quot;https://ludwigwinkler.github.io/blog/Poisson/&quot;&gt;here&lt;/a&gt;.
Thus given a rate $\lambda$ which is given by the elements in the matrix $Q$, we have multiple competing exponentially distributed random variables.
The question now arises what transition will occur first.&lt;/p&gt;

&lt;p&gt;Conveniently, we can ask a different question and split the problem into two subproblems, namely ‘How long will we stay in $i$?’ and ‘Once we exit state $i$, where do we go?’.&lt;/p&gt;

&lt;p&gt;The first question can be answered by observing what the probability is, that &lt;em&gt;any&lt;/em&gt; process activates.
Any means here simply the minimum that any process changes, namely $\min{X_1, X_2, X_3}$.
Each off-diagonal entry $\lambda_j$ denotes the rate of an independent exponentially distributed random variable $\tau_j \sim \text{Exp}[\lambda_j]$.
Thus we are interested in the probability of how the minimum of all holding times $\tau_j$ is distributed vis-a-vis being larger than some aggregated holding time $t$.
For that we can employ the complementary cumulative density function (cCDF = 1-CDF so not standard CDF $p(\tau_i &amp;lt; t)$ but $p(\tau_i &amp;gt; t)$ lying outside of $t$) of the exponential distribution and ask whether $p(\tau_i &amp;gt; t)$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(\min \{ \tau_1, \tau_2, \tau_3 \} &gt; t) &amp;= p(\tau_1 &gt; t, \tau_2 &gt; t, \tau_3 &gt; t) \\
  &amp;= \prod_{i=1}^3 p(\tau_i &gt; x) \\
  &amp;= \prod_{i=1}^3 \overbrace{1 - \underbrace{(1 - \exp[-\lambda_i x])}_{\text{Exp CDF}}}^{\text{Exp cCDF}} \\
  &amp;= \prod_{i=1}^3 \exp[-\lambda_i x] \\
  &amp;=  \exp \left[- \sum_{i=1}^3 \lambda_i x \right]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Thus the minimum holding time is again an exponential distribution with the rate $\sum_i \lambda_i$.
Once the holding time is ‘exhausted’ or has passed, we have to determine the second question, namely which process is going to occur next.
Therefore we want to know the probability
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(i = \text{argmin}_i \{ \tau_1, \tau_2, \tau_3 \}) &amp;= \int_0^\infty p(\tau_i = t) \ p(\forall_{k \neq i} \tau_k &gt; t) dt \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which states that our $i$ in question is precisely the holding time $t$ and factorizing via conditional independence that every other exponentially distributed holding time $\tau_k$ is larger than $t$ which is again the cCDF.
Finally we marginalize over $t$ to incorporate the fact the holding time $t$ is itself a random variable which ought to be marginalized out, effectively taking into account every possible value of $t$.
We obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(i = \text{argmin}_i \{ \tau_1, \tau_2, \tau_3 \}) 
  &amp;= \int_0^\infty p(\tau_i = t) \ p(\forall_{k \neq i} \tau_k &gt; t) dt \\
  &amp;= \int_0^\infty \underbrace{p(\tau_i = t)}_{\text{Exp}} \ \underbrace{p(\forall_{k \neq i} \tau_k &gt; t)}_{\text{cCDF}} dt \\
  &amp;= \int_0^\infty \lambda_i \exp[-\lambda_i t] \ \prod_{k \neq i} \exp[- \lambda_k t] dt \\
  &amp;= \int_0^\infty \lambda_i  \ \prod_{k} \exp[- \lambda_k t] dt \\
  &amp;= \lambda_i \int_0^\infty  \ \exp \left[- \sum_{k} \lambda_k t \right] dt \\
  &amp;= \lambda_i  \left[ - \frac{\exp \left[- \sum_{k} \lambda_k t \right]}{\sum_{k} \lambda_k} \right]_0^\infty \\
  &amp;= \lambda_i  \left(  0 + \frac{1}{\sum_{k} \lambda_k} \right) \\
  &amp;= \frac{\lambda_i}{\sum_{k} \lambda_k} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Thus we can conclude that the process that ‘acts first’ is $i \sim \text{Cat}[\lambda_1, \lambda_2, \lambda_3]$.
We can thus devise a sampling algorithm based on rates $\lambda_i$ which first samples holding time $t \sim \text{Exp}[\sum_i \lambda_i]$ and after the holding time samples the next state as $i \sim \text{Cat}[\lambda_i]$.&lt;/p&gt;

&lt;h3 id=&quot;birth-death-process&quot;&gt;Birth-Death Process&lt;/h3&gt;

&lt;p&gt;A birth-death process is a stochastic process $\{X_t\}_{t \in \mathbb{R}^+}$ with $X_t = X_t^\lambda - X_t^\mu$ with two ‘duelling’ rates: the birth rate $\lambda$ and the death rate $\mu$.
Both rates induce a counting process (albeit be it with opposite signs) such that the infinitissimal generators are
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\lim_{h \rightarrow 0^+} P(X_{t+h} = X_t + 1 | X_t) &amp;= \lambda h \\
\lim_{h \rightarrow 0^+} P(X_{t+h} = X_t - 1 | X_t) &amp; = \mu h \\
\lim_{h \rightarrow 0^+} P(X_{t+h} = X_t + 0| X_t) &amp; = 1- (\lambda + \mu) h
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;the derivation of which can be followed up &lt;a href=&quot;https://ludwigwinkler.github.io/blog/Poisson/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We can thus construct a straight forward sampling algorithm by sampling the holding time $\tau$ in state $X_{t+\tau} = X_t$ with $\tau \sim \text{Exp}[ \lambda + \mu]$ and once the holding time is over, sampling either the death or the birth process with ${+1, -1} \sim \text{Cat}[\lambda, \mu]$.&lt;/p&gt;

&lt;p&gt;Sampling from the exponential distribution can be easily done with inverse transform sampling by first sampling $u \sim U[0,1]$ and using 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
u &amp;= \text{ExpCDF}(x, \lambda) \\
u &amp;= 1 - \exp[-\lambda x] \\
\underbrace{1 - u}_{=u'} &amp;= \exp[-\lambda x] \\
\log u' &amp;= -\lambda x \\
-\frac{1}{\lambda} \log u' &amp;= x \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where we used the fact that $u’ = 1 - u \sim U[0,1]$ is again a uniformly distributed random variable.&lt;/p&gt;

&lt;h3 id=&quot;jax&quot;&gt;Jax&lt;/h3&gt;

&lt;p&gt;If you haven’t been living under a proverbial analog rock in the digital machine learning space, you might have heard of &lt;a href=&quot;https://github.com/google/jax&quot;&gt;Jax&lt;/a&gt;.
At the core of Jax are four function transforms with three to four letters: &lt;code class=&quot;highlighter-rouge&quot;&gt;grad&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pmap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;grad&lt;/code&gt; traces a function when it’s called the first time and transforms that function by replacing the operations in the function with its derivatives with respect to the input&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt; adds a just-in-time compilation function transform which aims at compiling an optimized version of your given function.
Python is an interpreted language which does executes every instruction without anticipating the future.
After having traced through a &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt; marked function for the first time, Jax will know what the whole execution of the function will entail.
Since it knows all the operations, it will upon finishing the first evaluation of the function start compiling it fusing operations and removing unneeded data copying for example (this is a very broad example).
Obviously this entails some programming restrictions which can be read up in the &lt;a href=&quot;https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html&quot;&gt;Jax - The Sharp Bits&lt;/a&gt;.
You have to essentially live with a few restrictions and in order to ensure deterministic function evaluations the random key has to passed to stochastic functions, rendering the deterministic given the random number generator key.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; is in my opinion the true power move of Jax by allowing to replicate the same function over an extra dimension.
You can write a possibly very complicated function for a single data point and simply vmap over to enable it to evaluate the function over batches of data points in parallel.
No more extra dimensions, no more custom forward passes changing every matrix-vector to a matrix-matrix operation etc.
Just vmap it.
We’ll see down below how powerful &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; can be.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pmap&lt;/code&gt; does automatic accelerator placement by automatically optimizing the given function over all sorts of TPU’s and GPU’s.
It is akin to PyTorch’s tensor.to(device) by compiles the entire function custom made for each accelerator on you compute node.
Powerful stuff right there.&lt;/p&gt;

&lt;h3 id=&quot;continuous-time-markov-chains-in-jax&quot;&gt;Continuous Time Markov Chains in Jax&lt;/h3&gt;

&lt;p&gt;The setup is going to be the following:
We are going to have a single birth death process with three dimensions.
Each dimension has its own birth rate $\lambda_i$ and death rate $\mu_i$.
Since it’s a Markov chain, we can treat each holding time $t$ independently from all previous holding times.
This allows us to construct the following algorithm to sample the multi-dimensional birth death process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample holding time $t \sim \text{Exp}(\sum_k \lambda_k + \mu_k)$&lt;/li&gt;
  &lt;li&gt;Once $t$ has passed, sample active process $i \sim \text{Cat}( \lambda_1 + \mu_1, … , \lambda_d + \mu_d)$&lt;/li&gt;
  &lt;li&gt;From ‘active’ process $i$ sample the increment $\Delta X_t \sim \text{Cat}(\lambda_i, \mu_i)$ with $\Delta X_t \in \{ +1, -1 \}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practice, we could combine step 2 and step 3 into a single categorical distribution but this is fiddly and splitting it into two sampling looks cleaner. 
Just-in-time compilation should in theory remove an inefficiencies anyway.
Up first on our to do list is to be able to sample from an exponential distribution.
This can be achieved readily with the reparameterization trick for the exponential distribution describes earlier,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import jax

la = jnp.array([3., 4., 0.5]).reshape(-1, 1) # birth rates, lambda is already reserved by Python for anonymous functions
mu = jnp.array([2., 3., 4.]).reshape(-1, 1) # death rate
key = jax.random.PRNGKey(1) #

def sample_exponential(lam, key):
  '''Reparameterization for exponential distribution```
	return - 1 / lam * jnp.log(jax.random.uniform(key))

@jax.jit
def batched_sample_exponential(lam, key):
	'''first argument has to be inner most vmap over anonymous lambda function'''
	return jax.vmap(lambda key: jax.vmap(lambda la: sample_exponential(la, key))(la))(key).squeeze(-1)

n=1_000
key, *subkey = jax.random.split(key,n+1)
subkey = jnp.stack(subkey)
samples = batched_sample_exponential(la, subkey)

fig = plt.figure(figsize=(20,10))
for samples_, la_ in zip(samples.T, la):
	_ = plt.hist(samples_, bins=50, density=True, alpha=0.5, label=f'{la_}')
_ = plt.xlim(-1,3)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which should return some exponential distributions looking like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../blog/CTMC/exponential.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up is to get acquainted to sampling processes given&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key = jax.random.PRNGKey(1)
la = jnp.array([0.1, 1.5, 2.]).reshape(-1,1)
mu = jnp.array([0.1, 1.5, 2.]).reshape(-1,1) - 0.01 * la # slightly smaller death rate

weights = jnp.concatenate([la, mu], axis=0) # concat them to one long vector

key, *subkey = jax.random.split(key, 10000)
subkey = jnp.stack(subkey)
active_process = jax.vmap(lambda key: jax.random.choice(key, a=jnp.size(weights), p=weights.squeeze()))(subkey)

_ = plt.hist(active_process, bins=jnp.size(weights), density=True)
_ = plt.xticks(jnp.arange(jnp.size(weights)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;from which we get the following plot where the first three bins are the probability of sampling the birth process and the last three bins the probability of sampling the death process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../blog/CTMC/activprocess.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can now implement our first CTMC sampling algorithm by packing the sampling into a while loop:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import jax.numpy as jnp
la = jnp.array([0.1, 1, 2.]).reshape(-1,1)
mu = jnp.array([0.1, 1, 2.]).reshape(-1,1) - 0.1 * la # slightly smaller da

def sample_process( la, mu, T, key):

	num_processes = jnp.size(la) # 3 processes since we have three birth rates λ
	concat_weights = jnp.concatenate([la, mu], axis=0) # [la_1, la_2, la_3, mu_1, mu_2, mu_3]

	traj = jnp.zeros((1,jnp.size(la),)) # [step=1, F=3]
	t = jnp.zeros((1,)) # [step=1]

	key = jax.random.PRNGKey(2)
	active_process_idxs = [] # collecting active process samplings

	while t[-1]&amp;lt; T:
		key, process_key, time_key = jax.random.split(key, 3)
		''' sampling process index from [la_1, la_2, la_3, mu_1, mu_2, mu_3]'''
		active_process = jax.random.choice(process_key, a=jnp.size(concat_weights), p=concat_weights.squeeze())
		active_process_idxs += [active_process]
		
		if active_process &amp;lt; num_processes: # [0, num_processes -1]
			# birth process
			rate = la[active_process]
			state_change = jax.nn.one_hot(jnp.array([active_process]), num_classes=num_processes)
		else:
			# death process
			rate = mu[active_process - num_processes]
			state_change = -jax.nn.one_hot(jnp.array([active_process - num_processes]), num_classes=num_processes)

		traj = jnp.concatenate([traj, traj[-1:] + state_change]) # do increment
		holding_time = sample_exponential(rate, time_key) # sample holding time where nothing happens
		t = jnp.concatenate([t, t[-1]+holding_time])

	return t, traj, active_process_idxs

key = jax.random.PRNGKey(1)
num_samples = 1
key, *subkey = jax.random.split(key, 1+num_samples)
subkey = jnp.stack(subkey, axis=0)
print(subkey.shape)
T = 100
t, traj, active_process_idxs = jax.vmap(lambda key: sample_process(la, mu, T, key))(subkey)

t.shape, traj.shape
color = {0: 'r', 1: 'g', 2:'b'}
fig = plt.figure(figsize=(20, 10))
for dim, traj_ in enumerate(traj[0].T):
	label=f'$\lambda={la[dim][0]:.2f}, \mu={mu[dim][0]:.2f}$'
	_ = plt.step(t[0], traj_.squeeze(), alpha=0.66, c=color[dim], label=label)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the result is a nice plot of a three dimensional continuous time Markov chain in which each process ‘duels’ for action.
We can see from the behaviour of the sampled dimension and their corresponding parameters $\lambda$ and $\mu$, that the birth death process with the highest rates also exhibits the most fluctuations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../blog/CTMC/firstctmc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our first plotting function was straight out of the matplotlib box.
We will now implement a custom &lt;code class=&quot;highlighter-rouge&quot;&gt;plot_ctmc()&lt;/code&gt; function which can plot multiple draws of the processes.
The following function allows us to plot multiple draws conveniently.
For this we adopt the convention that all samples from a CTMC must be of the shape &lt;code class=&quot;highlighter-rouge&quot;&gt;[MC, T, F]&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;MC&lt;/code&gt; is the number of random draws, &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; is the number of time index and &lt;code class=&quot;highlighter-rouge&quot;&gt;F&lt;/code&gt; is the dimensionality of the process.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import matplotlib
def plot_ctmc(t, traj, la, mu):
	'''
	t: [T]
	traj: [MC, T, F]
	labels: 2F birth mu and death lambda
	colors: 2F
	'''
	assert traj.shape[0]==t.shape[0], f&quot;{traj.shape=} vs {t.shape[0]=}&quot;
	if traj.ndim==2:
		traj = jnp.expand_dims(traj, 0) # [T, F] -&amp;gt; [1, T, F]
	assert traj.ndim==3
	if t.ndim==2:
		t = jnp.expand_dims(t, 0)

	if t.ndim==1 and t.size==traj.shape[1]:
		t = jnp.expand_dims(t, -1) # [T] -&amp;gt; [T, 1]
	assert t.shape[:2]==traj.shape[:2], f&quot;{t.size=} vs {traj.shape=}&quot;
	
	color = {0: 'r', 1: 'g', 2:'b'}
	fig = plt.figure(figsize=(20,10))
	for dim in range(traj.shape[-1]):
		for mc_t, mc_traj in zip(t, traj):
			_ = plt.step(mc_t[:,0], mc_traj[:,dim], c=color[dim], alpha=max(0.1, 1/traj.shape[0]))
	plt.grid()
	plt.legend(handles=[matplotlib.lines.Line2D([0],[0], color=color[dim], label=f'$\lambda={la[dim]:.2f}, \mu={mu[dim]:.2f}$') for dim in range(traj.shape[-1])])
	return fig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;using-jaxjit-and-making-things-go-vrooooom&quot;&gt;Using jax.jit and making things go ‘vrooooom’&lt;/h3&gt;

&lt;p&gt;Unfortunately &lt;code class=&quot;highlighter-rouge&quot;&gt;jax.jit&lt;/code&gt; is not very good with stochastic control flow.
We have the problem that we sample the active processes indices and then decide which part of the condition we execute.
That is a big no-no for JIT compilation as the stochasticity makes mapping out the data flow through the entire function not 100% deterministic.
Secondly, &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; will won’t work either as each process that will vmap will decide differently due to separately sampling it’s condition.
Figuratively, the first vmapped process will sample the a birth process, but the second vmapped process will sample a death process.
What branch of the condition should the vmapped function then take?
Should it follow the first condition or the second?
Through some testing, which is too verbose for here, I have the very substantiated hunch that &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; would follow the first condition.&lt;/p&gt;

&lt;p&gt;What to do then?
We have to generalize the control flow by using a &lt;code class=&quot;highlighter-rouge&quot;&gt;mask = jax.numpy.where(condition, a, b)&lt;/code&gt; function which allows for control flow to be applied to an arbitrary array, and thus is also vmap-able as new dimension can be added to these arrays.&lt;/p&gt;

&lt;p&gt;Additionally, the time index in the while loop is not ideal, as &lt;code class=&quot;highlighter-rouge&quot;&gt;while(current_time &amp;lt; T )&lt;/code&gt; allows for vastly different lengths of the time index. Imagine that right before reaching &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt;, one vmapped process suddenly samples a super long holding time $t$.
Then you’d have one process which is suddenly significantly longer time-wise than all other processes.
To alleviate this problem we will discretize the time axis with a &lt;code class=&quot;highlighter-rouge&quot;&gt;for(jax.numpy.linspace(0,T, steps))&lt;/code&gt; similar to a standard stochastic differential equation solved with Runge-Kutta 4 or Euler-Maryuama.
Sure, in theory we would want to sample continuously and correctly, but if we choose the discretized time grid with a fine enough resolution we get a sufficient result with the added bonus that our time index is standardized over all experiments.&lt;/p&gt;

&lt;p&gt;Let’s get coding:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key = jax.random.PRNGKey(1)

la = jnp.array([0.1, 1, 2.])
mu = jnp.round(la - 0.01 * la, 2) # slightly smaller da

def matrix_birth_death_process(key, la, mu):
	```Time resolution```
  T = 20
	steps = T * 100
	
  ```Preparing rates```
  num_processes = jnp.size(la)
	rates = jnp.stack([mu, la], axis=-1)
  
  ```Preparing arrays holding sampled process```
	traj = jnp.zeros_like(la).reshape(1,-1)
	t = jnp.zeros((1,1))
	
  for t_ in tqdm(jnp.linspace(0, T, steps)): # discretized time axis
    ```Use jax.numpy.where() to enable vmap-able control flow by using multiplication of arrays```
		mask = jnp.where(t[-1:]&amp;lt;=t_, jnp.ones((1,)), jnp.zeros((1,))) # if t_ has progressed further than last t, time to jump
    ```Splitting the relevant keys```
    key, process_key, birth_death_key, time_key = jax.random.split(key, 4)
    ```Sampling the active process, birth or death and holding time in new state```
		active_process_idx = jax.random.categorical(key=process_key, logits=la + mu)
		birth_death_idx = jax.random.categorical(key=birth_death_key, logits=rates[active_process_idx])
    holding_time_intensity = jnp.sum(la+mu)
		holding_time = sample_exponential(lam = holding_time_intensity, key=time_key)

    ```Updating state of process```
    increment = 2* birth_death_idx - 1 # rescale {0,1} to {-1, 1}
		update = increment * jax.nn.one_hot(x=active_process_idx, num_classes=num_processes)
    
    ```Concatenating current state and holding time to solution arrays of CTMC```
		traj = jnp.concatenate([traj, traj[-1:] + mask* update])
		t = jnp.concatenate([t, t[-1:] + mask *holding_time])

	return t, traj

t, traj = matrix_birth_death_process(key, la, mu)

fig = plot_ctmc(t, traj, la=la, mu=mu)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../blog/CTMC/matrixctmc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately we have to wait close to a minute for the sampling process to execute fully.
To speed things up, we’ll introduce a single function &lt;code class=&quot;highlighter-rouge&quot;&gt;jax.jit&lt;/code&gt; and see what will happen.
We’ll also ‘allocate’ the appropriate memory by fixing the size of arrays holding the sampled trajectories instead of concatenating it.
The more information the JIT compiler has, the more it can condense the verbose code into it’s minimum viable representation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key = jax.random.PRNGKey(1)
num_process_samples = 20
key, *process_samples_key = jax.random.split(key, 1 + num_process_samples)
process_samples_key = jnp.stack(process_samples_key)


la = jnp.array([0.1, 0.1, 0.1])*10
mu = jnp.round(la - 0.01 * la, 2)  # slightly smaller death process

def fast_matrix_birth_death_process(key, la, mu, jit=True):

    def sampling(t, idx, next_event, traj, la, mu, key):
        '''Some prepration for sampling'''
        num_processes = jnp.size(la)
        rates = jnp.stack([mu, la], axis=-1)
        '''
        vmap-able and jit-able control flow in shape of jax.numpy.where
        If current time t has progressed further than last event time, time to act!
        '''

        mask = jnp.where(t &amp;gt;= next_event[idx-1], jnp.ones((1,)), jnp.zeros((1,)))

        '''Split keys for sampling: process selection key, birth or death selection key, holding time key'''
        key, process_key, birth_death_key, time_key = jax.random.split(key, 4)

        '''Sampling the active process, whether birth or death, and holding time until next event'''
        active_process_idx = jax.random.categorical(key=process_key, logits=la + mu)
        birth_death_idx = jax.random.categorical(key=birth_death_key, logits=rates[active_process_idx])
        holding_time = sample_exponential(lam=jnp.sum(la+mu), key=time_key)
        
        '''Rescale increment from {0,1} to {-1, +1}'''
        increment = 2 * birth_death_idx - 1
        update = increment * jax.nn.one_hot(x=active_process_idx, num_classes=num_processes)

        '''
        Only adds when mask is one, if idx=0, then idx-1 wraps around to final time step which is zeros anyway
        If mask=0, it simply copies the event time from last step, but if mask=1, sampling is kicked off and the next holding time is added to current time
        '''
        traj = traj.at[idx].set(traj[idx-1] + mask.squeeze() * update.squeeze())
        next_event = next_event.at[idx].set(next_event[idx - 1] + mask.squeeze() * holding_time.squeeze())
        return next_event, traj, key

    '''Fixing time index'''
    T = 100
    steps = T * 100
    
    '''Allocating arrays to make life easier for JIT compiler, but shouldn't make a huge difference in this case'''
    traj = jnp.zeros((steps, la.size))  # [T, F]*0
    next_event = jnp.zeros((steps, 1))  # [T, 1]*0
    
    '''Single function transformation makes things go vroooom'''
    sampling = jax.jit(sampling) if jit else sampling

    for idx, t_ in enumerate(tqdm(jnp.linspace(0, T, steps))):
        next_event, traj, key = sampling(t_, idx, next_event, traj, la, mu, key)

    t = jnp.linspace(0, T, steps).reshape(-1, 1)
    return t, traj


t, traj = fast_matrix_birth_death_process(key, la, mu, jit=False)
fig = plot_ctmc(t, traj, la=la, mu=mu)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you run the notebook cell above in Visual Studio Code, and switch the jit flag in &lt;code class=&quot;highlighter-rouge&quot;&gt;fast_matrix_birth_death_process&lt;/code&gt; you will encounter that the jited code block will accelerate the execution to &lt;strong&gt;0.5 seconds&lt;/strong&gt; whereas the unjitted version takes a stupendous &lt;strong&gt;23.9 seconds&lt;/strong&gt;. That’s a &lt;strong&gt;50X&lt;/strong&gt; speed up by merely introducing a single function call more.&lt;/p&gt;

&lt;h3 id=&quot;making-parallel-evaluations-a-breeze-with-vmap&quot;&gt;Making parallel evaluations a breeze with &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Next up is the simple but powerful functionality of &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Sampling a single process is all nice and stuff, but as soon as we’re dealing with probabilistic systems, we need to draw samples,
In classic numpy or PyTorch we would know rewrite the code from single operations to batch-able operations.
If you’re smart, you’d always written your code with parallel evaluations in mind, such as a matrix-vector product is just a matrix-matrix product with a single column in the second matrix and so on and so forth.&lt;/p&gt;

&lt;p&gt;With &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; we have to introduce a single modified line to make parallel evaluations of the same function on different data (SIMD: single instruction multiple data) a reality.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'''New birth death rates'''
la = jnp.array([0.1, 1, 2])
mu = jnp.round(la - 0.1 * la, 2)  # slightly smaller death process

key = jax.random.PRNGKey(5)
num_process_samples = 4
key, *process_samples_key = jax.random.split(key, 1+num_process_samples)
'''Stack four PRNG keys via to [4,2] with the first dimension being the vmap dimension'''
process_samples_key = jnp.stack(process_samples_key)

'''Wrap the fast_matrix_birth_death_process() function by fixing la and mu parameters and making process_samples_key the vmap argument'''
t, traj = jax.vmap(lambda key: fast_matrix_birth_death_process(key, la, mu))(process_samples_key)
f'{t.shape=} and {traj.shape=}'
fig = plot_ctmc(t, traj, la=la, mu=mu)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;jax.vmap&lt;/code&gt; command acts as a wrapper around our sampling function.
It says that we keep the rates &lt;code class=&quot;highlighter-rouge&quot;&gt;la&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; fixed and the &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; wrapped function takes a only the sampling keys.
A single pseudo random number generator key is of shape &lt;code class=&quot;highlighter-rouge&quot;&gt;(2,)&lt;/code&gt;.
By stacking the keys we obtain an array of PRNG keys of shape &lt;code class=&quot;highlighter-rouge&quot;&gt;(4,2)&lt;/code&gt;.
&lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; will automatically recognize the added dimension and evaluate the same function in parallel for each of the 4 PRNG keys.&lt;/p&gt;

&lt;p&gt;Executing the sampling function with &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt;, it takes us &lt;strong&gt;2.3 seconds&lt;/strong&gt; while the unjitted version takes a whopping &lt;strong&gt;106 seconds&lt;/strong&gt;, again the speed up factor of &lt;strong&gt;50X&lt;/strong&gt;.
One caveat is that jiting loops themselves is usually a bad idea as the compiler will trace the entire slow loop and then optimize it.
It usually better to &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt; code blocks in for loops as the jit tracer doesn’t have to unroll the loops and subsequently optimize them.
Using JIT on our little for loop in the sampling function predicted a &lt;strong&gt;5 minutes&lt;/strong&gt; run time for the first evaluation as it executes and traces the slow unjitted version of the function on its first call.
I tried recurseively jiting and vmaping hoping that &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt; would reuse previously jited code blocks but that’s apparently not the case and it executes only the outer most &lt;code class=&quot;highlighter-rouge&quot;&gt;jit&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Where things get really funky is once you start to realize that you can &lt;code class=&quot;highlighter-rouge&quot;&gt;vmap&lt;/code&gt; over all sorts of things.
Let’s assume you have different birth death rates, you can then do the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'''Vmap over parameters'''

key = jax.random.PRNGKey(5)
la = jnp.array([[0.01, 0.01, 0.01], [0.01, 1, 10], [0.1, 1, 2], [0.5, 1., 2.]]) # [4, 3] birth rate set
mu = jnp.round(la - 0.01 * la, 2) # slightly smaller [4,3] death rate set
print(la.shape, mu.shape)

num_process_samples = la.shape[0]*5 # draw 4 x 5 samples to get 20 processes with distinct behaviour
key, *process_samples_key = jax.random.split(key, 1+num_process_samples)
process_samples_key = jnp.stack(process_samples_key).reshape(la.shape[0],5,2) # reshape to [4, 5, 2] random keys

'''
Outer/First vmap takes keys [4,5,2], birth rates [4,3] and death rates [4,3]
Outer/First vmap has 4 vmap inputs
Inner/Second vmap takes vmapped keys [5,2] for fixed birth rates [3] and death rates [3]
Outer/First vmap strips away the outer most dimension (=4)
Inner/Second vmap maps only over vmap-dimension of keys (second key dimension=5)
'''
vmap_matrix_birth_death_process = jax.vmap(lambda key, la, mu: jax.vmap(lambda key: fast_matrix_birth_death_process(key, la, mu))(key)) 
t, traj = vmap_matrix_birth_death_process(process_samples_key, la, mu) # ([4,5,2], [4,3], [4,3])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;traj.shape=[params, samples, time, F]
Loop over params dimension as plot_ctmc is only laid out for [Samples, T, F] data&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for t_, traj_, la_, mu_ in zip(t, traj, la, mu): # [params=4, samples=5, time=T, features=F]
	fig = plot_ctmc(t_, traj_, la=la_, mu=mu_)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and with a few extra lines of code and proper vmaping we quickly have the verification plots&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../blog/CTMC/vmap1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../../blog/CTMC/vmap2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../../blog/CTMC/vmap3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../../blog/CTMC/vmap4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">jax.jit(jax.vmap(x=Discrete Space, t=Continuous Time))</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/CTMC/vmap4.png" /></entry><entry><title type="html">Stein’s Lemma for Trace Estimation</title><link href="http://localhost:4000/blog/TraceEstimation/" rel="alternate" type="text/html" title="Stein's Lemma for Trace Estimation" /><published>2022-12-09T00:00:00+01:00</published><updated>2022-12-09T00:00:00+01:00</updated><id>http://localhost:4000/blog/TraceEstimation</id><content type="html" xml:base="http://localhost:4000/blog/TraceEstimation/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\def\tr#1{\text{Tr}\left[ #1 \right]}
 \def\Efunc#1{\mathbb{E}\left[ #1\right]}
 \def\Efuncc#1#2{\mathbb{E}_{#1}\left[ #2 \right]}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-trace-of-a-matrix&quot;&gt;The Trace of a Matrix&lt;/h3&gt;

&lt;p&gt;For a square matrix $A \in \mathbb{R}^{d \times d}$ the trace is defined as
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\tr{A} = \sum_i^d A_{ii}
\end{align}&lt;/script&gt;
which sums over the diagonal terms of the matrix $A$. Plain and simple.&lt;/p&gt;

&lt;h3 id=&quot;hutchinsons-stochastic-trace-estimation&quot;&gt;Hutchinson’s Stochastic Trace Estimation&lt;/h3&gt;

&lt;p&gt;By definition we are only interested in the diagonal terms of a matrix when computing the trace of it.
But in cases where the matrix is computationally expensive to compute we might want to approximate it.&lt;/p&gt;

&lt;p&gt;Given a matrix $A$ one might think why the stochastic estimation is necessary when all we need to do is sum up the diagonal terms.
But Hutchinson’s trick can unfold its full potential when leveraging the specific structure of the matrix $A$.
Just wait until the Jacobian joins the party down below.&lt;/p&gt;

&lt;p&gt;We can approximate the exact trace with a stochastic estimate.
We therefore sample from $Z \in \mathbb{R}^D$, the mean of which is a zero vector and the covariance matrix is a identity matrix, i.e. $\Sigma[Z] = I$.
More precisely we determine the covariance matrix as
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \Sigma[Z] 
    &amp;= \Efunc{(z - \Efunc{Z})(z - \Efunc{Z})^T}\\
    &amp;= \Efunc{zz^T} - \Efunc{Z} \Efunc{Z}^T \\
    &amp;= \Efunc{zz^T} \\
    &amp;= I
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The Rademacher distribution which samples from the set ${-1, +1}$ with equal probability offers the lowest estimator variance and is commonly used in the trace estimation trick for this reason.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \text{Tr}[A]
    &amp;= \text{Tr}[I A] \\
    &amp;= \text{Tr}[\Efuncc{z \sim p(z)}{z z^T} A] \\
    &amp;= \Efuncc{z \sim p(z)}{\text{Tr}{z z^T A}} \\
    &amp;= \Efuncc{z \sim p(z)}{\text{Tr}{z^T A z}} \\
    &amp;= \Efuncc{z \sim p(z)}{z^T A z} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where the trace operator disappears as $z^T A z \in \mathbb{R}$ is a scalar value for which the trace is a superfluous operation.&lt;/p&gt;

&lt;p&gt;For estimating the trace of the Jacobian, we can circumvent the quadratic nature of the Jacobian by reducing the network output with a random vector z to a scalar, which can then be readily derived with a single backward pass.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \text{Tr}[J_f(x)]
    &amp;= \Efuncc{z \sim p(z)}{z^T J_f(x) z} \\
    &amp;= \Efuncc{z \sim p(z)}{z^T \nabla_x [f(x)^T] z} \\
    &amp;= \Efuncc{z \sim p(z)}{z^T \nabla_x [f(x)^T z] } \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The important piece of information lies with the contraction $f(x)^T x$ which is an inner product.
Naively in equation (12), we would compute the full Jacobian matrix $J_f(x)$ and then contract it.
But since $z$ is a constant quantity for each sample in the expectancy, we can instead interpret $z$ as a constant scaling factor in the derivation of each output to each input which so happens to contract the full matrix.
You can think of it as a inner product of random vectors in which the Jacobian matrix provides the metric tensor.
So instead of Jacobian matrix times vector, we suddenly have a derivative of the scalar $J_f(x)^T z$.
The Jacobian evaluation $J_f(x): \mathbb{R}^\mathcal{X} \rightarrow \mathbb{R}^{\mathcal{X}\times \mathcal{Y}}$ reduces to the stochastic $\nabla_x [ f(x)^T z ]: \mathbb{R} \rightarrow \mathbb{R}^\mathcal{X}$.
Thus we saved us a lot of computations.
There is obviously a price to pay, namely that we’re working with stochastic evaluations which introduces the curse of dimensionality into our evaluation.&lt;/p&gt;

&lt;h3 id=&quot;stein-is-entering-the-picture&quot;&gt;Stein is entering the picture&lt;/h3&gt;

&lt;p&gt;Let $X \in \mathbb{R}^N$ be a normally distributed random variable $p(x) =\mathcal{N}(x ; \mu, \sigma^2)$ with mean $\mu$ and variance $\sigma^2$.
Let the derivative of the normal distribution with respect to $x$ be
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\partial_x p(x) 
&amp;= \partial_x \left[\frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \right]\\
&amp;= -\frac{(x-\mu)}{\sigma^2} \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
&amp;= - \frac{(x-\mu)}{\sigma^2} p(x).
\end{align} %]]&gt;&lt;/script&gt;
Integration by parts (IbP) serves as a inverse of the product rule $\partial_x [u(x) v(x)] = \partial_x u(x) v(x) + u(x) \partial_x v(x)$ namely 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
[u(x)v(x)]_{x=-\infty}^{\infty} &amp;= \int_{x=-\infty}^{\infty} u(x) \partial_x v(x) + \partial_x u(x) v(x) dx \\
&amp;= \int_{x=-\infty}^{\infty} u(x) \partial_x v(x) dx + \int_{x=-\infty}^{\infty} \partial_x u(x) v(x) dx
\end{align} %]]&gt;&lt;/script&gt;
which yields the often used identity
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\int_{x=-\infty}^{\infty} u(x) \partial_x v(x) dx 
&amp;= [u(x)v(x)]_{x=-\infty}^{\infty} - \int_{x=-\infty}^{\infty} \partial_x u(x) v(x) dx.
\end{align} %]]&gt;&lt;/script&gt;
In practice, the property that either $u(x)$ or $v(x)$ or both evaluate to zero at $x = \pm \infty$ as it is the case with common probability distributions is leveraged as an algebraic trick to ‘switch the derivative to the other function’.&lt;/p&gt;

&lt;p&gt;Given a function $g(x)$ we can obtain a gradient estimator with the following steps via integration by parts
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    \Efuncc{p(x))}{g(x) ( x - \mu)}
    &amp;= \int g(x) (x-\mu) \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx\\
    &amp;= \int g(x) (x-\mu) \frac{-\sigma^2}{-\sigma^2}\frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \\
    &amp;= -\sigma^2 \int g(x) \underbrace{\frac{(x-\mu)}{-\sigma^2}\frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}}_{\partial_x p(x)} dx \\
    &amp;= - \sigma^2 \underbrace{\int g(x) \partial_x p(x) dx}_{\text{IbP}} \\
    &amp;= -\sigma^2 \big\{ \underbrace{[ g(x) p(x)]_{x=-\infty}^{\infty}}_{p(\pm \infty)=0} - \int \partial_x g(x) p(x) dx \big\} \\
    &amp;= \sigma^2 \int \partial_x g(x) p(x) dx \\
    &amp;= \sigma^2 \Efuncc{p(x)}{\partial_x g(x)}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;trace-estimation-with-steins-lemma&quot;&gt;Trace Estimation with Stein’s Lemma&lt;/h3&gt;

&lt;p&gt;By choosing a perturbation $\epsilon \sim p(0, \sigma_\epsilon^2)$ with zero mean and a small variance $\sigma_\epsilon^2$ we can define a perturbed data point $x’ \sim p(x,\sigma_\epsilon^2)$ via $x’ = x + \epsilon$.
This transforms Stein’s lemma into
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
    &amp;\Efuncc{p(\nu))}{g(x') ( x' - x)}
    = \Efuncc{p(\epsilon))}{g(x + \epsilon) \epsilon}
    = \sigma_\epsilon^2 \Efuncc{p(\epsilon)}{\partial_{x'} g(x')}.
\end{align} %]]&gt;&lt;/script&gt;
In practice we rescale with $1/\sigma_\epsilon^2$ and evaluate the left side of the following identity
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \Efuncc{p(\epsilon)}{g(x + \epsilon) \frac{\epsilon}{\sigma_\epsilon^2}} = \Efuncc{p(\epsilon)}{\partial_{x+\epsilon} g(x+\epsilon)}.
\end{align}&lt;/script&gt;
which gives us an estimator of the gradient $\partial_x g(x)$ by averaging the gradients in the $\epsilon$-neighborhood of $x$.
For a function $g: \mathbb{R}^M \rightarrow \mathbb{R}^N$, the gradient estimation with Stein’s lemma estimates the trace of the Jacobian $J_g(x+\epsilon)$
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \Efuncc{p(\epsilon)}{g(x + \epsilon) \frac{\epsilon}{\sigma_\epsilon^2}} = \Efuncc{p(\epsilon)}{\text{Tr}\left[ J_g(x+\epsilon)\right]}.
\end{align}&lt;/script&gt;
In the limit of $\sigma_\epsilon \rightarrow 0$ we obtain the trace estimator
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \text{Tr}\left[ J_g(x) \right] 
    = \lim_{\sigma_\epsilon \downarrow 0} \Efuncc{p(\epsilon)}{\text{Tr}\left[ J_g(x+\epsilon)\right]}
    = \lim_{\sigma_\epsilon \downarrow 0} \Efuncc{p(\epsilon)}{g(x + \epsilon) \frac{\epsilon}{\sigma_\epsilon^2}}
\end{align}&lt;/script&gt;
in which we compute the right most term to obtain the left most term.&lt;/p&gt;

&lt;!-- The scaling of the perturbation scale $\sigma_\epsilon$ offers at least in theory intriguing similarities to the forward diffusive process of diffusion models.
These models estimate the scores of the data distribution $x'_t \sim p(x, \sigma_t^2)$ in which $x$ is a sample from the true data distribution which is being modelled and the perturbation scale $\sigma_t$ is time dependent which decreases as the generative process is integrated in time.
Thus to stabilize the score estimation in higher dimensions we aim to to make the perturbation scale in the Stein trace estimator time dependent. --&gt;</content><author><name></name></author><summary type="html">Warning: May contain traces of nuts (and matrices)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/blogthumbnails/stein.png" /></entry><entry><title type="html">Money Laundering</title><link href="http://localhost:4000/blog/moneylaundering/" rel="alternate" type="text/html" title="Money Laundering" /><published>2022-11-26T00:00:00+01:00</published><updated>2022-11-26T00:00:00+01:00</updated><id>http://localhost:4000/blog/moneylaundering</id><content type="html" xml:base="http://localhost:4000/blog/moneylaundering/">&lt;h3 id=&quot;why-should-you-put-your-bills-in-the-laundry-machine&quot;&gt;Why should you put your bills in the laundry machine?&lt;/h3&gt;

&lt;p&gt;Obviously, the spread of Covid-19 has made us all very sensitive to germs and viruses.
Cash changes hands a lot of times and thus is the perfect medium to transmit diseases.
… yeah, I’m actually talking about the art of financial subterfuge obscuring the illicit origin of illegally obtained money.&lt;/p&gt;

&lt;p&gt;Apart from their ideologically motivated peers, criminals are driven by one thing: profit. 
Greed is the principal goal, but people that require money laundering skills don’t sell stolen chewing gum on the school yard.
Instead, they have millions or even billions originating from illegal sources which need to be moved into official, legitimate and unsuspecting bank accounts of the criminals.
Thus the money needs to be able to be handled just like any other legitimate money.&lt;/p&gt;

&lt;p&gt;The problem is, as soon as a sovereign (and hopefully clean) police force would ascertain that an investment was done with money originating from illicit sources, they are most of the time allowed to seize the investments.&lt;/p&gt;

&lt;p&gt;The goal is now to disguise the financial assets so they can be used without detection of the illegal activity that produced them. 
The term ‘money laundering’ describes the process of transforming the monetary proceeds derived from criminal activity into funds of an apparently legal origin.&lt;/p&gt;

&lt;p&gt;Who is a potential user of money laundering? 
Drug cartels, terrorists, mobsters or the mafia, human traffickers, illegal arms dealers and the list goes on and on.&lt;/p&gt;

&lt;h3 id=&quot;money-laundering-methods&quot;&gt;Money Laundering Methods&lt;/h3&gt;

&lt;p&gt;In principal, money laundering consists of three distinct stages: placement, layering, and integration.
Personally, I use the terms entry, obfuscation, and integration which are more vivid terms in my opinion.&lt;/p&gt;

&lt;p&gt;The first step is to place the money at the entry points of the modern financial system.
If you got cash, you get ‘smurfs’ which repeatedly deposit money amounts just below the reporting line.
Usually, deposits below 10.000 USD don’t have to be reported in the US, which led to bank tellers close to the Mexican border on the US side receiving up to 400.000 USD of deposits per week in cartel-linked accounts parceled out in 10k portions and deposited by local Americans to reduce suspicion.&lt;/p&gt;

&lt;p&gt;The second step is to obfuscate the origin of the cash.
You start moving money through different accounts or start to create round-about invoices with complicit companies.
For example, a smurf would deposit money in a bank account or would pay a bogus invoice for a company that only exists on paper.
Then the fake company would pay a fake invoice of another company which would then have an apparently legitimate income stream.&lt;/p&gt;

&lt;p&gt;Layering or obfuscation serves the purpose of creating a fake, seemingly legitimate origin of the funds.
Once a fake story for the money has been created, the main financial investments can be accomplished.
Imagine a suspicious person trying to buy a condominium in New York with a pile of unexplained cash and a real estate company headquartered in the Bahamas buying the apartment next to him.
The real estate company got its capital from the drug trade, human trafficking or sanction-busting arms trade just like the individual next door, but the police would be way more interested in the sole person with 15 million USD in unexplained cash and an unexplained stream of income.
The real estate company first laundered its capital through multiple layers of obfuscation and wouldn’t provide a reasonable doubt for the origin of its money.
It’s basically all about appearances.&lt;/p&gt;

&lt;p&gt;I’ll outline some of the more common money laundering schemes.
It’s important to note that dubious origin of criminally obtained money can in theory always be traced back.
But the money launderers put in place so many layering steps that when law enforcement turns around the 20th corner and still has an apparently legit money stream, they’ll eventually give up.&lt;/p&gt;

&lt;p&gt;You can have fake trials in which both parties are in on the jig and ultimately under your control.
The defendant uses your illicit cash to pay the plaintiff the reimbursements.
But, lo and behold, both the plaintiff and the defendant are just conduits with the damages paid creating a clean income receipt.
If the police comes knocking, you can present the receipt of the indemnities.&lt;/p&gt;

&lt;p&gt;Exaggerated or even fake invoices which can include non-existent services.
Consulting on intellectual property is a favourite scheme, as the intellectual property is hard to value.
Since the worth of intellectual property is more symbolic or even intangible you can pay whatever you want for consultation.
I’ll outline an interesting scheme that was concocted to launder money in Germany using the sales of gangster rap music.
Organized crime supported musicians and ordered their drug dealers, respectively and more formal vendors of miscellaneous illegal wares, to accept purchases of the musicians work in exchange for illicit goods.
Instead of paying for drugs, they paid for intellectual property of the artist and thus the organized crime group had a steady inflow of money from music sales, which in itself is a legit business.
This was expanded to bot farms in the era of music streaming with bot farms which are controlled out of the dark part of the internet anyway.
The bot farms listened repeatedly to a musicians music who was under contract with an organized crime group.
Thus Spotify paid the artist for the increased number of streams and the OC group got their cut.&lt;/p&gt;

&lt;p&gt;Alternatively Mr. Criminal can buy real estate way below the market value with the difference to the full market price price being paid in illegal cash. 
Afterwards, the property is quickly sold for the appropriate price on the open market.
The illicit cash is given to the original owner and the money launderer creates a bogus profit via the sale at market level.&lt;/p&gt;

&lt;p&gt;You can convert illegal cash to casino jetons and a little bit is used for gambling.
Since gambling is intrinsically stochastic nobody can really trace back when you lost or when you won more money.
After some time, the casino jetons are cashed back in and you get a receipt that you’re apparently an awesome gambler.&lt;/p&gt;

&lt;p&gt;The most straightforward method is still fictitious business activity of cash-based enterprises.
There, the illegal cash is simply mixed into the income stream proportional to the untainted money stream.
I mean proportional since even the most amazing taco truck usually doesn’t make 200 million USD in revenue per year.
The usual suspects are dry cleaning shops, casinos, supermarkets, drug stores, car shops, and anything that has a largely cash-based income stream.&lt;/p&gt;

&lt;p&gt;Auctions of hard-to-value objects have also become a popular method and especially the art market has been captured.
You basically let two fake personas outbid each other with your own illegal cash.
The object with little intrinsic value is offered by you and the bidders get your illegal cash.
The sale creates a fake yet clean receipt.
Funnily enough, auction houses are exempt from money laundering prevention laws and you can often see people on phones bidding millions for an anonymous bidder watching it all from his or her sofa.&lt;/p&gt;

&lt;h3 id=&quot;offshore-jurisdictions-and-banking&quot;&gt;Offshore Jurisdictions and Banking&lt;/h3&gt;

&lt;p&gt;The principal safeguard against money laundering is the stringent enforcement of Know-Your-Customer (KYC) and Anti Money Laundering (AML) laws.
Ideally, the entry points to the financial system should have gatekeepers who check for potentially illegal sources of money.
Obviously, there is a conflict of interest since a lot of money means a lot of business for banks as was shown by HSBC scandal with the cartel money or the Raul Salinas story.&lt;/p&gt;

&lt;p&gt;Fortunately, a whole industry sector has blossomed between the beaches of the Caribbean, the mountains of the Alps, and in general small insignificant countries selling the legislative sovereignty to the highest bidder: the offshore finance industry.
It’s basically a sunny places for shady people.&lt;/p&gt;

&lt;p&gt;Offshore finance centers implement effectively no regulation of the financial industry within their borders and similarly implement convenient financial secrecy laws by allowing trusts, straight-out banking secrecy limiting the information of who actually owns a bank account, nominee directors of shell companies hiding the true owner of a company, and the like.&lt;/p&gt;

&lt;p&gt;By using the intricacy of modern tax codes, you can create a multitude of opaque companies which move money between them for no apparent reason.
For law enforcement, proving that the money being moved around is originally from illicit conduct is close to impossible.
Since innocence until proven wrong is a tenet in well-functioning judicial systems, it becomes close to impossible to prove the illegal source of the money.&lt;/p&gt;

&lt;p&gt;Using international banking, one could use mirror trades like the ones Deutsche Bank executed for Russian customers in 2007.
Two well-capitalized accounts, one in London and one in Moscow, would be under the control of the same entity at Deutsche Bank.
The Moscow based account would use dirty money to buy stocks in a company and Deutsche Bank holds the stocks as a broker.
Via ‘remote booking’ the Moscow front office would then conduct a sale of the same stocks in the London market place for an apparently unaffiliated entity in London obtaining euros or dollars in that brokerage account.
The money to buy the stocks in London in the first place can be obtained via loans in Russia for example.
But since the London entity was controlled by the same person, they spend Roubles in Moscow and obtained and an equal amount of Dollars in a financial offshore jurisdiction.
The sale of the stocks in London created a squeaky-clean bill of financial health for the Dollars.&lt;/p&gt;

&lt;p&gt;Alternatively, you can also buy options in the derivative market for which you pay a premium.
These options have intrinsic value and can be sold, such that the dirty money can be used for the premium payments and the option itself can be sold for a higher value.&lt;/p&gt;

&lt;p&gt;Or you could use looped swap contracts across borders.
For example, interest rate swaps make one party pay the London Interbank Offered Rate (the LIBOR, itself subject to major manipulations at the cost of millions of people as portrayed in ‘The Spider Network’ by David Enrich) while the other party pays a fixed rate and thus they ‘swap’ interest rates.
If you now chain these swaps in a circle, you can easily move money with a valid, seemingly clean contract from Moscow to London and then to Moscow back again.
Of course, the round-tripping is not obfuscated but each party seemingly gets a clean origin of money.&lt;/p&gt;

&lt;p&gt;Once the money has been safely moved into offshore financial centers it can then be used to invest.
The irony of many international active criminals is that they move their wealth out of their often unstable countries and park it in luxury real estate in judicially stable places like London, Paris, and New York.
There has been a rise in super tall skyscrapers in New York City which is commonly referred to as ‘Billionaire’s Row’.
But while these apartments offer amazing views of the City high above the problems of ordinary people, the ownership is more than opaque.
A majority of the luxury real estate is bought by shell companies and they are often occupied only a couple of days of the year.
Since they were legitimately bought, and the front companies in secrecy jurisdictions buying them being nearly impossible to crack open, it is next to impossible to prove that they had been bought with money obtained through illegal means.&lt;/p&gt;

&lt;p&gt;Especially in the case of the US, there is an interesting historical anecdote to tell.
Before 9/11, not a lot of people were interested in anti-money-laundering (AML) legislation.
But once the terrorist attacks on 9/11 occurred, everybody suddenly became very interested in how it was possible that a terrorist organization was able to pay for the flight school of the terrorist hijackers.
Somehow money was funneled half-way around the world out of Aghanistan into the US and used to pay for hotels, rental cars, flight schools and flights in and out of the US.
It’s not exactly that Al-Qaida was selling hot dogs in front of the Metropolitan Museum in Central Park.
Thus stringent AML laws were passed targeting every possible nook and cranny in which illegal money could be parked.
Included were luxury real estate and luxury products such that prior to a transaction in these industries KYC procedures and other AML laws had to be conducted.&lt;/p&gt;

&lt;p&gt;But a short while later, precisely these luxury industries were exempt from KYC and AML procedures.
The industries lobbied lawmakers to exempt them such that they could continue to sell luxury real estate and products to customers they knew little about.
To be fair, at a certain wealth you might want peace of mind and privacy and not see your latest purchase plastered over the morning edition of a tabloid.
Nevertheless, it is a peculiar coincidence precisely because luxury items give you the ability to store large amounts of money in a small number of objects.
Semi-fancy art paintings for 25 Million USD in an auction, a super expensive apartment in the UK or a gold-plated Lamborghini, none of these transactions require a stringent KYC procedure now anymore.&lt;/p&gt;

&lt;h3 id=&quot;reputation-laundering&quot;&gt;Reputation Laundering&lt;/h3&gt;

&lt;p&gt;But what do you do after you’ve laundered all your money?
Essentially you have a criminal or a criminally connected person with large amounts of cash.
After you laundered your money, it’s now time to launder your reputation.&lt;/p&gt;

&lt;p&gt;A good example of this is the story of Dmitry Firtash.
In essence, Dmytro Firtash is just a puppet for Semion Mogilevich, the mastermind of the Russian mob.
He came out of nowhere and was suddenly made co-owner of RosUkrEnergo, which is an intermediary that for some unknown reason gets to buy gas cheaply from Gazprom and sell expensively to Europe.
Since Gazprom is solidly in the hand of the Kremlin, it is assumed that these intermediary organizations pocket the difference and transfer it to Putin’s kleptocracy.
Dmitry Firtash came out of nowhere within a position nobody could really explain resulting in money that is a bit shady, to say the least.
Nevertheless, he quickly ingratiated himself with the who is who in the UK’s public sphere.&lt;/p&gt;

&lt;p&gt;He went through all the necessary procedures that I will outline down below to become one of London’s elite and seemingly respected members.
The first step was to obviously buy expensive real estate which gives ample room to host dinners and invite important people.
This lets you put roots down and people consider you a member of London’s exclusive super-rich social circle.
Next, you hire a public relations firm ( in itself a more than a weird concept) that will manage your public image.
They will put you in touch with members of parliament and other prestigious people which will serve as directors for you charity or foundation which you obviously founded only for the reason to pay prestigious people.
The charity, foundation or whatever should be nice and uncontroversial when people think of it: be socially active, give out stipends, build sports facilities or anything that gives you a nice PR photo op.&lt;/p&gt;

&lt;p&gt;Given that you have paid a company to give you a good image with a suitable background, the PR firm will continue to actively forge new relationships.
You host dinners with ex-politicians with a powerful rolodex, you cast your dubious conglomerate as a useful and relevant aid to the national interests of the country.
Commodities, energy, media, and construction are of interest to a country but are hardly irreplaceable.
Nevertheless, you are now a useful member of society and worth keeping in the fold.
Another step is to be philanthropic in high society circles by sponsoring new buildings for colleges, preferably Oxford, Cambridge, and the London colleges like UCL.
Dmitry Firtash founded a Ukrainian studies program that brought him in close contact with exclusive circles of Oxbridge.
Given that London’s elite is definitely to almost exclusively educated at Oxbridge that brings you in fortuitous contact with a little of influential people.&lt;/p&gt;

&lt;p&gt;The first aim of these PR shenanigans is to make the client too well-connected and too important to discard.
The second part of reputation laundering in the UK is to use its potent libel laws to make reporting on the source of these people’s wealth impossible.&lt;/p&gt;

&lt;h3 id=&quot;libel-law--tourism&quot;&gt;Libel Law &amp;amp; Tourism&lt;/h3&gt;

&lt;p&gt;Ultra-wealthy people don’t just travel to the UK to park their money there but also like to travel there to suffocate any reporting on their dodgy wealth.&lt;/p&gt;

&lt;p&gt;English law allows them to sue for libel for any published statements which are allegedly defaming a named or identifiable individual if these have caused a loss in their trade or profession or just plainly damage their reputation.
This is pretty standard around the world and is present in almost any judicial code.
What sets the UK apart is that in a libel case, the allegedly defamatory statement is presumed to be false unless the defendant can prove its veracity.&lt;/p&gt;

&lt;p&gt;This has conferred the status of ‘most favored nation’ (to borrow a WTO term) onto the UK from ultra-high net worth people with questionable sources of income.
In essence, the burden of proof lies with the defendant such that even the slightest insinuation is impossible unless is airtight and reinforced with proof.
Any sentence like ‘ … who has reportedly close dealings with the Russian mob …’ could set loose the legal hounds upon the publication outfit under which auspice this sentence was printed.
Just imagine a multi-billionaire going to court against an investigative newspaper with the billionaire plaintiff being able to bankrupt the investigative newspaper through legal costs alone.&lt;/p&gt;

&lt;p&gt;A good example of this is the excellently written book ‘Putin’s Kleptocracy - Who owns Russia’ by Karen Dawisha which was essentially shunned by every publisher in the UK for fear of being bankrupted by legal proceedings and ultimately had to be published in the US.
The rebellious colony across the Atlantic had passed the SPEECH Act of 2010 which made foreign libel judgements without equivalent free speech enforcements unenforcible in the US.&lt;/p&gt;

&lt;p&gt;To top it out, a public official or public figure as a plaintiff has to prove actual malice in writing possible ‘slander’ causing professional or reputational harm.
A private individual instead must only prove negligence to be able to win the libel case.
Especially in the case of off-shore finance and other dealings taking place in the dark, nothing is ever fully known and the slightest mistake which can be exploited by the plaintiff as negligence can lead to bankrupting the entire publishing outfit or journalist.&lt;/p&gt;

&lt;p&gt;The misuse of libel lawsuits become so egregious that the libel law slightly reformed in 2013.
While it made winning libel cases harder for plaintiffs as they now had to prove actual and objective harm, it left the legal costs unchanged.
Simply maintaining a legal team which can hold its own against high-powered legal advice from oligarchs would bankrupt the defendant.&lt;/p&gt;

&lt;p&gt;Unfortunately, this is not limited to news papers.
Even the National Crime Agency, the UK’s main police force for economic crime and organized crime, is capped by litigious oligarchs.
In a more than questionable legal setup the NCA is required to pay the full legal fees in case of a loss in court of the defendant.
Given that shady ultra-high net worth people are financially able to employ law firms of the likes of Mishcon de Reya, the legal fees quickly become too much to bear by the public office.
Thus the effect is that the economic crime division of the UK is legally hamstrung to prosecute shady billionaires afraid it will loose its entire annual budget to a single criminal case.&lt;/p&gt;

&lt;p&gt;The annual budget of the NCA is around 4.3 million pounds.&lt;/p&gt;

&lt;p&gt;Oligarchs buy third houses in Kensington and Mayfair for 80 million pounds …&lt;/p&gt;</content><author><name></name></author><category term="globalizedfinance" /><summary type="html">Greenbacks, Soap and Hopefully No Questions</summary></entry></feed>