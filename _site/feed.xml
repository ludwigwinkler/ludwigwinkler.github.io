<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-22T18:24:25+02:00</updated><id>http://localhost:4000/</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">The Adjoint Method in Neural Ordinary Differential Equations</title><link href="http://localhost:4000/blog/AdjointMethod/" rel="alternate" type="text/html" title="The Adjoint Method in Neural Ordinary Differential Equations" /><published>2020-05-20T00:00:00+02:00</published><updated>2020-05-20T00:00:00+02:00</updated><id>http://localhost:4000/blog/AdjointMethod</id><content type="html" xml:base="http://localhost:4000/blog/AdjointMethod/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h3 id=&quot;the-adjoint-method-in-neural-ordinary-differential-equations&quot;&gt;The Adjoint Method in Neural Ordinary Differential Equations&lt;/h3&gt;

&lt;p&gt;Back at NeurIPS 2018 the best paper award was given to the authors of &lt;a href=&quot;https://arxiv.org/pdf/1806.07366.pdf&quot;&gt;Neural Ordinary Differential Equations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The motivation of the paper was the mainly given through the interpretation of the ResNet architectures being interpreted as the Euler discretization of ordinary differential equations (ODE).
The paper took this insight to its logical extreme and asked the question whether we really have to remain content with just the Euler discretization of an ODE or whether we can go deeper … or more continuous in our case.&lt;/p&gt;

&lt;p&gt;By definition an ODE is defined in its differential form as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dz_t = f(z_t, t, \theta)&lt;/script&gt;

&lt;p&gt;which basically means that the function $f$ computes the rate of change of $z_t$ at timestep $t$ with its parameters $\theta$.
Often these equations $f$ are constructed analytically or are known from physics but a more interesting question cane be posed by asking whether this function $f$ can actually be learned … with a neural network for example.&lt;/p&gt;

&lt;p&gt;Ultimately, if we want to use gradient based optimization to train a neural network we need to compute a scalar loss function and compute the gradients of the parameters $\theta$ through reverse-mode autodifferentiation.&lt;/p&gt;

&lt;p&gt;An important part of solving/simulating differential equations is that although they are defined in the continuous space, we have to discretize eventually in order to make the problem amenable to a solution with computers.&lt;/p&gt;

&lt;p&gt;For that reason we will work with four samples ${z_0, z_1, z_2, z_3 }$ from a differential equation as shown in the image below. Our prediction with a neural network will be denoted as ${\hat{z}_1, \hat{z}_2, \hat{z}_3 }$.&lt;/p&gt;

&lt;p&gt;Working with neural networks, we want to obtain gradients which which we can perform gradient descent at the end of the day. In order to do that we will need a scalar cost function on which we can perform reverse-mode autodifferentiation which is very efficient for models with potentially a lot of parameters. If the model were to be very small we could also do forward-mode autodifferentiation but that’s another topic.&lt;/p&gt;

&lt;p&gt;Let’s define such a scalar cost function $\mathcal{L}(\text{ODESolver}(z_0, t_0, t_3, f))$ that takes in $z_0$ and solves the ODE for four timesteps by integrating forward $f(z_t, t, \theta)$ in time until it reaches $t=3$ and compares the prediction with the true values ${z_0, z_1, z_2, z_3 }$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Adjoint/Adjoint1.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now comes the interesting part: How do we actually compute the gradients with respect to the parameters, namely $\frac{\partial \mathcal{L}}{\partial \theta}$?&lt;/p&gt;

&lt;p&gt;The thing is that the parameters $\theta$ occur at multiple timesteps in the prediction.
The key insight is now that we actually have to ask ourselves two questions: “How much did each timestep contribute to the loss?” and “At each timestep how much did each parameter contribute to the loss?”.&lt;/p&gt;

&lt;p&gt;Enter adjoint sensitivity analysis …&lt;/p&gt;

&lt;p&gt;The first question can be answered by examining the sensitivity of the scalar loss with respect to the different timesteps $\frac{\partial \mathcal{L}}{\partial z_t}$.&lt;/p&gt;

&lt;p&gt;The sensitivity $\frac{\partial L}{\partial z_3}$ of the loss with respect to the last timestep can be readily evaluated.
More interesting is how we could propagate the sensitivity backwards in time to all evaluated timesteps.
The solution to this problem is the use of the Jacobian of the output $z_t$ with respect to the input $z_{t-1}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
J(f)= \frac{\partial f(z, t, \theta)}{\partial z} = 
\left[
\begin{array}{cccc}
\frac{\partial f(z, t, \theta)_1}{\partial z_1} &amp; \dots &amp; \frac{\partial f(z, t, \theta)_D}{\partial z_1} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f(z, t, \theta)_1}{\partial z_D} &amp; \dots &amp; \frac{\partial f(z, t, \theta)_D}{\partial z_D} \\
\end{array} 
\right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Jacobian of the function $f$ with respect to the input tells us how sensitive the output is to the input.
Since the solution of the ODE is theoretically an infinite series of evaluations of the neural network $f$ we can similarly backpropagate the initial sensitivity $\frac{\partial \mathcal{L}}{\partial z_t}$ by repeatedly multiplying it with the Jacobian with respect to the input but backward in time, which is an ODE again but this time it’s backwards.
Said differently, we simply reweight the initial sensitivity repeatedly with the Jacobian backwards through time.&lt;/p&gt;

&lt;p&gt;The sensitivity backward pass for our discretized ODE problem would then look something akin to this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial \mathcal{L}}{\partial z_1} =
 \frac{\partial \mathcal{L}}{\partial z_3} 
 \frac{\partial f(z_2, t, \theta)}{\partial z_2} 
 \frac{\partial f(z_1 t, \theta)}{\partial z_1} 
\end{align}&lt;/script&gt;

&lt;p&gt;This procedure is actually very similar to how the normal backpropagation pass is done.
In a neural network we use the chain rule to first compute the gradients for the last layer, and then repeatedly reweight the gradients as they are passed through the network.
Take a three layer network as an example with $y = f_3(f_2(f_1(x, \theta_1), \theta_2), \theta_3)$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Adjoint/Adjoint2.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Computing the gradients for $\theta_1$ from the loss amounts to little more than:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial \mathcal{L}}{\partial \theta_1} =
 \frac{\partial \mathcal{L}}{\partial y_3} 
 \frac{\partial y_3}{\partial y_2} 
 \frac{\partial y_2}{\partial y_1} 
 \frac{\partial y_1}{\partial \theta_1} 
\end{align}&lt;/script&gt;

&lt;p&gt;This is what the authors in the paper refer to as “… which can be thought of as an instantaneous analog of the chain rule.”.
In essence, the adjoint sensitivity pass allows us to propagate the importance of each timestep to the overall loss backwards through time.&lt;/p&gt;

&lt;p&gt;Once we propagated the sensitivity backwards through time, we can answer the second question by computing the gradient of the output with respect to the parameter in question.&lt;/p&gt;

&lt;p&gt;While the authors of the paper use the term &lt;em&gt;adjoint state $a(t)$&lt;/em&gt; I find the term &lt;em&gt;sensitivity $s(t)$&lt;/em&gt; more intuitive and appealing.
The beauty of the adjoint state training became apparent to me when I used sensitivity $s(t)$ in equation (5):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial L}{\partial \theta} = \int_{t_1}^{t_0} s(t)^T \frac{\partial f(z(t), t, \theta)}{\partial \theta} dt
\end{align}&lt;/script&gt;

&lt;p&gt;The integral above states that we scale the gradient of the output $\partial_\theta f(z(t), t, \theta)$ with the sensitivity $s(t)$ to the overall loss.&lt;/p&gt;

&lt;p&gt;Interestingly, the reception by the differential equation community was not as unanimous as one would think as this method has been used for a fairly long time. The key insight was its application to neural networks since we only need the Jacobians of the neural network irrespective of what goes on inside the neural network. One of the coauthors said as much in a &lt;a href=&quot;https://www.youtube.com/watch?v=YZ-_E7A3V2w&quot;&gt;talk a year later&lt;/a&gt;.&lt;/p&gt;</content><summary type="html">Reverse-Mode Sensitivity Training</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/Adjoint/Adjoint1.png" /></entry><entry><title type="html">Solving (Some) SDEs</title><link href="http://localhost:4000/blog/SolvingSDEs/" rel="alternate" type="text/html" title="Solving (Some) SDEs" /><published>2020-04-14T00:00:00+02:00</published><updated>2020-04-14T00:00:00+02:00</updated><id>http://localhost:4000/blog/SolvingSDEs</id><content type="html" xml:base="http://localhost:4000/blog/SolvingSDEs/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h3 id=&quot;geometric-brownian-motion&quot;&gt;Geometric Brownian Motion&lt;/h3&gt;

&lt;p&gt;Brownian motion can have both positive and negative values as long as its mean is centered around zero and the distribution over time follows the characteristics of the Wiener process.
Yet certain quantities can only have positive values such as stocks.&lt;/p&gt;

&lt;p&gt;In order to accomodate the specific requires of such quantities we can work with geometric Brownian motion,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	dS_t &amp;= \mu_t S_t dt + \sigma_t S_t dW_t \\
	\frac{dS_t}{S_t} &amp;= \mu_t dt + \sigma_t dW_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which has the nice property that as $S_t$ approaches zero, so does the change.
This effectively limits $S_t$ to positive values, $S_t \geq 0$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SolvingSDEs/GeomBM.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;10%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The question is as so often with differential equation, whether there exists an analytic solution.
In order to show this analytic solution we will examine the quantity $dS_t/ S_t$ and apply the stochastic version of the log-derivative trick.
The quantity $dS_t/S_t$ has a striking familiarity to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	\frac{\partial \ln S(x)}{\partial x} = \frac{1}{S(x)} \frac{\partial S(x)}{\partial x}
\end{align}&lt;/script&gt;

&lt;p&gt;But since we are working with stochastic processes, we can’t apply regular calculus to derive such a stochastic process but use Ito’s lemma instead:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	d \ln S_t &amp;= \underbrace{\frac{\partial \ln S_t}{\partial t}}_{=0} dt + \frac{\partial \ln S_t}{\partial S_t} dS_t + \frac{1}{2} \frac{\partial^2 \ln S_t}{\partial S_t^2} dS_t^2 \\
	&amp;= \frac{dS_t}{S_t} - \frac{1}{2} \frac{1}{S_t^2} (\mu_t S_t + \sigma_t S_t dW_t)^2 \\
	&amp;= \frac{dS_t}{S_t} - \frac{1}{2} \frac{1}{S_t^2} (\mu_t S_t dt + \sigma_t S_t dW_t)^2 \\
	&amp;= \frac{dS_t}{S_t} - \frac{1}{2} \frac{1}{S_t^2} (\mu_t^2 S_t^2 \underbrace{dt^2}_{\rightarrow 0} + 2 \mu_t S_t \underbrace{dt dW_t}_{\rightarrow 0} + \sigma_t^2 S_t^2 \underbrace{dW_t^2}_{=dt}) \\
	&amp;= \frac{dS_t}{S_t} - \frac{1}{2} \sigma_t^2 dt \\
	\frac{dS_t}{S_t} &amp;= d \ln S_t + \frac{1}{2} \sigma_t^2 dt
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $\ln S_t$ does not have $t$ as an argument, the first term evaluates to zero.
Plugging our alternative definition of &lt;script type=&quot;math/tex&quot;&gt;\frac{dS_t}{S_t}&lt;/script&gt; into the original SDE and integrating it we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	d \ln S_t + \frac{1}{2} \sigma^2 dt &amp;= \mu_t dt + \sigma_t dW_t \\
	\int_0^t d \ln S_s &amp;= \int_0^t \mu_s ds + - \int_0^t \frac{1}{2} \sigma_s^2 ds + \int_0^t \sigma_s dW_s  \\
	\ln S_t - \ln S_0 &amp;= \mu_t t - \frac{1}{2} \sigma_t^2 t + \sigma_t W_t  \\
	\ln \frac{S_t}{S_0} &amp;= \mu_t t - \frac{1}{2} \sigma_t^2 t + \sigma_t W_t  \\
	S_t &amp;= S_0 \ e^{\mu_t t - \frac{1}{2} \sigma_t^2 t + \sigma_t W_t}  \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;ornstein-uhlenbeck-process&quot;&gt;Ornstein-Uhlenbeck Process&lt;/h3&gt;

&lt;p&gt;The Ornstein-Uhlenbeck (OU) process is a SDE that exhibits mean reversion and momentum properties.
Mathematically it is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
dX_t = \theta(\mu - X_t)dt + \sigma dW_t
\end{align}&lt;/script&gt;

&lt;p&gt;where $\theta$ is the momentum parameter that makes the OU process undulate around the mean.
The mean parameter $\mu$ sets the value around which the OU process moves in somewhat smooth arcs.
Visually it looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SolvingSDEs/OU.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;10%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where we can see that even though two of the three sample paths start far away from the mean, they quickly converge to a region around the mean.
Once in the vicinity of the mean they move about it in arcs through the momentum factor.&lt;/p&gt;

&lt;p&gt;The first time I heard of the OU process was in a &lt;a href=&quot;https://arxiv.org/pdf/1706.01905.pdf&quot;&gt;reinforcement learning paper&lt;/a&gt; where it was used to force an agent to repeat the same action a couple of times through the momentum property.&lt;/p&gt;

&lt;p&gt;First, let’s try to clean up the notation to something more succinct and define a new random variable $Y_t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
Y_t = X_t - \mu
\end{align}&lt;/script&gt;

&lt;p&gt;We can easily compute the infinitesimal differential $dY_t$ of $Y_t$ via:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
dY_t &amp;= dX_t \\
&amp;= \theta(\mu - X_t)dt + \sigma dW_t \\
&amp;= - \theta \underbrace{(X_t - \mu)}_{Y_t}dt + \sigma dW_t \\
&amp;= - \theta Y_tdt + \sigma dW_t \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is to recognize that we are equating the derivative of a random variable with itself.
We’ll abuse the mathematical notation for brief period to make the point more clear:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
dY_t &amp; \propto \theta Y_t dt \\
\frac{dY_t}{dt} &amp;\propto \theta Y_t \Leftrightarrow \frac{d e^{ax}}{dx} = ae^x
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The proportional equation above has a striking similarity to the derivative of a scaled exponential $e^{ax}$.
In our case the derivative is not with respect to $x$ but to $t$.
To solidify this intuition let’s define another random variable $Z_t$ as a function of $Y_t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Z_t &amp;= f(t, \theta, Y_t) \\
&amp;= e^{\theta t} Y_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The question naturally arises how $Z_T$ behaves in the infinitesimal differential $dZ_t$.
But since $Z_t$ is a function of a stochastic process we will have to apply Ito’s lemma in order to compute the differential.
Since $Z_t = f(t, \theta, Y_t)$ is linear in $Y_t$, we will deal with a simplified version of Ito’s lemma because the second derivative of a linear function is zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
df(t, \theta, Y_t) &amp;= \partial_t \ f(t, \theta, Y_t) dt + \partial_{Y_t} \ f(t, \theta, Y_t) dY_t + \frac{1}{2} \overbrace{\partial_{Y_t}^2 \ f(t, \theta, Y_t)}^{=0} dY_t^2 \\
&amp;=\partial_t \ f(t, \theta, Y_t) dt + \partial_{Y_t} \ f(t, \theta, Y_t) dY_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Applying the simplified Ito’s lemma to our equation at hand yields:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	dZ_t &amp;= \partial_{t} \left[e^{\theta t}Y_t \right] dt + \partial_{Y_t} \left[e^{\theta t}Y_t \right] dY_t \\
	&amp;= \theta e^{\theta t}Y_t dt + e^{\theta t} dY_t \\
	&amp;= \theta e^{\theta t}Y_t dt + e^{ \theta t}\left(-\theta Y_t dt + \sigma dW_t \right) \\
	&amp;= e^{\theta t} \sigma dW_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This can be easily solved via&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	Z_T &amp;= Z_S + \int_{S}^T dZ_t \\
	&amp;= Z_S + \sigma \int_{S}^T e^{\theta t} dW_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $S$ is the start of the integration through time.
Now that we found a solution to the random variable $Z_t$ it is time to go back through the substitutions to find the solution to $X_t$.
In order to achieve that we first reverse the exponential component in the relationship between $Y_t$ and $Z_t$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	Y_t &amp;= e^{-\theta t}Z_t \\
	Y_T &amp;= e^{-\theta T} Z_T \\
	&amp;= e^{-\theta T}(Z_S + \sigma \int_{S}^T e^{\theta t} dW_t) \\
	&amp;=e^{-\theta T}(e^{kS} Y_S + \sigma \int_{S}^T e^{\theta t} dW_t) \\
	&amp;=e^{-\theta(T-S)} Y_S + \sigma e^{-\theta T} \int_{S}^T e^{\theta t} dW_t \\
	&amp;=e^{-\theta(T-S)} Y_S + \sigma \int_{S}^T e^{\theta(t-T)} dW_t \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finally plugging $Y_t =X_t -\mu$ back in yields:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	Y_T &amp;=e^{-\theta(T-S)} Y_S + \sigma \int_{S}^T e^{\theta(t-T)} dW_t \\
	X_T - \mu &amp;=e^{-\theta(T-S)} (X_S - \mu) + \sigma \int_{S}^T e^{\theta(t-T)} dW_t \\
	X_T &amp;= \mu + e^{-\theta(T-S)} (X_S - \mu) + \sigma \int_{S}^T e^{\theta(t-T)} dW_t \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Starting from $S=0$ we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	X_T = \mu + e^{-\theta T}(X_0 - \mu) + \sigma \int_{S=0}^T e^{-\theta (T-t)} dW_t
\end{align}&lt;/script&gt;

&lt;p&gt;In fact the Ornstein-Uhlenbeck process is one of the few stochastic processes that has a stationary distribution under the assumption of a Normal initial value.
In order to show that we can compute the mean and variance of the process and then evaluate it in infinity with the limit of $\lim t\rightarrow 0$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{E} \left[ X_t \right] &amp;= \mathbb{E} \left[ \mu + e^{-\theta t}(X_0 - \mu) + \sigma \int_{s=0}^t e^{-\theta (t-s)} dW_s\right] \\
	&amp;= \mu + e^{-\theta t}(X_0 - \mu) + \underbrace{\mathbb{E} \left[ \sigma \int_{s=0}^t e^{-\theta (t-s)} dW_s\right]}_{\text{Wiener process} \rightarrow \mathbb{E}[W_t]=0} \\
	&amp;= \mu + e^{-\theta t}(X_0 - \mu)
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{V} \left[ X_t \right] &amp;= \mathbb{E} \left[ \left( X_t - \mathbb{E}[X_t] \right)^2 \right] \\
	&amp;= \mathbb{E} \Bigg[ \Big( \underbrace{\mu + e^{-\theta t}(X_0 - \mu)}_{=\mathbb{E}[X_t]} + \sigma \int_{s=0}^t e^{-\theta (t-s)} dW_s - \mathbb{E}[X_t] \Big)^2 \Bigg] \\
	&amp;= \mathbb{E} \left[ \left(\sigma \int_{s=0}^t e^{-\theta (t-s)} dW_s \right)^2 \right] \quad \leftarrow \text{Ito Isometry} \\
	&amp;= \sigma^2 \int_{s=0}^t e^{-2\theta (t-s)} ds  \\
	&amp;= \sigma^2 \left[ \frac{1}{2\theta} e^{-2\theta (t-s)} \right]_{s=0}^t \\
	&amp;= \frac{\sigma^2}{2\theta} (e^{-2\theta*0} - e^{-2\theta t}) \\
	&amp;= \frac{\sigma^2}{2\theta} (1 - e^{-2\theta t})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Applying the limit $\lim t \rightarrow \infty$ allows us to recover the stationary distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\lim_{t\rightarrow 0} \mathbb{E} \left[X_t\right] &amp;= \lim_{t\rightarrow 0} \mu + \underbrace{e^{-\theta t}}_{=0}(X_0 - \mu) \\
	&amp;= \mu \\
	\lim_{t\rightarrow 0} \mathbb{V}\left[ X_t \right] &amp;= \lim_{t\rightarrow 0} \frac{\sigma^2}{2\theta} (1 - \underbrace{e^{-2\theta t}}_{=0}) \\
	&amp;= \frac{\sigma^2}{2\theta}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While the mean $\mu$ is somewhat expected, the variance can be interpreted intuitively: If the momentum is large, the process is very slow to change and thus the stationary distribution does not move far away from $\mu$.
If the momentum is only small, the Wiener process can exert a stronger influence and the stationary distribution has a wider variance.&lt;/p&gt;

&lt;p&gt;More importantly, since the Wiener process is the only random influence on the process and is Gaussian, the entire process is a Gaussian process.
Thus the stationary distribution is a Gaussian distribution as well.&lt;/p&gt;</content><summary type="html">Geometric Brownian Motion &amp; Ornstein-Uhlenbeck Process</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/SolvingSDEs/GeomBM.png" /></entry><entry><title type="html">Ito’s (Di)Lemma</title><link href="http://localhost:4000/blog/ItosLemma/" rel="alternate" type="text/html" title="Ito's (Di)Lemma" /><published>2020-04-10T00:00:00+02:00</published><updated>2020-04-10T00:00:00+02:00</updated><id>http://localhost:4000/blog/ItosLemma</id><content type="html" xml:base="http://localhost:4000/blog/ItosLemma/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h3 id=&quot;differentiability&quot;&gt;Differentiability&lt;/h3&gt;

&lt;p&gt;Probably one of the most fundamental uses of calculus is the derivation of functions.
A function $f$ is differentiable if the following value $f’(x)$ exists:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'(x) = \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h}&lt;/script&gt;

&lt;p&gt;If we let $h$ go towards zero, a function is called differentiable, if the fraction converges towards some constant value.&lt;/p&gt;

&lt;p&gt;Let’s look at an example of how this might work.
We’ll need an additional mathematical trick called &lt;a href=&quot;https://en.wikipedia.org/wiki/L%27Hôpital%27s_rule&quot;&gt;L’Hopitals rule&lt;/a&gt; which says that for evaluating the limit of a fraction we can simply derive both nominator and denominator with respect to the same variable and still obtain the valid result.
Applying L’Hopitals rule often simplifies the computation of the derivative since we’re always working with the limit of a fraction.&lt;/p&gt;

&lt;p&gt;Let’s try to compute the derivative of the squared function $f(x) = x^2$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  f'(x) &amp;= \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h} \\
  &amp;= \lim_{h\rightarrow 0} \frac{(x+h)^2 - x^2}{h} \\
  &amp;= \lim_{h\rightarrow 0} \frac{x^2 + 2hx + h^2 - x^2}{h} \\
  &amp;= \lim_{h\rightarrow 0} \frac{\frac{\partial }{\partial h} \ 2hx + h^2}{\frac{\partial }{\partial h}h} \quad \quad \quad &amp;&amp;\Leftarrow \text{Applying L'Hopitals rule} \\
  &amp;= \lim_{h\rightarrow 0} \frac{2x + 2h}{1} &amp;&amp; \Leftarrow \text{Evaluating $h$ to zero} \\ 
  &amp;= 2x
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Sure enough it’s the correct result which we anticipated.
In effect, we’re zooming infinitely far into the function and ask ourselves how the function changes in this tiny window $h$.
Visually, this looks something like this for the exponential function $f(x) = e^x$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ItosLemma/Diff01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What this functions tells us is that we can approximate an arbitrarily complex, differentiable function with a linear function for a extremely small window $\lim h \rightarrow 0 $.
But in order for the function to be differentiable, the limit has to actually converge to a linear function as we decrease the window size $h$.&lt;/p&gt;

&lt;p&gt;Unfortunately, for stochastic processes this is not as straight forward and we will require some more elaborate tools to show some notion of differentiability.&lt;/p&gt;

&lt;h3 id=&quot;stochastic-processes&quot;&gt;Stochastic Processes&lt;/h3&gt;

&lt;p&gt;In order to keep things simple in the following steps, we will work with the Brownian Motion. &lt;br /&gt;
Brownian motion $W_t$ is defined through the following properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$W_0 = 0$&lt;/li&gt;
  &lt;li&gt;Independent increments: covariance $\mathbb{C}[W_{t+u} - W_s, W_s] =0$ for $u \geq 0$ and $s \leq t$&lt;/li&gt;
  &lt;li&gt;Gaussian increments: $W_{t+u} - W_t \sim \mathcal{N}(0, u)$&lt;/li&gt;
  &lt;li&gt;Continuous paths in time $t$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now it turns out that there exists a stochastic differential equation which fulfills all of the properties above.
This SDE in question is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dx_t = dW_t = \epsilon \sqrt{dt} \quad \quad \quad ;\epsilon \sim \mathcal{N}(0,1)&lt;/script&gt;

&lt;p&gt;Intuitively, we equate the infinitesimal change in $x_t$ with Brownian Motion which in turn is defined as the standard normally distributed random variable $\epsilon$ scaled by $\sqrt{dt}$.
The problem of classical differentiability of stochastic processes lies precisely in this SDE as we defined the change with respect to $dt$.
By defining the infinitesimal change $dt$ we are acknowledging that we could always use a shorter $dt$ and zoom even further into the time axis.
After all, the infinitesimal change of the Brownian Motion $dW_t$ is defined as a limit in time not unlike the limit we used to show the differentiability of the quadratic function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dW_t = \lim_{\Delta t \rightarrow 0} W_{t + \Delta t} - W_t \quad \sim \mathcal{N}(0, \Delta t)&lt;/script&gt;

&lt;p&gt;While $\Delta t$ goes rapidly towards zero, it will actually never be exactly zero.
Thus, if we were to zoom into the time axis we would realize that the Brownian Motion keeps moving randomly for whatever time resolution we choose.
In turns out that Brownian Motion actually has &lt;a href=&quot;https://en.wikipedia.org/wiki/Fractal&quot;&gt;fractal properties&lt;/a&gt; which are probably the trippiest mathematical animations you can experience without doing actual acid.&lt;/p&gt;

&lt;p&gt;No matter how far we zoom into the Brownian Motion, we will always encounter a Brownian Motion on a finer time scale since $dx_t$ moves randomly on any time scale we choose.
Probably the best animation for that is directly from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wiener_process&quot;&gt;Wikipedia page&lt;/a&gt; of Brownian Motion:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ItosLemma/WienerProcess.gif&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You might wonder: Well, why is that a problem with respect to classical differentiability?
For that we can simply evaluate the differential&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{\Delta t\rightarrow 0} \frac{W_{t+\Delta t} - W_t}{\Delta t}&lt;/script&gt;

&lt;p&gt;but alas, $W_t$ is by definition a Normally distributed random variable.
So let’s have a look at the mean and variance of the differential operator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\lim_{\Delta t\rightarrow 0} \mathbb{E}\left[ \frac{W_{t+\Delta t} - W_t}{\Delta t} \right] 
	&amp;= \lim_{\Delta t} \frac{1}{\Delta t} \mathbb{E} [ \underbrace{W_{t+\Delta t} - W_t}_{\sim \mathcal{N}(0,\Delta t)} ] \\
	&amp;= \lim_{\Delta t} \frac{1}{\Delta t} 0 \\
	&amp;= 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\lim_{\Delta t\rightarrow 0} \mathbb{V} \left[\frac{W_{t+\Delta t} - W_t}{\Delta t} \right]
	&amp;= \lim_{\Delta t\rightarrow 0} \frac{1}{\Delta t^2} \mathbb{V} [ \underbrace{W_{t+\Delta t} - W_t}_{\sim \mathcal{N}(0,\Delta t)} ] \\
	&amp;= \lim_{\Delta t\rightarrow 0} \frac{1}{\Delta t^2} \Delta t \\
	&amp;= \lim_{\Delta t\rightarrow 0} \frac{1}{\Delta t} \\
	&amp;= \infty
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Both the mean and the variance are possibly the worst values you can expect in terms of functional analysis.
The mean is zero, indicating we have no derivative what so ever while the variance goes to infinity which is equally unusable.
Ultimately, we can’t derive a Wiener process in the classical sense.&lt;/p&gt;

&lt;p&gt;It turns out that &lt;a href=&quot;https://en.wikipedia.org/wiki/Kiyosi_Itô&quot;&gt;Kiyosi Ito&lt;/a&gt; had a series of great insights that we can use.
But before we can dive into his ideas, we first have to learn about Taylor expansions …&lt;/p&gt;

&lt;h3 id=&quot;taylor-expansion&quot;&gt;Taylor Expansion&lt;/h3&gt;

&lt;p&gt;The Taylor expansions or Taylor series is one of the most ubiquitous mathematical tools in applied math.
Once at the DeepBayes summer school in Moscow, a fellow attendee and physicist said that if you have no clue what to do next with your equations, do a Taylor expansion and see if it gets you ahead.&lt;/p&gt;

&lt;p&gt;The core idea of a Taylor expansion is to approximate a function locally around a root point with a series of terms which rely on the derivatives of the function.
So for example a function might be a polynomial of order 10 but locally, we only need a quadratic function to approximate it quite well.&lt;/p&gt;

&lt;p&gt;Mathematically, a Taylor expansion of a infinitely differentiable function $f(x)$ around a root point $x_0$ is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)|_{x_0} = \sum_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n&lt;/script&gt;

&lt;p&gt;In order to keep things simple we will only work with a Taylor expansion of the second order, meaning that we will stop the sum after the term with the second order derivative.
Practically, many problems are posed as linear or quadratic problems so the need seldomly arises to compute higher order Taylor expansions (at least in machine learning where computing higher order gradients at scale can be expensive).&lt;/p&gt;

&lt;p&gt;So we’ll be working with the following sum:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  f(x)|_{x_0} &amp;\approx f(x_0) + \frac{f'(x_0)}{1!}(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 \\
  &amp;= f(x_0) + f'(x_0) \underbrace{(x-x_0)}_{\Delta x} + \frac{1}{2!}f''(x_0) \underbrace{(x-x_0)^2}_{\Delta x^2} \\
  &amp;= f(x_0) + \underbrace{f'(x_0) \ \Delta x}_{\text{linear in $\Delta x$}} + \underbrace{\frac{1}{2}f''(x_0) \ \Delta x^2}_{\text{quadratic in $\Delta x$}} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\Delta x = (x - x_0)$ signifies the distance of x to the root point $x_0$.
By using the second order Taylor expansion we approximate the higher order polynomial $f(x)$ with just its first and second order derivative packed into a polynomial in $\Delta x$.
The locality of the Taylor expansion around the $x_0$ is essential for the approximation since we use the first order order derivative $f’(x_0)$ and second order derivative &lt;script type=&quot;math/tex&quot;&gt;f''(x_0)&lt;/script&gt; explicitly evaluated at $x_0$.&lt;/p&gt;

&lt;p&gt;We can visually the individual points of the Taylor expansion around the root point:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ItosLemma/Taylor01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can observe a couple of things in these plots:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The root point $x_0$ stays the same for all plots since this is the point around which we try to locally approximate the function $f(x)$ with a lower order polynomial&lt;/li&gt;
  &lt;li&gt;The first order derivative $f’(x_0) \Delta x$ is a linear function that goes through $x_0$. I omitted the constant term $f(x_0)$ for visual clarity what the individual components contribute to the overall approximation. Strictly speaking it would need to be $f(x_0) + f’(x_0) \Delta x$.&lt;/li&gt;
  &lt;li&gt;The second order derivative &lt;script type=&quot;math/tex&quot;&gt;f''(x_0)\Delta x^2&lt;/script&gt; is a constant value which doesn’t change.
The root $x_0$ lies on a stretch with almost no curvature ergo &lt;script type=&quot;math/tex&quot;&gt;f''(x_0)&lt;/script&gt; is almost constant and doesn’t contribute much to the final approximation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let’s move the root point $x_0$ and plot the different terms again to see the second order derivative &lt;script type=&quot;math/tex&quot;&gt;f''(x_0)&lt;/script&gt; in action:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ItosLemma/Taylor02.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ItosLemma/Taylor03.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With this root point $x_0$ we can see the second order derivatives actually contribute to the final approximation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Now the root point lies in an area in which there is high curvature.&lt;/li&gt;
  &lt;li&gt;The second order term of the Taylor approximation plays a more significant role and we can see that it tries to approximate the function around $x_0$ with a quadratic function.&lt;/li&gt;
  &lt;li&gt;Furthermore the sum of the two terms approximate the original function around the root point more precisely than either could have done on its own.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now the question can be raised on how this could be applied to stochastic processes …&lt;/p&gt;

&lt;h3 id=&quot;itos-lemma&quot;&gt;Ito’s Lemma&lt;/h3&gt;

&lt;p&gt;Let’s assume we a classic SDE with a drift term $\mu(t, X_t)$ and a diffusion term $\sigma(t, X_t)$ which together form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dX_t = \mu(t, X_t) dt + \sigma(t, X_t) dW_t&lt;/script&gt;

&lt;p&gt;and $dW_t$ is the infinitesimal differential of a Wiener process $W_t$.
Such a process is commonly called an Ito drift-diffusion process.&lt;/p&gt;

&lt;p&gt;Now let’s say that we have some function $f(t, X_t)$ that takes whatever value $X_t$ is at the moment $t$ and returns some other value $Y_t$ such that we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_t = f(t, X_t)&lt;/script&gt;

&lt;p&gt;We could use relatively easy functions such as as the exponential function $e^{X_t}$ or the quadratic function $X_t^2$ for starters.
In the financial markets, these functions $f$ quickly get very complex as stock prices are routinely modeled as stochastic differential equations with $f$ capturing complex relationships like a portfolio performance or default probability.&lt;/p&gt;

&lt;p&gt;Since we are working with infinitesimal differentials we would like to know how $Y_t$ changes for very small time differentials $dt$.
So we actually want to be able to define the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dY_t = df(t, X_t)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In order to answer that question, Ito’s lemma applies a Taylor expansion to $f(t, X_t)$ with special numerical conditions for the infinitesimal values.&lt;/strong&gt;
There are a few constraints on $f$, though.
It has to be twice differentiable with respect to $X_t$ and at least once differentiable with respect to $t$.
If these equations are met we can start deriving!&lt;/p&gt;

&lt;p&gt;The first step is to define the Taylor expansion for $f(t, X_t)$ in it’s general form around the root point $(t_0, X_0)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  f(t, X_t) &amp;\approx f(t_0, X_0) + \frac{\partial f(t_0, X_0)}{\partial t} \underbrace{(t - t_0)}_{\Delta t}
  + \frac{\partial f(t_0, X_0)}{\partial X_t}\underbrace{(X_t - X_0)}_{\Delta X_t}
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t}\underbrace{(X_t - X_0)^2}_{\Delta X_t^2} \\
  &amp;= f(t_0, X_0) + \frac{\partial f(t_0, X_0)}{\partial t} \Delta t
  + \frac{\partial f(t_0, X_0)}{\partial X_t}\Delta X_t
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t}\Delta X_t^2 \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is to examine the Taylor expansion in the limit $\lim [t \rightarrow t_0, X_t \rightarrow X_0]$.
This is of interest as we are again interested in the infinitesimal behavior of $f(t, X_t)$, namely $df(t, X_t)$:
By pulling the root evaluation $f(t_0, X_0)$ over to the left side we lay the groundwork for the differential.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\lim_{t\rightarrow t_0, X_t \rightarrow X_0} f(t, X_t) - f(t_0, X_0) 
&amp;= \lim_{t\rightarrow t_0, X_t \rightarrow X_0} \frac{\partial f(t_0, X_0)}{\partial t} \Delta t
  + \frac{\partial f(t_0, X_0)}{\partial X_t}\Delta X_t
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t}\Delta X_t^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In fact the limit above is precisely the differentiability operator from the very beginning.
Remember that we define the differentiability as the difference of two evaluations for an ever more decreasing difference in their arguments.
This is precisely what we are defining in the limit above by moving $t$ ever closer to $t_0$ and simultaneously $X_t$ towards $X_0$.
Furthermore the limit also allows us to rewrite the difference $\Delta t$ and $\Delta X_t$ in their infinitesimal differential form $dt$ and $dX_t$.&lt;/p&gt;

&lt;p&gt;So we obtain the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\lim_{t\rightarrow t_0, X_t \rightarrow X_0} f(t, X_t) - f(t_0, X_0) 
&amp;= \lim_{t\rightarrow t_0, X_t \rightarrow X_0} \frac{\partial f(t_0, X_0)}{\partial t} \Delta t
  + \frac{\partial f(t_0, X_0)}{\partial X_t}\Delta X_t
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t}\Delta X_t^2 \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Downarrow&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  df(t, X_t) &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} dX_t
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} dX_t^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is substituting $dX_t = \mu(t, X_t)dt + \sigma(t, X_t)dW_t$ into the equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  df(t, X_t) &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} dX_t
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} dX_t^2 \\
  &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)dt + \sigma(t, X_t)dW_t)
  + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)dt + \sigma(t, X_t)dW_t)^2 \\
  &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)dt + \sigma(t, X_t)dW_t) \\
  &amp; \quad + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)^2 dt^2 + 2 \mu(t, X_t) \sigma(t, X_t)^2 dt dW_t + \sigma(t, X_t)^2 dW_t^2) \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now comes a pivotal part in the derivation in which examine how $dt$ and $dW_t$ behave when multiplied or squared.
The differential Wiener process can be rewritten as $dW_t = \epsilon \sqrt{dt}$.
Thus we have the following time-dependent terms appearing in the equation above: $dt^2$, $dt dW_t = \epsilon dt^{1.5}$ and $dW_t^2 = \epsilon^2  dt = dt$ under the mean-square interpretation which states $\mathbb{E}[\epsilon^2] = \mathbb{V}[\epsilon] = 1$ for $\epsilon \sim \mathcal{N}(0,1)$.&lt;/p&gt;

&lt;p&gt;The important aspect of simplifying Ito’s lemma is to think about how $dt^2$, $dt^{1.5}$ and $dt$ behave for infinitesimal changes.
Any $dt^k$ with $k&amp;gt;1$ and $dt &amp;lt; 1$ will decrease by an order of magnitude faster to zero than $dt$ itself for the infinitely small values that we’re dealing with.
So if we evaluate for a infinitesimal small $dt$, the terms $dt^2$ and $dt^{1.5}$ will be smaller by larger order of magnitudes.
This allows us to simply drop them from our equation.&lt;/p&gt;

&lt;p&gt;So we now have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  df(t, X_t)
  &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)dt + \sigma(t, X_t)dW_t) \\
  &amp; \quad + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)^2 \overbrace{dt^2}^{\rightarrow 0} + 2 \mu(t, X_t) \sigma(t, X_t)^2 \overbrace{dt dW_t}^{\rightarrow 0} + \sigma(t, X_t)^2 \overbrace{dW_t^2}^{=dt}) \\
  &amp;= \frac{\partial f(t_0, X_0)}{\partial t} dt
  + \frac{\partial f(t_0, X_0)}{\partial X_t} (\mu(t, X_t)dt + \sigma(t, X_t)dW_t) + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} \sigma(t, X_t)^2 dt \\
  &amp;= \underbrace{\left(\frac{\partial f(t_0, X_0)}{\partial t} + \frac{\partial f(t_0, X_0)}{\partial X_t} \mu(t, X_t) + \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} \sigma(t, X_t)^2 \right)dt}_{\text{deterministic}} + \underbrace{\frac{\partial f(t_0, X_0)}{\partial X_t} \sigma(t, X_t)dW_t}_{\text{stochastic}} \\
  &amp;= \mu_f\left(t, \mu(t, X_t), \frac{\partial f(t_0, X_0)}{\partial t}, \frac{\partial f(t_0, X_0)}{\partial X_t}, \frac{1}{2} \frac{\partial^2 f(t_0, X_0)}{\partial X_t} \sigma(t, X_t)^2 \right)dt + \sigma_f\left(t, \sigma(t, X_t), \frac{\partial f(t_0, X_0)}{\partial X_t} \right) dW_t
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;So ultimately, it turns out that the derivative of the function $f(t, X)$ with an Ito drift-diffusion process as input is an Ito drift-diffusion process itself.
Albeit with a few first and second order derivatives sprinkled in between.&lt;/p&gt;

&lt;p&gt;Thus we can model the function $f(t, X_t)$ just like any other drift-diffusion process and can evaluate the distribution of such a process at a later point in time.&lt;/p&gt;</content><summary type="html">Or how to differentiate a function of a stochastic process.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/ItosLemma/WienerProcess.gif" /></entry><entry><title type="html">Stochastic Differential Equations</title><link href="http://localhost:4000/blog/SDE/" rel="alternate" type="text/html" title="Stochastic Differential Equations" /><published>2020-03-30T00:00:00+02:00</published><updated>2020-03-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/SDE</id><content type="html" xml:base="http://localhost:4000/blog/SDE/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h4 id=&quot;non-differential-equations&quot;&gt;Non-Differential Equations&lt;/h4&gt;

&lt;p&gt;Most of us are quite familiar with linear and non-linear equations from our 101 math classes and lectures.
These equations define an equality between the two terms left and right of the equal sign:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(x)&lt;/script&gt;

&lt;p&gt;These functions assert an equality between $y$ and $x$ through the function $f(\cdot)$ and describe a “static” relationship between a value $x$ and its corresponding value $y$.
Examples of these functions are numerous and we can list a couple of them here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear equations: $y = A x + b$&lt;/li&gt;
  &lt;li&gt;Exponential equations: $y = e^x$&lt;/li&gt;
  &lt;li&gt;Polynomials: $y = \sum_{k=0}^n a_k x^k$&lt;/li&gt;
  &lt;li&gt;Trigonometric equations: $y = \sin(x)$&lt;/li&gt;
  &lt;li&gt;and the list goes on and on …&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the equations above share the characteristic that they equate two separate values $y$ and $f(x)$.&lt;/p&gt;
&lt;h4 id=&quot;differential-equations&quot;&gt;Differential Equations&lt;/h4&gt;

&lt;p&gt;As you can guess from the title there is another important class of equations: differential equations.
These equations relate one or more functions to their derivatives.
Mathematically this looks like the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{\frac{d y}{dx}}_{\text{derivative}} = f(x)&lt;/script&gt;

&lt;p&gt;As we can see from above the one thing that changed to our earlier, non-differential equation is the derivative.
Instead of telling us what the value $y$ is given the function $f(x)$ as in the case of non-differential equations, the differential equation above tells us the change of $y$ with respect to $x$.
In plain English, it tells us how much $y$ changes if we change $x$ by simply evaluating the function $f(x)$.&lt;/p&gt;

&lt;p&gt;Naturally, the question arises where we ask ourselves what the heck do these equations tell us.
In non-differential equations, the relationship between input to a function and output is quite straight forward.&lt;/p&gt;

&lt;p&gt;I struggled for quite some time to arrive at an intuitive interpretation of what differential equations actually represent.
Fortunately, one field where differential equations pop up en masse is physics (which apart from quantum physics tends to be quite intuitive for humans).
So we’ll make a detour through physics to keep the intuition alive while diving into differential equations.&lt;/p&gt;

&lt;p&gt;Differential equations are often employed in physics when a physical system is &lt;strong&gt;most accurately described through its instantaneous change in time&lt;/strong&gt;.
It should be noted that the differential could be defined with respect to any argument of the function $f(\cdot)$, but in physics the time differential $d / dt$ is often the differential of interest as we want to predict things into the future.
In the simplest case, a physical object $x(t)$ moves through time and space according to some function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{\frac{d}{dt} \ x(t)}_{\text{change over time}}  = \underbrace{f(t, x(t))}_{\text{value of change}} \quad \cong \quad f(x(t))&lt;/script&gt;

&lt;p&gt;The equation above simply states that the change over time, $d  x(t) / dt$ is equivalent to the function $f(t, x(t))$.
Mathematically, we require the time $t$ to appear in the function $f(t, x(t))$ since otherwise the time derivative wouldn’t exist.
For a more intuitive notation we can drop it and equate the change $d/dt x(t)$ with the function $f( \cdot )$ with the ‘essentially the same’ symbol $\cong$.&lt;/p&gt;

&lt;p&gt;We can write the differential equation in a shorter way by using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_(infinitesimal)&quot;&gt;infinitesimal differential&lt;/a&gt; by pulling $dt$ over to the other side:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dx(t) = f(t, x(t)) dt&lt;/script&gt;

&lt;p&gt;which simply states that a “super small” change $dx(t)$ in $x(t)$ corresponds to function $f(t, x(t))$ “scaled” by the “super small” time difference $dt$.&lt;/p&gt;

&lt;p&gt;Below is an image juxtaposing what we refer to as non-differential equations and a differential equations with respect to time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SDE/EqVsDiffEq.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead of working with a “absolute” equation as shown on the left side, the differential equation on the right gives us the change $dx(t)$ for any point $x(t)$ at any point in time $t$ (which is mathematically a vector field).
&lt;strong&gt;Each arrow in the right plot is an evaluation of the differential equation $dx(t)$ at a specific point $x(t)$ at a specific point in time $t$.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A more intuitive example of the right hand plot above is the temperature of a hot coffee mug.
The hotter the coffee mug, the larger the temperature gradient between coffee mug and the surrounding.
So the larger the gradient the more temperature (thermal energy) is passed off into the environment of the hot coffee mug, ergo the temperature decrease is faster for coffee mugs with high temperatures.
(To be frank, this is not the most physically correct way of how energy behaves, but this is just for an intuitive visualization.)&lt;/p&gt;

&lt;p&gt;The grey lines in in the right plot model the changing temperatures over time of three coffee mugs with different temperatures.
We model the thermal energy dissipation through a (ordinary) differential equation and would like to know what the temperature of the three coffee mugs will be at a later point in time.
Computing the later temperature amounts to “little more” than following the arrows.
These arrows are computed through the differential equation and tell us what the temperature change $dx(t)$ is for a mug with a specific temperature $x(t)$ at time $t$.&lt;/p&gt;

&lt;p&gt;On a side note: Notice how the arrows don’t change in their direction and magnitude for a specific value $x(t)$ while we progress in time. This signals that $dx(t)$ doesn’t actually use $t$ to compute the change in temperature.&lt;/p&gt;

&lt;p&gt;The way we solve differential equations is to start at some initial point $x(0)$ and add up all the temperature changes $dx(t)$ that the hot coffee mug is exposed to over time.
Mathematically, this amounts to little more than:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(T) = x(0) + \underbrace{\int_{t=0}^T dx(t)}_{\text{sum up all the changes}}&lt;/script&gt;

&lt;p&gt;the solution of which is shown as the grey line in the right plot.&lt;/p&gt;

&lt;p&gt;Another analogy would be kicking a soccer ball over a soccer field.
The ball starts somewhere $x(0)$ and you kick it repeatedly in some direction (adding $dx(t)$ repeatedly).
Each kick changes the location of the soccer ball and results in the ball lying in a new position $x(t)$.
After we kicked the soccer ball about the soccer field enough, we’ll finally leave it at $x(T)$.&lt;/p&gt;

&lt;p&gt;Unfortunately, computers can’t really work with infinitesimal small number like $dx(t)$ or $dt$ since numbers in computers are stored with a finite amount of bits.
As so often, the (approximate) solution is to discretize the changes to very small, yet still representable values of $\Delta x(t)$ and $\Delta t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
x(T) &amp;= x(0) + \underbrace{\int_{t=0}^T dx(t)}_{\text{sum up all the changes}} \\
&amp; \underbrace{\approx}_{\text{discretize}} x(0) + \sum_{t=0}^T \Delta x(t)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $t$ is some finite partition of time into discrete values.&lt;/p&gt;

&lt;p&gt;It turns out that the integral above (and its respective discrete approximation) is all we need to solve (ordinary) differential equations.
More importantly it’s all we need to get a basic understanding of stochastic differential equations.
But before we can proceed to stochastic differential equations, we have to talk above stochasticity over time.&lt;/p&gt;

&lt;p&gt;Enter Wiener processes …&lt;/p&gt;

&lt;h4 id=&quot;wiener-process&quot;&gt;Wiener Process&lt;/h4&gt;

&lt;p&gt;In order to understand Wiener processes we need to think about the position of a particle in an Euclidean space that moves purely randomly.
The question is how we could model such a particle.&lt;/p&gt;

&lt;p&gt;The first idea would be to determine that at any point in time the particle has the tendency to move randomly in space.
Therefore it does not jiggle and bounce at discrete time steps but will always move an infinitesimally small distance $dx(t)$ in a random direction $\epsilon$ for any infinitesimally short period of time $dt$.
&lt;!-- We can also conclude, that the longer the particle moves, the farther the particle can actually move from its starting point. --&gt;
&lt;!-- This introduces a relationship between how long and how far the little jiggly particle can move. --&gt;&lt;/p&gt;

&lt;p&gt;Visually we want the random moving particle looking something like this in two dimensions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SDE/BrownianMotion.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can thus proclaim the following, somewhat un-mathematical property of this rambunctious little particle:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{dx_t}_{\text{change in space}} \equiv \overbrace{\epsilon}^{\text{random move}} \underbrace{&quot;dt&quot;}_{\text{some change in time}}&lt;/script&gt;

&lt;p&gt;There are a couple of things to observe here:&lt;/p&gt;

&lt;!-- * First is the modified notation of $dx_t$ which is simply a more compact notation for $dx(t)$ and is completely equivalent. --&gt;
&lt;ul&gt;
  &lt;li&gt;First we introduced a random variable $\epsilon$ which follows some probability distribution.&lt;/li&gt;
  &lt;li&gt;Secondly, through the infinitesimal differentials on both sides we equated the random move in space with the duration of the movement just like in a differential equation.&lt;/li&gt;
  &lt;li&gt;Thirdly, the infinitesimal movements of $x_t$ through time are completely independent since $\epsilon$ is sampled uncorrelated through time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It turns out that if we choose the random movements $\epsilon$ and the change in time $”dt”$ smartly, we can derive convenient theoretical properties about the movement of the little random particle $x_t$.&lt;/p&gt;

&lt;p&gt;Since $\epsilon$ is a random variable at any point in time, the position of $x_t$ will never be predictable with absolute certainty.
Instead we have to treat the position of the particle $x_t$ itself as a random variable, the behavior of which is governed by the differential equation above.&lt;/p&gt;

&lt;p&gt;First up is the choice of $\epsilon$.
The usage of the Normal distribution $\mathcal{N}(\mu, \sigma)$ is prevalent in a lot of modelling approaches due to the convergence of sequences of random variables and it furthermore has nice theoretical properties.
For that reason we will model the probability of the random movement $\epsilon$ with a standard normal distribution, namely $\epsilon \sim \mathcal{N}(0,1)$.&lt;/p&gt;

&lt;p&gt;Secondly we will chose the “amount of time $dt$” to actually be $\sqrt{dt}$, the reason of which will be clear in an instant.&lt;/p&gt;

&lt;p&gt;Thirdly, we want to particle to start at zero, so $x_0 = 0$.&lt;/p&gt;

&lt;p&gt;Given these modelling assumptions, we are interested where the particle could turn up at a later point in time, so we want to know what $x_T$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  x_T &amp;= x_0 + \int_{t=0}^T dx_t \\
  &amp;= \underbrace{x_0}_{\text{$=0$}} + \int_{t=0}^T \epsilon \sqrt{dt} \\
  &amp;= \int_{t=0}^T \epsilon \sqrt{dt}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;But since $\epsilon$ is a random variable we actually have to treat the position of the particle at $x_T$ as a random variable.
The most that we can do is thus to treat $x_T$ as a probability distribution for which we can compute the first two moments, the mean and the variance:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathbb{E}\left[ x_T \right] &amp;= \mathbb{E}\left[\int_{t=0}^T dx_t \right] \\
  &amp;= \int_{t=0}^T \underbrace{\mathbb{E}\left[ \epsilon \right]}_{\mathcal{N}(0,1)} \sqrt{dt} \\
  &amp;= 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathbb{V}\left[ x_T \right] &amp;= \mathbb{V}\left[\int_{t=0}^T dx_t \right] \\
  &amp;= \int_{t=0}^T \underbrace{\mathbb{V}\left[ \epsilon \sqrt{dt} \right]}_{\mathbb{V}[a X] = a^2 \mathbb{V}[X]}  \\
  &amp;= \int_{t=0}^T dt \underbrace{\mathbb{V}\left[ \epsilon \right]}_{\epsilon \sim \mathcal{N}(0,1)}  \\
  &amp;= \int_{t=0}^T dt  \\
  &amp;= T
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can validate the properties of the Wiener process experimentally through what a good friend of mine calls “computational evidence”:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SDE/BrownianMotionExperiment.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;with the following code&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;drift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;diffusion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# start at zero&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# simulate Brownian Motion in parallel&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diffusion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot all the trajectories&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# compute the analytical means and variances of the Brownian Motion&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analytic_var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# compute the empirical mean and variance from the sampled Brownian Motion&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;empiric_var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'-.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'-.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([],[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_yticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([],[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can observe both the sampled mean and the variance (in red) of our 100 Wiener processes match the expected mean and variance (in gray) up to the noise that we introduce through sampling.
Basically all the paths stay around zero where they start and they spread out according to our analytical computed variance of $\mathbb{V}[x_t] = t$ over time.&lt;/p&gt;

&lt;p&gt;Since we chose $\epsilon$ and $”dt”$ smartly, we arrive at quite succinct definitions for the mean and variance of this random variable $x_T$.
In fact, this specific kind of stochastic process has a specific name.
By choosing the random movement $\epsilon \sim \mathcal{N}(0,1)$, the starting value $x_0=0$ and the time differential $\sqrt{dt}$ we have defined our little, rambunctious particle to follow a &lt;strong&gt;Wiener process&lt;/strong&gt; which is a specific kind of stochastic process.
The defining properties of a Wiener process $W_t$ ( $W_t$ being the common notation of a Wiener process) that describes the infinitesimal movement of a particle through $dx_t = \epsilon \sqrt{dt}$ are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$W_0 = 0$ &lt;br /&gt;
This means that the Wiener process always starts at zero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Independent increments: $\mathbb{C}[W_{t+u} - W_s, W_s] =0$ for $u \geq 0$ and $s \leq t$ &lt;br /&gt;
Increments (the movement of the Wiener process) are independent from the past movements. $\mathbb{C}[\cdot , \cdot ]$ is the covariance between two random variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gaussian increments: $W_{t+u} - W_t \sim \mathcal{N}(0, u)$ &lt;br /&gt;
The difference between any two realizations is Gaussian distributed accordingly to the time difference between these two realizations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continuous paths in time $t$.&lt;br /&gt;
We can basically zoom infinitely far into the movements on the time axis and we will never find a discontinuous jump. Yet, due to it being a stochastic process it turns out that the Wiener process is not differentiable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to be all set up for the final chapter of this post, we will define an infinitesimal version of the Gaussian increment property of the Wiener process:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_{t+u} - W_t \sim \mathcal{N}(0, u) \quad \Leftrightarrow \quad dW_t \sim \mathcal{N}(0,dt)&lt;/script&gt;

&lt;h4 id=&quot;stochastic-differential-equations--differential-equations--wiener-processes&quot;&gt;Stochastic Differential Equations (= Differential Equations + Wiener Processes)&lt;/h4&gt;

&lt;p&gt;Once we understood differential equations and Wiener processes, we’ll realize that (basic) stochastic differential equations are just the combination of the two.
We can thus define a stochastic differential equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{dX_t}_{\text{total change}} = \underbrace{\mu_t dt}_{\text{deterministic}} + \underbrace{\sigma_t dW_t}_{\text{stochastic}}&lt;/script&gt;

&lt;p&gt;which defines the infinitesimal change in the random variable $X_t$ at time $t$ as the combination of a deterministic change $\mu_t dt$ and a scaled Wiener process $\sigma_t dW_t \sim \mathcal{N}(0,\sigma_t^2 dt)$.&lt;/p&gt;

&lt;p&gt;With a constant drift, this looks something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SDE/SDEDriftDiff.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;through&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;drift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;diffusion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# start at zero&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# simulate Brownian Motion in parallel&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diffusion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot all the trajectories&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# compute the analytical means and variances of the Brownian Motion&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analytic_var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# compute the empirical mean and variance from the sampled Brownian Motion&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;empiric_var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empiric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'-.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analytic_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'-.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([],[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_yticklabels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([],[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For such drift-diffusion processes, or more specifically Ito drift-diffusion processes, we can compute the analytical mean and variance of how $X_t$ will be distributed in the future.
To keep things simple, we will work with a constant mean $\mu = \mu_t$ and diffusion $\sigma = \sigma_t$.
Solving the SDE amounts to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  X_T &amp;= \int_{t=0}^T dX_t \\ 
  &amp;= \int_{t=0}^T \mu dt + \sigma dW_t \\
  &amp;= \mu \int_{t=0}^T dt + \sigma \int_{t=0}^T dW_t \\
  &amp;= \mu T + \sigma \int_{t=0}^T dW_t \\
  &amp;= \mu T + \sigma W_T \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly to earlier, the Brownian motion $W_T \sim \mathcal{N}(0,T)$ is a random variable, which entices us to compute the mean and variance of the term above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathbb{E}[X_T] &amp;= \mathbb{E}[\mu T + \sigma W_T] \\
  &amp;= \mu T + \sigma \mathbb{E}[W_T] \\
  &amp;= \mu T
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathbb{V}[X_T] &amp;= \mathbb{V}[\mu T + \sigma W_T] \\
  &amp;= \underbrace{\mathbb{V}[\mu T]}_{=0} + \mathbb{V}[\sigma W_T] \\
  &amp;= \sigma^2 \mathbb{V}[ W_T] \\
  &amp;= \sigma^2 T \\
  \mathbb{Std}[X_T] &amp;= \sigma \sqrt{T}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which we were able to validate with our “computational evidence” in the plot above.
The mean increases constantly as time progresses and the standard deviation above and below the mean increases asymptotically due to the $\sqrt{T}$.&lt;/p&gt;

&lt;p&gt;This is a fairly simple SDE since we assume that $\mu_t$ and $\sigma_t$ are constant in time and do not depend on the value of $X_t$.
Things get significantly more interesting when both $\mu_t$ and $\sigma_t$ change over time depending on the value of $X_t$ such that we are working with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dX_t = \mu(t, X_t) dt + \sigma(t, X_t) dW_t&lt;/script&gt;

&lt;p&gt;The drift $\mu(t, X_t)$ and $\sigma(t, X_t)$ can now be potentially highly non-linear and complex functions which could even take in other stochastic processes as additional input.
But Ito’s lemma, Ornstein-Uhlenbeck processes and Geometric Brownian Motion are topics for another time …&lt;/p&gt;</content><summary type="html">A Wiener twist to differential equations</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/SDE/EqVsDiffEq.png" /></entry><entry><title type="html">Intro to Bayesian Inference</title><link href="http://localhost:4000/blog/BayesianInference/" rel="alternate" type="text/html" title="Intro to Bayesian Inference" /><published>2020-02-13T00:00:00+01:00</published><updated>2020-02-13T00:00:00+01:00</updated><id>http://localhost:4000/blog/BayesianInference</id><content type="html" xml:base="http://localhost:4000/blog/BayesianInference/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;This is a talk I was invited to give at Europe’s largest robo advisor &lt;a href=&quot;https://uk.scalable.capital&quot;&gt;Scalable Capital&lt;/a&gt; in Munich .
It serves as an introduction to Bayesian Inference covering the two main approaches of Markov &lt;a href=&quot;https://ludwigwinkler.github.io/blog/HMC/&quot;&gt;Chain Monte Carlo sampling&lt;/a&gt; and Variational Inference.
It is fairly high-level since it was given to a large audience with diverse professional backgrounds.&lt;/p&gt;

&lt;p&gt;The slides can be downloaded &lt;a href=&quot;BayesML.pdf&quot;&gt;here&lt;/a&gt; with corrected typos.&lt;/p&gt;</content><summary type="html">Do you have a minute to talk about our lord and saviour, Thomas Bayes ... ?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/BayesianInference/BayesML.png" /></entry><entry><title type="html">(Basic) Inducing Points in Gaussian Processes</title><link href="http://localhost:4000/blog/InducingPoints/" rel="alternate" type="text/html" title="(Basic) Inducing Points in Gaussian Processes" /><published>2019-09-30T00:00:00+02:00</published><updated>2019-09-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/InducingPoints</id><content type="html" xml:base="http://localhost:4000/blog/InducingPoints/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;Gaussian processes (GP) are extremely flexible probabilistic models with a sound theoretical footing.
In essence, you treat your available data as a giant Normal distribution and infer the covariances between the data points via a kernel.
Now every data point becomes a dimension of the Normal distribution.
This is in contrast to how we normally think about Normal distributions and data where each feature has its own dimension in a Normal distribution.
So for example, a data set with 200 data points $x_n$, five features per data point and a scalar target value would create a Normal distribution of dimensionality 200.&lt;/p&gt;

&lt;p&gt;Usually the squared exponential kernel is used to compute the covariance between two data points $x_i$ and $x_j$ via&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  K_{ij} = k(x_i, x_j ; l) =  exp \left[ \frac{(x_i - x_j)^2}{2l^2} \right]
\end{align*}&lt;/script&gt;

&lt;p&gt;Once new data points $X_*$ are obtained, we compare it via the kernel to our existing data set $X$, compute a couple of linear operations with the resulting kernel matrices and the target information in your data set and voila, you arrive at your prediction:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  \mu(x_*) = K_{XX_*} (K_{XX} + \sigma^2 I)^{-1} y \\
  \Sigma(x_*) = K_{X_*X_*} - (K_{XX} + \sigma^2 I)^{-1} K_{XX_*}
\end{align*}&lt;/script&gt;

&lt;p&gt;The training of GP’s consists of finding the right parameters, namely the length scale $l$ in the kernel and the variance in the data $\sigma^2$.
These two terms can be found via the non-linear optimization problem which minimizes the negative log-likelihood of the available training data in the GP defined by the length scale and kernel parameter:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
     \min_{\theta}  -\log{p(\mathcal{D};\theta)}
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |k(XX;\theta) + \sigma^2 I|\right] + \frac{1}{2} y^T  (K_{XX} + \sigma^2 I)^{-1} y
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;While the necessary computations for training the GP and predicting new data points are only linear, they have quadratic memory and cubic computational cost due to square kernel matrix and the required inversions of the kernel matrix.
The memory and computational cost arises mainly from the fact that kernel methods like SVM’s or GP’s require the training data set at hand to compute new predictions.
Neural networks in comparison store the “learned information” in their weights, whereas GP’s and SVM’s always need the full training data set to accomplish anything.
A lot of work has therefore gone into making GP’s more scalable and finding ways of reducing their memory and computational cost.&lt;/p&gt;

&lt;p&gt;A majority of the efforts focus on the reduction of the training data set kernel matrix $K_{XX}$ while keeping as much information of the full kernel matrix as possible.
One idea in this line of research has been the introduction of inducing points.
A number of inducing points are selected which are meant to represent the full training data set.
One can think of this along the line of k-means clustering of the training data set.&lt;/p&gt;

&lt;p&gt;Let’s first set up the environment and import all the necessary libraries:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch.distributions
from torch.distributions import Normal, MultivariateNormal
from torch.utils.data import DataLoader, TensorDataset

import sklearn
from sklearn.datasets import make_moons
from sklearn.preprocessing import scale

import numpy as np
import matplotlib.pyplot as plt
import time
import copy
import sys, os, argparse, datetime, time

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

if torch.cuda.is_available():
	FloatTensor = torch.cuda.FloatTensor
elif not torch.cuda.is_available():
	FloatTensor = torch.FloatTensor

import torch.nn.functional as F

params = argparse.ArgumentParser()
params.add_argument('-logging',                   type=int,           default=0)

params.add_argument('-num_samples',               type=int,           default=200)
params.add_argument('-num_inducing_points',       type=int,           default=6)
params.add_argument('-x_noise_std',               type=float,         default=0.01)
params.add_argument('-y_noise_std',               type=float,         default=0.1)
params.add_argument('-zoom',                      type=int,           default=10)

params.add_argument('-lr_kernel',                 type=float,         default=0.01)
params.add_argument('-lr_ip',                     type=float,         default=0.1)

params.add_argument('-num_epochs',                type=int,           default=300)

params = params.parse_args()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can do some plotting:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def generate_weightuncertainty_data():

	x = np.linspace(-0.35, 0.55, params.num_samples)
	x_noise = np.random.normal(0., params.x_noise_std, size=x.shape)
	y_noise = np.random.normal(0., params.y_noise_std, size=x.shape)

	y = x + 0.3 * np.sin(2 * np.pi * (x + x_noise)) + 0.3 * np.sin(4 * np.pi * (x + x_noise)) + y_noise

	x, y = x.reshape(-1, 1), y.reshape(-1, 1)

	mu = np.array([[-0.3,-0.3],[-0.18, -0.8],[0,0], [0.15, 0.8], [0.35,0.3], [0.55, 0.6]])

	if True:
		plt.scatter(x, y, alpha=0.5)
		plt.scatter(mu[:,0], mu[:,1], color='red', marker='+', s=100)
		plt.show()

	return x, y

generate_weightuncertainty_data()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/InducingPoints/Data.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Judging from the regression data, we can see that the function which is represented by the noisy data points can be approximated quite reasonably with 6 data points.
And this is what inducing points are all about: finding a set of representative points $\{ \widetilde{X}, \widetilde{y} \} $ which capture the original data structure $\{ X, y \}$ sufficiently well while reducing the memory and computational cost of the GP.
Remember that we went from 200 data points to 6 data points, ergo a memory cost of $\mathcal{O}(200^2) = \mathcal{O}(40000)$ to just $\mathcal{O}(6^2) = \mathcal{O}(36)$.&lt;/p&gt;

&lt;p&gt;The training objective consists now of maximizing the probability of the training data under the distribution of the GP with the inducing points $\widetilde{\mathcal{D}} = \{ \widetilde{X}, \widetilde{y} \}$.
Thus we have the following objective function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 \min_{\theta, \widetilde{\mathcal{D}}}  -\log p(\mathcal{D};\theta, \widetilde{\mathcal{D}})
&amp;= \min_{\theta, \widetilde{\mathcal{D}}}  -\log p(y| X; \theta, \widetilde{\mathcal{D}}) \\
&amp;= \min_{\theta, \widetilde{\mathcal{D}}} -\log \mathcal{N} \Big(
	\overbrace{ K_{X \widetilde{X}} ( K_{\widetilde{X}, \widetilde{X}} + \sigma^2 I)^{-1} y}^{\mu(X)},
	\overbrace{ K_{XX} - K_{X \widetilde{X}} (K_{\widetilde{X} \widetilde{X}} + \sigma^2 I)^{-1} K_{\widetilde{X} X} + \sigma^2 I }^{\Sigma(X)}
	\Big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So let’s code that down in PyTorch!&lt;/p&gt;

&lt;p&gt;So we already have the data.
Next up is the base class for the GP.
The most straight-forward way of using inducing points is to simply declare them as parameters which have gradients.
Remember that both the objective function via the logarithm of the Normal distribution as well as the predictions consist of linear terms, so we can easily backpropagate through these operations to obtain the gradients for the kernel parameters and inducing points from the log probability of the true data under the Normal distribution of the GP.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class GP_InducingPoints(torch.nn.Module):

	def __init__(self, _x=None, _y=None, _num_inducing_points = params.num_inducing_points, _dim=1):

		super().__init__()

		assert type(_x) != type(None) # some sanity checking
		assert type(_y) != type(None) # some sanity checking for the correct input

		self.x = _x # save data set for convenience sake, not recommended for large data sets
		self.y = _y

		self.num_inducing_points = _num_inducing_points

		inducing_x = torch.linspace(_x.min().item(), _x.max().item(), self.num_inducing_points).reshape(-1,1) 	# distribute the data points as a linspace between x.min() and x.max() to get a good initializaiton of the inducing points
		self.inducing_x_mu = torch.nn.Parameter(inducing_x + torch.randn_like(inducing_x).clamp(-0.1,0.1)) 			# add some noise to the x values of the inducing points
		self.inducing_y_mu = torch.nn.Parameter(FloatTensor(_num_inducing_points, _dim).uniform_(-0.5,0.5)) 		# since we normalized the data to N(0,1) we initialize the y values in the middle of N(0,1)

		self.length_scale = torch.nn.Parameter(torch.scalar_tensor(0.1)) 	# the kernel hyperparameter to be optimized alongside inducing points
		self.noise = torch.nn.Parameter(torch.scalar_tensor(0.5)) 				# the noise hyperparameter to model the inherent variance in the data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need the kernel method to compute the kernel/covariance matrix between arbitrary points:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def compute_kernel_matrix(self, x1, x2):

			assert x1.shape[1] == x2.shape[1] # check dimension
			assert x1.numel() &amp;gt;= 0 # sanity check
			assert x2.numel() &amp;gt;= 0 # sanity check

			pdist = ( x1 - x2.T)**2 # outer difference
			kernel_matrix = torch.exp(-0.5*1/(self.length_scale+0.001)*pdist)

			return kernel_matrix
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The third class method of the GP class to implement is the forward method of the GP such that we can take the gradients through PyTorch AutoDiff library:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def forward(self, _X):

			# compute all the kernel matrices
			self.K_XsX = self.compute_kernel_matrix(_X, self.inducing_x_mu)
			self.K_XX = self.compute_kernel_matrix(self.inducing_x_mu, self.inducing_x_mu)
			self.K_XsXs = self.compute_kernel_matrix(_X, _X)

			# invert K_XX and regularizing it for numerical stability
			self.K_XX_inv = torch.inverse(self.K_XX + 1e-10*torch.eye(self.K_XX.shape[0]))

			#compute mean and covariance for forward prediction
			mu = self.K_XsX @ self.K_XX_inv @ self.inducing_y_mu
			sigma = self.K_XsXs - self.K_XsX @ self.K_XX_inv @ self.K_XsX.T + self.noise*torch.eye(self.K_XsXs.shape[0])

			# for each point in _X output MAP estimate and variance of prediction ( that's the torch.diag (...) )
			return mu, torch.diag(sigma)[:, None]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Up next is the loss function as described above:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def NMLL(self, _X, _y):

			# set reasonable constraints on the optimizable parameters
			self.length_scale.data   = self.length_scale.data.clamp( 0.00001, 3.0)
			self.noise.data          = self.noise.data.clamp(0.000001,3)

			# compute all the kernel matrices again ... now you see why we want to use inducing points
			K_XsXs = self.compute_kernel_matrix(_X, _X)
			K_XsX = self.compute_kernel_matrix(_X, self.inducing_x_mu)
			K_XX = self.compute_kernel_matrix(self.inducing_x_mu, self.inducing_x_mu)
			K_XX_inv = torch.inverse(K_XX + 1e-10*torch.eye(K_XX.shape[0]))

			Q_XX = K_XsXs - K_XsX @ K_XX_inv @ K_XsX.T

			# compute mean and covariance and GP distribution itself
			mu = K_XsX @ K_XX_inv @ self.inducing_y_mu
			Sigma = Q_XX + self.noise**2*torch.eye(Q_XX.shape[0]) # noise regularized covariance

			p_y = MultivariateNormal(mu.squeeze(), covariance_matrix=Sigma)
			mll = p_y.log_prob(_y.squeeze()) # evaluate the probability of the target values in the training data set under the distribution of the GP

			mll -= 1/( 2 * self.noise**2) * torch.trace(Q_XX) # add a regularization term to regularize variance

			return -mll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally a nice plotting function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	def plot(self, _title=&quot;&quot;):

			x = torch.linspace(self.x.min()*1.5, self.x.max()*1.5, 200).reshape(-1,1)

			with torch.no_grad():
				mu, sigma = self.forward(x)

			x = x.numpy().squeeze()
			mu = mu.numpy().squeeze()
			sigma = sigma.numpy().squeeze()

			plt.title(_title)
			plt.scatter(self.inducing_x_mu.detach().numpy(), self.inducing_y_mu.detach().numpy())
			plt.scatter(self.x.detach().numpy(), self.y.detach().numpy(), alpha=0.1, c='r')
			plt.fill_between(x, mu-3*sigma, mu+3*sigma, alpha = 0.1, color='blue')
			plt.plot(x, mu)
			plt.xlim(self.x.min()*1.5, self.x.max()*1.5)
			plt.ylim(-3,3)
			plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can let the whole thing train via the following script:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# generating data and normalizing it
X, y = generate_weightuncertainty_data()
X = FloatTensor(scale(X))
y = FloatTensor(scale(y))

# initialize the GP and plot initial prediction
gp = GP_InducingPoints(_x=X, _y=y)
gp.plot(_title=&quot;Init&quot;)

# use two different learning rates since inducing points need to potentially cover a far larger distance than kernel parameters
optim = torch.optim.Adam([{&quot;params&quot;: [gp.length_scale, gp.noise], &quot;lr&quot;: params.lr_kernel},
                          {&quot;params&quot;: [gp.inducing_x_mu, gp.inducing_y_mu,], &quot;lr&quot;: params.lr_ip}])

# put it all in a data loader ...
train_loader = DataLoader(TensorDataset(FloatTensor(X), FloatTensor(y)),
                        batch_size=params.num_samples,
                        shuffle=True,
                        num_workers=1)

# ... and let it train
for epoch in range(params.num_epochs):
	for i, (data, label) in enumerate(train_loader):
		optim.zero_grad()

		mll = gp.NMLL(data, label)

		mll.backward()
		optim.step()

		if epoch%(params.num_epochs//10)==0:
			print(f'Epoch: {epoch} \t NMLL:{mll:.2f} \t LS {gp.length_scale:.2f} \t Noise: {gp.noise:.2f}')
			gp.plot(_title=f&quot;Training Epoch {epoch:.0f}&quot;)

gp.plot(_title=&quot;Post Training&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I combined the entire training loop into a nice little gif which shows how the inducing points and the kernel parameters are adjusted to the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/InducingPoints/IP_GP.gif&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One can see nicely how the inducing points are moved to precisely the points in space which we predicted earlier in the image at the top.
This training routine would even be amenable to mini batch training, we would introduce more variance though.&lt;/p&gt;</content><summary type="html">Tackling the computational cost of GP's</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/InducingPoints/Cover.png" /></entry><entry><title type="html">Sinkhorn Iterations</title><link href="http://localhost:4000/blog/Sinkhorn/" rel="alternate" type="text/html" title="Sinkhorn Iterations" /><published>2019-07-15T00:00:00+02:00</published><updated>2019-07-15T00:00:00+02:00</updated><id>http://localhost:4000/blog/Sinkhorn</id><content type="html" xml:base="http://localhost:4000/blog/Sinkhorn/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;!-- ## Non-Negative Matrix Factorization --&gt;

&lt;p&gt;Computing the difference between two distributions is a problem commonly encountered in machine learning.
Monte Carlo approximations of the KL divergence commonly suffer from the problem of sampling from the proposal distribution and computing relevant statistics with respect to the target distribution.
In this case a problem can arise when the support of the proposal distribution from which the samples are drawn does not match the support of the target distribution.
Intuitively one can image two Normal distributions with vastly different means and similar covariance matrices.
It will be very hard to draw a sample from the source distribution which also lies in the support of the target distribution.
The KL divergence is not symmetric in that respect since swapping the two distributions will give different loss values.
The non-symmetry is the reason the KL divergence is not considered a distance, but only a divergence.&lt;/p&gt;

&lt;p&gt;Ultimately, we are dependent on the proposal distribution in the KL divergence to cover the support of the target distribution with sufficient probability mass such that we can sample from the relevant portions of the target distribution.
For the more interested reader, I would recommend reading up on Forward and Backward KL divergences and the behaviour of mode-seeking and mean-seeking.&lt;/p&gt;

&lt;p&gt;The Wasserstein distance provides a remedy for this by posing a different distance measure: the earth mover distance.
Contrary to the KL divergence, the Wasserstein distance asks how much probability mass we need to move around the source target distribution such that it matches the target distribution.
This notion is independent of the support of the distribution and one of the reasons why it has become popular in GAN’s.&lt;/p&gt;

&lt;p&gt;The only extra information we need is a cost matrix which determines how expensive it is to move probability mass from one point in the source distribution to another point in the source distribution.
In the analogy of moving mass around, one can imagine that it is more work to move some mass for five ‘steps’ than it is to just move the same mass one step away.
While the earth mover distance can be derived for continuous distributions, it’s easiest to visualize with discrete distributions and moving probability mass between the ‘buckets’ of the discrete distributions.&lt;/p&gt;

&lt;p&gt;Let’s define two probability vectors $p$ and $q$ which both define a categorical distribution in $\mathbb{R}_+^{d}$.
Both these probability vectors form a simplex through two constraints which apply to them (analogously for $q$):
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p^T \mathbb{1} &amp;= 1 \\
	p_i &amp; \in \mathbb{R}_+
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The second thing we need is a cost matrix $C$ for the objective function of the Wasserstein distance which characterizes how expensive it is to move probability mass from one category of a categorical distribution to another.
We can simply use different Euclidean distance between two vectors $x$ and $y$ for that:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	c_{ij} = |(x_i-y_j)| \quad \text{or} \quad c_{ij} = ||(x_i-y_j)|| _2^2 \ \forall i, j \in \{1, \ldots, d\}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The cost matrix $T$ for categorical distributions should be a symmetric matrix which increases its values as it moves away from the diagonal.
But of course if you are dealing with a different transport problem, the cost matrix could be different and non-symmetric depending on your transport manifold.&lt;/p&gt;

&lt;p&gt;For a categorical distribution with three possible events (intuitively bins) ${ 0, 1, 2}$ it should be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	C = \begin{bmatrix}
		0 &amp; 1 &amp; 2 \\
		1&amp; 0 &amp; 1\\
		2 &amp; 1 &amp; 0
\end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;All this cost matrix says is that we have to ‘hop’ twice to move probability mass from bin 0 to bin 2.&lt;/p&gt;

&lt;p&gt;The final component of the Wasserstein distance is the coupling matrix $T$ which is defined through a polyhedral set
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	U(p, q) = \{ T \in \mathbb{R}_+^{d \times d} \quad | \quad T \mathbb{1} = p, T^T \mathbb{1} = q \}
\end{align}&lt;/script&gt;
A polyhedral set refers to a set of solutions which are constrained by a finite number of half-spaces.
Half-spaces can be created through inequality constraints which form a convex set in our case.&lt;/p&gt;

&lt;p&gt;Remember that $p$ and $q$ are both distribution vectors.
The purpose of the coupling matrix $T$ is to quantify a way to move probability mass about the source distribution such that it becomes equal to the target distribution.
The next logical step should be to guarantee that a valid coupling matrix $T$ moves the exact amount of probability mass such that we obtain both the distribution vectors $p$ and $q$ if we sum up the rows and columns of the matrix.&lt;/p&gt;

&lt;p&gt;For a small example let $p = [ 1, 0, 0]^T$ and $q = [ 0, 0, 1]^T$.
The only valid coupling matrix $T$ would be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	T &amp;= p q^T\\
	&amp;= 	\begin{bmatrix}
			0 &amp; 0 &amp; 1 \\
			0 &amp; 0 &amp; 0 \\
			0 &amp; 0 &amp; 0
		\end{bmatrix}
		\Rightarrow
		\begin{bmatrix}
			1 \\
			0 \\
			0 \\
		\end{bmatrix} = T \mathbb{1} = p \\
		&amp; \qquad \quad \Downarrow \\
		&amp; \quad \ \begin{bmatrix}
		0 &amp; 0 &amp; 1
		\end{bmatrix} \\
		&amp; \quad = T^T \mathbb{1} = q
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A closer look at the coupling matrix shows the expected behaviour that we move all the probability mass from bin 1 in $p$ to bin 3 in $q$.&lt;/p&gt;

&lt;p&gt;The Wasserstein distance between two distributions is defined as the minimum of the inner Frobenius product $\langle \cdot, \cdot \rangle_F$ of the coupling matrix $T$ and the cost matrix $C$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\mathcal{W}(p,q) := \min_{T\in U(p,q)} \left\langle T; C \right\rangle_F.
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In the equation above, the membership of $T \in U(r,c)$ contains the constraints that $T$ has to be a valid coupling matrix.&lt;/p&gt;

&lt;p&gt;The hard part is now to find the coupling matrix $T$ which accurately represents the probability mass that we have to move around in $p$ to arrive at the distribution vector $q$.
There is a very large number of possible coupling matrices $T$ which all result in a valid transport of the probability mass, yet we are interested in the coupling matrix $T$ which transports the least amount of probability mass!&lt;/p&gt;

&lt;p&gt;Since we are computing the Frobenius norm with respect to the cost matrix $C$ we want the Wasserstein distance to be as small as possible, meaning that we want to move as little probability mass as possible on the off-diagonal entries in $T$.
We don’t want an artificially high cost because of valid, though unreasonable, transport plans if two distributions are the same except for a tiny fraction of probability mass.&lt;/p&gt;

&lt;p&gt;We could solve the problem above with linear programming since it’s simply finding the minimum of a linear program given a series of inequality constrains on the coupling matrix.
Linear programming becomes infeasible and difficult for large solution spaces and furthermore is not differentiable which makes this solution unattractive for the use in gradient-based optimization schemes.&lt;/p&gt;

&lt;p&gt;The solution of our original linear program would result in a sparse solution on one of the vertices given by the constraints.
One can imagine this as searching for the absolute perfect solution in the remotest corner of the simplex.
But we could ask ourselves in the age of approximate function fitting and empirical risk minimization, whether we really require such a perfect solution.
Could we arrive at an easier to optimize objective function if we were to accept a slightly less accurate solution?&lt;/p&gt;

&lt;p&gt;Enter Sinkhorn distances as optimal transport with entropic constraints!&lt;/p&gt;

&lt;p&gt;The answer is yes with the help of entropic regularization.
We therefore augment the original convex set:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	U_\lambda (p,q) &amp;= \{ T \in U(p,q) \quad | \quad \text{KL}[T || pq^T] \leq \lambda \} \\
	&amp;= \{ T \in U(p,q) \quad | \quad H[T] \geq H[p] + H[q] - \mu\} \subset U(p,q)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;From information theory we know that the mutual information of two random variables $X$ and $Y$, should they follow a joint distribution, is defined as
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	I[ p(x), p(y)] &amp;= \text{KL}[p(x,y)||p(x), p(y)] \\
	&amp;= \int \int p(x,y) \log \frac{p(x,y)}{p(x)p(y)} dx dy \\
	&amp;= \int p(x,y) \log p(x,y) dx dy - \int p(x,y) \log p(x) dx dy - \int p(x,y) \log p(y) dx dy \\
	&amp;= -H[p(x,y)] - \int p(x) \log p(x) dx - \int p(y) \log p(y) dy \\
	&amp;= -H[p(x,y)] + H[p(x)] + H[p(y)]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Given the fact that we construct the coupling matrix $T$ from both $p$ and $q$, and it therefore represents a joint probability distribution between $p$ and $q$, we can derive the additional constraints in the augmented convex set.&lt;/p&gt;

&lt;p&gt;Analogously, we can define the same equations in our specific case for our joint probability $T$ and marginal probability vectors $p$ and $q$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\text{KL}[T||pq^T] &amp;\leq \lambda \\
	-H[T] + H[p] + H[q] &amp;\leq \lambda \\
	H[T] &amp;\geq H[p] + H[q] - \lambda
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is similar to how trust regions in optimization and reinforcement learning work.
From an information theoretic point of view, the coupling matrix $T$, which we are optimizing, should not move too far from the joint probability distribution $pq^T$.
We thus construct a trust region in terms of the KL divergence and parameter $\lambda$ in which we can freely optimize but not move too far away from our initial value.
The aim of the original optimization is to find a transport scheme which minimizes the entropy of the coupling matrix $T$ due to the sparse solution on the vertices of the probability simplex.&lt;/p&gt;

&lt;p&gt;The additional constraint forces the entropy of the coupling matrix $T$ not to fall below a certain threshold.
The values of $H[p]$, $H[q]$ and $\lambda$ are all fixed and thus $H[T]$ should not fall below the combinations of these values.
Therefore the optimization is discouraged from finding sparse solutions, as for example with the L1-norm, and encouraged to find a smooth transport plan $T$.&lt;/p&gt;

&lt;p&gt;If $\lambda \rightarrow \infty$, the constraint disappears and the entropy of the coupling matrix will result in the same solution as the linear program since $H[T] \in \mathbb{R}_+$.&lt;/p&gt;

&lt;p&gt;An alternative objective function would therefore be the following:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	T^\lambda = \text{argmin}_{T \in U(p,q)} \left\langle T , C \right\rangle - \lambda H[T] \quad \text{with} \quad \lambda \geq 0
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Adding the original marginal constraint $T \mathbb{1} = p$ and $T^T \mathbb{1} = q$ and forming the Lagrangian gives us a convenient objective function:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathcal{L}(T, \alpha, \beta) &amp;= \left\langle T, C \right\rangle_F + \lambda H[T] + \alpha^T (T^T \mathbb{1} - q) + \beta^T (T \mathbb{1} - p) \\
	&amp;= \sum_{ij} t_{ij} c_{ij} + \lambda \sum_{ij} t_{ij} \log t_{ij} + \sum_i \alpha_i ( \sum_j t_{ij} - p_j) + \sum_i \beta_i ( \sum_j t_{ij} - q_i)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The derivative $\partial \mathcal{L} / \partial t_{ij}$ yields
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\frac{\partial \mathcal{L}}{\partial t_{ij}} &amp;= c_{ij} + \lambda (\log t_{ij} + 1) + \alpha_i + \beta_i \stackrel{!}{=} 0 \\
	\log t_{ij} &amp;= - \frac{1}{\lambda} c_{ij} - 1 - \frac{1}{\lambda} \alpha_i - \frac{1}{\lambda} \beta_i \\
	t_{ij} &amp;= \exp \left[ - \frac{1}{\lambda} \alpha_i \right] \exp \left[-\frac{1}{\lambda} c_{ij}-1 \right] \exp \left[ - \frac{1}{\lambda} \beta_i \right]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Now Sinkhorn’s theorem enters the plot.
Sinkhorn’s theorem states that if a matrix $A$ has strictly positive elements, then there exist two diagonal matrices $D_1$ and $D_2$ with strictly positive diagonal elements such that $B = D_1 A D_2$ is doubly stochastic.
Applying this to our problem, we have $T$ which by definition only has positive elements, and the diagonal matrices $\exp[ \text{diag}[ - 1/\lambda \cdot \alpha]]$ and $\exp [ \text{diag}[ - 1/\lambda \cdot \beta]]$.
The diagonal matrices arise from the constraint on the rows and columns of the coupling matrix $T$.
All of these matrices are the element-wise negative exponential which makes their entries by construction strictly positive.
For the product of these three matrices, the result will be a double stochastic matrix $B$, which is defined by the properties $B\mathbb{1} = \mathbb{1}$ and $B^T\mathbb{1} = \mathbb{1}$.
&lt;!-- % So to sum up:
% \begin{align}
% 	B = D_1 A D_2 \ \Leftrightarrow \ T = \exp[ \text{diag}[ - 1/\lambda \cdot \alpha]] \exp[-\frac{1}{\lambda} C -1] \exp [ \text{diag}[ - 1/\lambda \cdot \beta]]
% \end{align} --&gt;&lt;/p&gt;

&lt;p&gt;We can rewrite the objective function in such a way that it reflects the Sinkhorn’s theorem:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	T = \text{diag}(u) K \text{diag}(v)
\end{align}&lt;/script&gt;
with
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	K &amp;= \exp[- \frac{1}{\lambda}C -\mathbb{1}\mathbb{1}^T ] \\
	\text{diag}(u) &amp;= D_1 = \text{diag}(\exp[ - \frac{1}{\lambda} \alpha]) \\
	\text{diag}(v) &amp;= D_2 = \text{diag}(\exp[ - \frac{1}{\lambda} \beta]).
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;By plugging $T = \text{diag}(u) K \text{diag}(v)$ into our constraints $p = T \mathbb{1}$ and $ q = T^T \mathbb{1}$, we obtain the following element-wise updates:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p &amp;= \text{diag}(u) K \text{diag}(v) \mathbb{1} \\
	q &amp;= \text{diag}(u) K^T \text{diag}(v) \mathbb{1} \\
	p_i &amp;= u_i (Kv)_i \\
	q_j &amp;= v_j (K^Tu)_j
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;By realigning the equations to $u$ and $v$, we obtain the alternating update rules:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	v_j &amp; \leftarrow p_j / (K^T u)_j \\
	u_j &amp; \leftarrow q_j / (K v)_j
\end{align} %]]&gt;&lt;/script&gt;
which can be rewritten into the element-wise operations
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	v &amp; \leftarrow p \oslash (K^T u) \\
	u &amp; \leftarrow q \oslash (K v)
\end{align} %]]&gt;&lt;/script&gt;
where $\oslash$ is the element-wise division of two vectors.&lt;/p&gt;

&lt;p&gt;Unfortunately, it turns out that these iterations are numerically not especially stable.
The repeated divisions and multiplications can lead to numerical under- or overflows.
Just imagine multiplying $0.1^{100}$ and you’ll quickly see that numerical issues can arise due to a limited bit number.&lt;/p&gt;

&lt;p&gt;In order to stabilize the computations, the logarithm is used.
For numerical computations, the logarithm has the nice practical property that it turns multiplications and divisions into additions and subtractions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\log v &amp; \leftarrow \log p - \log[K^T u] \\
	\log u &amp; \leftarrow \log q - \log[K v]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The logarithm of the vector matrix multiplication $\log[K^Tu]$ and $\log[Kv]$ need some extra handling.
For simplicities sake, we will continue the derivation for $\log[Kv]$ but it holds for $\log[K^Tv]$ as well:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\log[Kv]_i &amp;= \log\left[ \sum_j K_{ij} v_j \right] \\
	&amp;= \log\left[ \sum_j \exp\left[-\frac{1}{\lambda}C_{ij} - 1 \right] v_j \right] \\
	&amp;= \log\left[ \sum_j \exp\left[-\frac{1}{\lambda}C_{ij} - 1 + \log v_j \right] \right].
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The computation log-sum-exp is implemented in many numerical software packages and utilizes an additional trick to stabilize the computation.
When we exponentiate very large or very small numbers (e.g. $\pm 10^10$) it can quickly happen that we encounter under- or overflows since such numbers go to either zero or infinity and we encounter the problem of represents very small or very large numbers with limited number of bits.
Yet, this was precisely what we wanted to circumvent by applying the log to our update rules.
Fortunately, we can simply subtract the maximum value inside the log-sum-exp operation and add it back on the outside.
A small algebraic proof follows here which $x_{max} = \max { x_0, x_1, x_2, \ldots x_N}$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	&amp; \log \left[ \sum_i \exp[x_i] \right] \underbrace{+ x_{max} - x_{max}}_{=0} \\
	&amp; \log \left[ \sum_i \exp[x_i] \right] + x_{max} + \log \exp[ -x_{max}] \\
	= &amp; \log \left[ \sum_i \exp[x_i] \exp[ -x_{max}] \right] + x_{max} \\
	= &amp; \log \left[ \sum_i \exp[x_i -x_{max}] \right] + x_{max}.
\end{align} %]]&gt;&lt;/script&gt;
Intuitively, the log-sum-exp trick rescales the terms which are exponentiated into the well-behaved area around zero.
This rescaling is then undone by simply adding the maximum value back.
This updated computations of the log-sum-exp is done in most linear algebra packages under the hood.&lt;/p&gt;

&lt;p&gt;Let’s get back to our original problem of computing the log of the matrix-vector product.
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\log[Kv]_i = \log\left[ \sum_j \exp\left[-\frac{1}{\lambda}C_{ij} -1 + \log v_j \right] \right].
\end{align}&lt;/script&gt;
We can compute the logarithmized matrix vector product by simply adding $\log v$ to each row of the rescaled cost matrix $C$ and the log-sum-exp trick is applied automatically.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\log v &amp; \leftarrow \log p - \log \left[ \sum_{rows} \left[ - \frac{1}{\lambda}C + \log u \right] \right] \\
	\log u &amp; \leftarrow \log q - \log \left[ \sum_{rows} \left[ - \frac{1}{\lambda}C + \log v \right] \right]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The Sinkhorn iterations give us a way to compute the optimal transport plan.
But we still have to compute the gradients for these iterations if we want to use them in gradient based optimization schemes.
Remember that we used the Sinkhorn iterations in the first place to find a transport plan for which the gradient of the cost function with respect to the transport plane is zero, i.e. $\smash{\partial \mathcal{L}/ \partial t_{ij} \stackrel{!}{=}0}$.
The vectors $u$ and $v$ were used as short-hand replacements for the exponentiated $\alpha$ and $\beta$ dual variables.
The entire point of using duality in optimization is that we can solve an alternative optimization problem, the dual with the dual variables (in our case $\alpha$ and $\beta$), and the solution will be equivalent to our original optimization problem, the primal (given certain properties and constraints, into the detail of which I won’t go here).
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial \mathcal{L}/ \partial t_{ij} &amp; \stackrel{!}{=}0 \\
	\Downarrow \\
	u = \exp[-\frac{1}{\lambda}\alpha] \ &amp; , \ v = \exp[-\frac{1}{\lambda}\beta] \\
	&amp; \Downarrow \\
	\alpha = -\lambda \log u \  &amp; , \ \beta = -\lambda \log v
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Once we found $\alpha$ and $\beta$ we can plug them back into the original objective function $\mathcal{L}$ and we can compute the gradients with respect to the distributions at hand:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\nabla_q\mathcal{L} &amp;= \nabla_q \left[ \left\langle T, C \right\rangle_F + \lambda H[T] + \alpha^T(T^T\mathbb{1} - q) + \beta^T (T\mathbb{1} - p) \right] \\
	&amp;= \alpha \\
	&amp;= - \lambda \log u
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Now that we have the gradient $\nabla_q \mathcal{L}$ for each ‘bin’ in the proposal probability vector $q$ we can update this vector with stochastic gradient descent and fit it to the probability vector $p$.&lt;/p&gt;

&lt;p&gt;How does all of this look in code?
Well … here you go&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch
import ot
import math
import matplotlib.pyplot as plt

Tensor = torch.FloatTensor
p = Tensor([0.1, 0.2, 0.3, 0.4]) # the target distribution we want to approximate

# Bunch of different initial distributions used for testing
# q = torch.nn.Parameter(Tensor([0.1, 0.25, 0.25, 0.4]))
q = torch.nn.Parameter(Tensor([0.4 , 0.2, 0.3, 0.1]))
# q = torch.nn.Parameter(Tensor([0.7, 0.1, 0.1, 0.1]))
# q = torch.nn.Parameter(Tensor([0.7, 0.2, 0.05, 0.05]))
# q = torch.nn.Parameter(Tensor([0.1, 0.2, 0.3, 0.4

lambda_reg = 10 # larger values reduce entropy term -&amp;gt; 1/lambda_reg

# Use python optimal transport library to check whether cost functions is computed correctly as reference
print('Python Optimal Transport')
dist = np.arange(q.shape[0])
C = np.abs(np.expand_dims(dist, axis=1) - np.expand_dims(dist, 0))
P = ot.sinkhorn(p.data.numpy(), q.data.numpy(), C, reg=1./lambda_reg, method='sinkhorn')
print('dist:', np.sum(P * C), np.sum(P))
print()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we’ve set up the the two distributions and are ready to optimize.&lt;/p&gt;

&lt;p&gt;The Sinkhorn iteration function takes the two distributions performs the Sinkhorn iterations and returns the gradient for the proposal distribution which is computed from alpha:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sinkhorn_iteration(p, q, lambda_reg, log=True): # the main sinkhorn iteration function
	'''

	:param a: target / parameterized distribution
	:param b: source distribution
	:return: Wasserstein distance between source / parameterized distribution and target distribution

	Solves the transport problem
	argmin &amp;lt;P, C&amp;gt; + 1/lambda_reg H[P}
	'''

	assert p.shape == torch.Size([4])
	assert q.shape == torch.Size([4])

	dist = torch.arange(p.shape[0]).float()
	C = torch.abs(dist.unsqueeze(1) - dist.unsqueeze(0))
	K = torch.exp(-lambda_reg * C-1)

	alpha = torch.ones_like(p)/4
	beta = torch.ones_like(q)/4

	log_alpha = torch.ones_like(p)*(-math.log(4))
	log_beta = torch.ones_like(p)*(-math.log(4))

	if not log:
		for iter in range(200):

			last_u = alpha
			alpha = p / (K @ beta)
			beta = q / (K @ alpha)

			if torch.mean(torch.abs(alpha - last_u)) &amp;lt; 0.05:
				break

	elif log:
		for iter in range(200):
			last_log_alpha = log_alpha

			log_alpha     = torch.log(p) - torch.logsumexp(-lambda_reg*C -1 + log_beta,     dim=1)
			log_beta      = torch.log(q) - torch.logsumexp(-lambda_reg*C -1 + log_alpha,    dim=1)

			if torch.mean(torch.abs(log_alpha - last_log_alpha)) &amp;lt; 0.005:
				break
		alpha = log_alpha.exp()
		beta = log_beta.exp()

	P = torch.diag(alpha) @ K @ torch.diag(beta)

	d = torch.sum(P * C)
	grad = torch.log(alpha)/lambda_reg #- torch.mean(torch.log(alpha))/lambda_reg

	return d, P, grad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally we’ll use the optimizer provided by PyTorch to do a couple of hundred steps to let the proposal distribution converge to the true distribution.
While we’re at it, we’ll also plot a couple of sanity checks such as the distributions still summing up to one&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;optim = torch.optim.SGD([q], lr=0.001)

epochs = 2000
for epoch in range(epochs):

	optim.zero_grad()

	dist, P, grad = sinkhorn_iteration(p, q, lambda_reg)

	q.grad = - grad

	optim.step()

	if epoch % (epochs//10) == 0: print('Epoch: ', epoch, dist.detach(), torch.sum(P).detach(), q.detach())
	if epoch &amp;lt;10: print('Epoch: ', epoch, dist.detach(), torch.sum(P).detach(), q.detach())

print(q.detach(), 'sum(source_dist)=', torch.sum(q).detach().item())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly, I was only able to make it work with the plain SGD optimizer.
ADAM for some reason went NaN on my after a couple of iterations.
I suspect it has to do something with the second-order estimations and maybe the gradient I provide somewhere let’s these second-order moments explode.&lt;/p&gt;

&lt;p&gt;Up to this point we had a look at discrete distributions and how to compute an optimal transport plan.
It turns out, that the optimal transport plan for continuous distributions (at least for the Normal distribution) is quite compact and doesn’t require an auxiliary optimization problem via the Sinkhorn iterations.&lt;/p&gt;</content><summary type="html">Computing Wasserstein Distances</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/Sinkhorn/Sinkhorn.png" /></entry><entry><title type="html">GPyTorch</title><link href="http://localhost:4000/blog/GPyTorch/" rel="alternate" type="text/html" title="GPyTorch" /><published>2019-05-15T00:00:00+02:00</published><updated>2019-05-15T00:00:00+02:00</updated><id>http://localhost:4000/blog/GPyTorch</id><content type="html" xml:base="http://localhost:4000/blog/GPyTorch/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;!-- ## Non-Negative Matrix Factorization --&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Seminar hosted by Ben in the offices of &lt;a href=&quot;https://lateral.io&quot;&gt;Lateral&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gaussian Processes are a class of popular and powerful probabilistic models which can be derived straight from a Normal distribution.
While vanilla GP’s are very flexible due to their nonparametric formulation, inference and training both rely on the evaluation of the entire data set through several kernel matrices, their products, log determinants and inverses.&lt;/p&gt;

&lt;p&gt;These kernel matrices over the entire training data set and their numerous use in matrix matrix pdocuts, log determinants and inverses result in poor scalability to large data sets.
The advent of GPU’s in machine learning has offered the possibility of computing matrix vector and matrix matrix products efficiently and in parallel.&lt;/p&gt;

&lt;p&gt;GPyTorch introduces several randomized and parallelized algorithms as replacements for exact computations which reduce the complexity of inference and training in GP’s.
These include stochastic approximations of the trace, parallel conjugate gradient descent and several efficient applications of eigendecompositions.&lt;/p&gt;

&lt;p&gt;More importantly, all of these adaptations allow full utilization of parallel hardware during inference and training.
Considerable focus will be on conjugated gradient descent which allows the efficient optimization of quadratic optimization problems through the use of line search and conjugate search directions.&lt;/p&gt;

&lt;p&gt;The core idea of the original paper is to exploit a parallel version of conjugate gradient descent to the fullest such that all relevant terms for the cost function and the gradients of the kernels can be obtained from a single modified batch conjugate descent run.&lt;/p&gt;

&lt;p&gt;The talk can be downloaded &lt;a href=&quot;/blog/GPyTorch/GPytorch.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><summary type="html">GPU Acceleration for Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/GPyTorch/GPyTorch_Cover.png" /></entry><entry><title type="html">Hamiltonian Mechanics in Monte Carlo Samplers</title><link href="http://localhost:4000/blog/HMC/" rel="alternate" type="text/html" title="Hamiltonian Mechanics in Monte Carlo Samplers" /><published>2019-03-14T00:00:00+01:00</published><updated>2019-03-14T00:00:00+01:00</updated><id>http://localhost:4000/blog/HMC</id><content type="html" xml:base="http://localhost:4000/blog/HMC/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;Here’s a common problem: You want to estimate some function $f$.
The function $f$ takes some input $x$ and outputs some value $y$ thus giving you $y=f(x)$.
The important part is that you don’t have access to the global shape of $f$.&lt;/p&gt;

&lt;p&gt;This could be due to $f(x)$ being an absolute black-box model such that you can only interact with it by handing it some value $x$ and receiving some value $y=f(x)$.
Something that is practically very similar is that the analytical form of $f(x)$ is very, very complicated but can be evaluated in a pointwise manner.
This means that we can’t evaluate the global shape of $f(x)$ but only the single points for some specific value of $x$.&lt;/p&gt;

&lt;p&gt;The lack of knowledge of the global shape can be encountered in different scenarios which all boil down to computing a complicated integral.
In order to offer some intuition let’s have a look at the univariate normal distribution $p(x|\mu, \sigma)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	p(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\end{align*}&lt;/script&gt;

&lt;p&gt;The univariate normal distribution is a true and tested distribution in machine learning with many convenient mathematical properties.
One property is the normalization of the exponential term $\smash{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}$ with the scaling term $1/\sqrt{2\pi\sigma^2}$ such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	\int_{- \infty }^{\infty} p(x | \mu, \sigma) dx = 1.
\end{align*}&lt;/script&gt;

&lt;p&gt;Conversely we know that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\int_{-\infty}^{\infty} p(x|\mu, \sigma) dx
	&amp;= \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \\
	&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \underbrace{\int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx}_{=\sqrt{2\pi\sigma^2}} \\
	&amp;= 1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Obviously we know the exact form of the scaling parameter through the standard deviation $\sigma$ … but what if we didn’t?&lt;/p&gt;

&lt;p&gt;In that case we would encounter the distribution as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	p(x|\mu, \sigma) &amp;= \frac{1}{Z} f(x|\mu, \sigma)\\
	&amp;= \frac{1}{Z} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
	\text{with} \ Z  = \int_{-\infty}^{\infty} f(x| \mu, \sigma) dx &amp;= \int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx = \sqrt{2\pi\sigma^2}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the case above we could evaluate the distribution $p(x | \mu, \sigma)$ correctly up to the scaling parameter $1/Z$ by evaluating $f(x | \mu, \sigma)$.
The value of $Z$ is given by the shape of the distribution $p(x | \mu, \sigma)$ which we can’t determine globally but only point-wise for some value $x$.&lt;/p&gt;

&lt;p&gt;The example above is supposed to illustrate the more general problem of estimating probability distributions with intractable partition functions $Z$ while still being intuitive by working with a well known distribution such as the univariate normal distribution.&lt;/p&gt;

&lt;p&gt;The most naive idea for computing $Z$ is to evaluate random values of $x$ and compute an approximate integral.
More precisely we would sample $N$ different $x$ from a uniform distribution $U(x_{min}, x_{max})$ with equal probability.
This set $\{ x_i \}_{i=0}^N$ serves as an approximation of the true function which becomes better and better, the more samples $x_i$ we obtain.
If we were to choose $N=\infty$ then the sample set would replicate the true shape perfectly.
This is practically impossible due to computational and time constraints.&lt;/p&gt;

&lt;p&gt;Once we have a set which we think contains enough samples, we could approximate the integral by simply summing over our set&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	Z = \int_{-\infty}^{\infty} f(x|\mu, \sigma) dx
	=\int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \approx \sum_{i=0}^N \frac{1}{\Delta x} e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
\end{align*}&lt;/script&gt;

&lt;p&gt;The $\Delta x$ is required since are we approximating the function $f( x | \mu, \sigma)$ with little columns where the exponential term tells us the height and $\Delta x$ the width of the column (look at the animations at Wikipedias entry for Riemann integral).&lt;/p&gt;

&lt;p&gt;This would get the job done as our approximation would asymptotically approach the true value of $Z$ for large enough sample sizes.
Yet, upon closer examination we can see that the univariate normal distribution has a specific shape which concentrates a lot of information (or more precisely probability) in the area around $\mu$.&lt;/p&gt;

&lt;p&gt;Remember that a the bulk of the integral $\smash{\int_{-\infty}^{\infty} f(x|\mu, \sigma) dx}$ is evaluated around $\mu$.
If we chose $x_{min}$ and $x_{max}$ at the very extreme ends of the number line, our $N$ samples would be very dispersed and our approximation of $Z$ would be very poor.&lt;/p&gt;

&lt;p&gt;Let’s examine a concrete problem.
We choose a standard normal distribution $\mathcal{N}(0,1)$ and sample $200$ values of $x$ from the uniform distribution $\mathcal{U}(-10,10)$.
Furthermore we consider all probabilities of $\mathcal{N}(0,1)$ interesting which have a probability above $p(x|\mu, \sigma) \geq 0.01$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print()
print('###########################################')
print('Uniform Sampler')
print('###########################################')

import numpy
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import torch
import math

FloatTensor = torch.FloatTensor

class Gaussian:

	def __init__(self, _mu, _sigma):
		self.mu = _mu
		self.std = _sigma
		self.dist = torch.distributions.Normal(self.mu, self.std)

	def eval(self, _x):
		return self.dist.log_prob(_x).exp()

	def unnorm_eval(self, _x):
		return torch.exp(-1/(2*self.std**2)*(_x - self.mu)**2)
std = 1
print('True Z:', np.sqrt(2*np.pi*std**2))
P = Gaussian(FloatTensor([0]), FloatTensor([std]))
U = torch.distributions.Uniform(0,1)
num_samples=1000

res = 100
x_min=-10
x_max=10
points = torch.from_numpy(np.linspace(x_min,x_max,res))
probs = FloatTensor([P.eval(point) for point in points])

plt.figure(figsize=(10,10))
plt.plot(points.numpy(), probs.numpy())
sample_points = torch.distributions.Uniform(x_min, x_max).sample((num_samples,))
sample_probs = FloatTensor([P.unnorm_eval(point) for point in sample_points])

# Compute Z
Z = sample_probs.sum()/(num_samples/(x_max - x_min))
sample_probs /= Z
print('Estimated Z:', Z.numpy())

print('1D Uniform Sampler: P(x)&amp;gt;=0.01: ', sample_probs[sample_probs&amp;gt;=0.01].shape[0]/sample_points.shape[0]*100, '%')

plt.scatter(sample_points.numpy(), sample_probs.numpy())
plt.show()

###########################################
Uniform Sampler
###########################################
True Z: 2.5066282746310002
Estimated Z: 2.5824146
1D Uniform Sampler: P(x)&amp;gt;=0.01:  26.1 %
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As it turns out $ \approx 74\%$ of the values $x \in [-10,10]$ are below $0.01$.
That means that by sampling randomly from a uniform distribution $\mathcal{U}(-10,10)$ we would evaluate $72\%$ of our randomly selected values $x$ with a probability below $0.01$ thus effectively wasting a large number of samples.
Conversely only $28\%$ of the samples would lie in probability regions around the mean $\mu$ with a probability higher than $0.01$.
The evaluation of the area close to $\mu$ would therefore be highly inefficient as we waste a considerable amount of samples in areas which do not contribute to the integral of $Z$.&lt;/p&gt;

&lt;p&gt;Obviously we could restrict the uniform distribution $\mathcal{U}(x_{min}, x_{max})$ to be closer to the value of $\mu$ but this could only be efficiently be done if we were to have specific prior information about the distribution.
Similarly, the area with probabilities below 0.01 decreases if we increase the variance of the Normal distribution.
Again, this is just a toy example to visualize the problems when sampling from high-dimensional spaces.
The main take way point is that one has to pay close attention to what the sampler is doing and how sampling algorithms are designed to make efficient use of computational resources.
I’ll get into the geometric details of high dimensional spaces later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Can we do any better? … In fact we can with the help of physicists from the Manhattan Project and a chain.&lt;/p&gt;

&lt;p&gt;In the example above we want to sample as often as possible in the areas which greatly contribute to the integral $Z = \int_{-\infty}^{\infty} f(x|\mu, \sigma) dx$.
So we could say that once we happen to sample close to $\mu$ we would like to sample as often as possible in the same area, effectively staying in the area around $\mu$.&lt;/p&gt;

&lt;p&gt;The achievement of Mr Metropolis et al. and the extensions by Mr Hastings (is the Netflix-Founder related to him?) was to elaborate precisely that idea and introduce the concept of a sampling chain to our problem.&lt;/p&gt;

&lt;p&gt;The sampling chain is a sequence of samples ${x_0, x_1, \ldots, x_N}$ which is constructed iteratively by sampling a proposal point $x_t’$ and accepting it as the next sample $x_{t+1}$ based on an acceptance condition.
The proposal sample $x’_t$ can be be rejected and in that case a new proposal sample $x’_t$ has to be proposed.
The important part is how the proposed samples in the chain are generated, accepted and rejected.&lt;/p&gt;

&lt;p&gt;In order to stay in areas which contribute to the value of Z, we can propose a sample $x’_t$ which is close to our current sample $x_t$ and evaluate $f(x’_t|\mu, \sigma)$.
We can then compare the new evaluated sample $f(x’_t|\mu, \sigma)$ and the current evaluated sample $f(x_t|\mu, \sigma)$ and accept the proposed sample $x’_t$ as the next sample if $f(x’_t|\mu, \sigma) \geq f(x|\mu, \sigma)$.&lt;/p&gt;

&lt;p&gt;Before long the straight-forward acceptance rule $f(x’_t|\mu, \sigma) \geq f(x_t|\mu, \sigma)$ would take us straight to $x = \mu$.
The problem is that we would be stuck in $x = \mu$ as every value $x \neq \mu$ has a lower evaluation $f(x\neq\mu |\mu, \sigma)$ than $f(x = \mu|\mu, \sigma)$.
You can verify that by simply looking at a plot of a normal distribution.&lt;/p&gt;

&lt;p&gt;In order to stay in the high value area around $\mu$ but also not get stuck in it, a proposal distribution $q(x’|x)$ is evaluated, the ratio $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma)$ is used which is then compared to an auxiliary sample: $U \sim \mathcal{U}(0,1)$.
This is done in the following way:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_MHAlgo.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The acceptance ratio $\alpha$ is computed in a smart way, namely that $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \in (0,\infty)$.
If $f(x’_t|\mu, \sigma)$ is larger than $f(x_t|\mu, \sigma)$, then $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \geq 1$ and the proposal sample will always be accepted.
If on the other hand $f(x’_t|\mu, \sigma) \leq f(x_t|\mu, \sigma)$, then $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \in (0,1)$ there is a chance that we accept the proposed sample $x’_t$ anyway if $u &amp;gt; \alpha$.
By sampling $u \sim \mathcal{U}(0,1)$, occasional steps into areas with lower values $f(x)$ are allowed.&lt;/p&gt;

&lt;p&gt;The final component is the proposal distribution $q(x’|x)$ which is chosen in its simplest form as a standard normal distribution around $x$, namely $x’ \sim \mathcal{N}(x, 1)$.
The chain $\{ x_t \}_ { i=0 }^N$ that is constructed during sampling exhibits the Markov property such that each state $x_t$ is only dependent on the previous state $x_{t-1}$ through the proposal distribution $q(x’|x)$ (and after the proposal was accepted).
The Markov chain property and the Monte Carlo sampling process together define the broader class of sampling algorithms called ‘Markov Chain Monte Carlo’ (MCMC).&lt;/p&gt;

&lt;p&gt;An important criterion while constructing such a Markovian chain of samples is that the sampler can move through the state space in an unbiased way.
The acceptance probability and the proposal distribution should therefore guarantee that the sampler can reach every state if we sample long enough.
For the Metropolis-Hastings algorithm it is required that the proposal density is symmetric such that $q(x’|x) = q(x|x’)$.
This can be realized easily with a Normal distribution with constant standard deviation.
Mathematically, it is required that the Markov chain is reversible which simply states that there should be equal probability when being in state $x$ and moving to $x’$ and reverse:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	p(x',x) &amp;= p(x, x') \\
	p(x' | x) \ p(x) &amp;= p(x | x) \ p(x').
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now decompose the transition probability
$ p(x’|x) $
into the proposal distribution $q(x’|x)$
and the acceptance probability
$\alpha(x’|x)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	\alpha(x'|x) q(x'|x) p(x) = \alpha(x|x') q(x|x') p(x')
\end{align*}&lt;/script&gt;

&lt;p&gt;The equation above simply states that the probability of being in state $x$, proposing to go to state $x’$ and finally accepting to go the state $x’$ should be the same as doing the three steps in reverse.
This guarantees that no state $x$ is favored in any particular way and that the final Markov chain asymptotically approaches the true value of $Z$.&lt;/p&gt;

&lt;p&gt;We can now plug in the acceptance probability of the Metropolis-Hastings sampler and pull the transition and stationary probabilities into the minimum operator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\min\left(1, \frac{p(x')}{p(x)} \right) q(x'|x) p(x) &amp;= \min \left(1, \frac{p(x)}{p(x')} \right) q(x|x') p(x') \\
	\min\left( q(x'|x) p(x), \frac{p(x') q(x'|x) p(x)}{p(x)} \right) &amp;= \min \left(q(x|x') p(x'), \frac{p(x)q(x|x') p(x')}{p(x')} \right) \\
	\min\big( q(x'|x) p(x), p(x') \underbrace{q(x'|x)}_{=q(x|x')} \big) &amp;= \min \big(q(x|x') p(x'), p(x) \underbrace{q(x|x')}_{=q(x'|x)} \big) \\
	% \min\big( q(x'|x) p(x), p(x') \underbrace{q(x'|x)}_{=q(x|x')} \big) &amp;= \min \big(q(x|x') p(x'), p(x) \underbrace{q(x|x')}_{=q(x'|x)} \big)\\
	\min\big( q(x'|x) p(x), p(x') q(x|x') \big) &amp;= \min \big( q(x|x') p(x'), p(x) q(x'|x) \big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above equation is valid and states that the ‘detailed balance’ property holds for the acceptance ratio of the Metropolis-Hastings sampler.
Thus in statistical lingo, we have an unbiased estimator of the true $Z$ which will asymptotically approach the true of value of $Z$!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	print()
	print('###########################################')
	print('Metropolis-Hastings')
	print('###########################################')


	import numpy
	import numpy as np
	import scipy as sp
	import matplotlib.pyplot as plt
	import torch
	import math

	FloatTensor = torch.FloatTensor

	class Gaussian:

		def __init__(self, _mu, _sigma):
			self.mu = _mu
			self.std = _sigma
			self.dist = torch.distributions.Normal(self.mu, self.std)

		def eval(self, _x):
			return self.dist.log_prob(_x).exp()

		def unnorm_eval(self, _x):
			return torch.exp(-1/(2*self.std**2)*(_x - self.mu)**2)

	std = 1
	print('True Z:', np.sqrt(2*np.pi*std**2))
	P = Gaussian(FloatTensor([0]), FloatTensor([std]))
	Q = torch.distributions.Normal(FloatTensor([0]), FloatTensor([1]))
	U = torch.distributions.Uniform(0,1)
	num_samples=100

	res = 200
	x_min=-10
	x_max=10

	#Deterministic evaluation for plotting purposes
	points = torch.from_numpy(np.linspace(x_min,x_max,res))
	probs = FloatTensor([P.eval(point) for point in points])
	plt.figure(figsize=(10,10))
	plt.plot(points.numpy(), probs.numpy())

	# Initialize starting point of chain randomly in U(-10,10)
	x_init = torch.distributions.Uniform(x_min, x_max).sample((1,))
	x = x_init

	accepted_x = []
	accepted_y = []
	rejected_x = []

	num_samples=1000
	burn_in = 200 # Burn_in to make up for random initialization
	for i in range(num_samples+burn_in):

		x_ = Q.sample()
		a = P.unnorm_eval(x+x_)/P.unnorm_eval(x)
		if a &amp;gt;= U.sample():
			if i &amp;gt;= burn_in:
				accepted_x.append(x+x_)
				accepted_y.append(P.unnorm_eval(x+x_))
			x = x+ x_
		else:
			if i &amp;gt;= burn_in: rejected_x.append(x+x_)

	accepted_x = torch.stack(accepted_x, dim=0)
	accepted_y = torch.stack(accepted_y, dim=0)
	rejected_x = torch.stack(rejected_x, dim=0)
	print('Accepted/Rejected: ', accepted_x.shape[0], '/', rejected_x.shape[0])

	#Computing Z
	num_bins = 200
	bins = np.linspace(x_min, x_max, num_bins)
	index = np.digitize(accepted_x, bins)


	bin_counts = np.array([np.sum([index == i]) for i in range(1, len(bins))])
	bin_y_sums = np.array([accepted_y.numpy()[index == i].sum() for i in range(1, len(bins))])

	tmp = bin_y_sums/bin_counts
	tmp[np.isnan(tmp)]=0

	print('Estimated Z:', sum(tmp)/(num_bins/(x_max-x_min)))

	accepted_probs = FloatTensor([P.eval(point) for point in accepted_x])
	print('1D MH Sampler: P(x)&amp;gt;=0.01: ', accepted_probs[accepted_probs&amp;gt;=0.01].shape[0]/num_samples*100, '%')

	plt.scatter(accepted_x.numpy(), accepted_probs.numpy())
	plt.show()

	###########################################
	Metropolis-Hastings
	###########################################
	True Z: 2.5066282746310002
	Accepted/Rejected:  701 / 299
	Estimated Z: 2.4467572424682826
	1D MH Sampler: P(x)&amp;gt;=0.01:  69.69999999999999 %
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The uniform sampled used $26 \%$ of its samples in useful areas whereas the Metropolis-Hastings sampler uses almost $70 \%$!
We can visualize the two samplers next to each other to visualize the increased efficiency of the Metropolis-Hastings Sampler.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/Uniform_SamplingDist.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/blog/HMC/MH_SamplingDist.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can even visualize a Metropolis-Hastings sampler in two dimensions with the accepted and rejected samples:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/MH_2D.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s observable how moves closer to the center of the probability distribution are always accepted whereas moves away are sometimes accepted.
In the cases where the proposed sample is in significantly lower value areas, the move will be rejected.&lt;/p&gt;

&lt;p&gt;We can even improve our sampler even further by treating the surface of the distribution as a physical model.
In order to illustrate we should first take the $-\log$ of the unnormalized function $f(x|\mu, \sigma)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_02.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By working with $-\log f(x | \mu, \sigma)$ we get rid of the pesky exponential term, effectively working with a linear function in case of the Normal distribution.
Additionally, we have transformed the function $f(x|\mu, \sigma)$ into a nice representation where the bottom of the curve $-\log f(x|\mu, \sigma)$ represents the areas in $f(x|\mu, \sigma)/Z$ with the highest probabilities.&lt;/p&gt;

&lt;p&gt;Remember that the Metropolis-Hastings sampler was developed by the physicists at the Manhattan Project to tackle the problem in statistical mechanics of estimating the partition function $Z$.
In statistical mechanics, the function $f(x)$ is the energy function $E(x)$ of a system in state $x$ and the probability of the system being in state $x$ is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	p(x) = \frac{1}{Z} e^{-E[x]}
\end{align*}&lt;/script&gt;

&lt;p&gt;The probability of a system being in state $x$ increases if the energy of that state $x$ is small.
This corresponds to the basic physical behavior, that physical systems want to be in a minimal energy state.
If a piece of metal is hot, it emanates heat and cools down and decreases its energy.
If a ball has high potential energy, it will decrease this energy by dropping down.
If a coil is being compressed, it will expand once the force relents to enter into a low energy state.&lt;/p&gt;

&lt;p&gt;We will now introduce a second idea from physics: Hamiltonian Mechanics.
Let’s say you have physical system $\mathcal{H}(x, p, t)$ in which two properties describes everything you know.
These two properties are the position $x(t)$ and the momentum $p(t)$.
For some position $x(0)$ and the momentum $p(0)$, the physical system $\mathcal{H}(x, p, t)$ will be able to predict every future position $x(t)$ and momentum $p(t)$.&lt;/p&gt;

&lt;p&gt;This is possible due to the following Hamiltonian mechanics:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\frac{d p(t)}{d t} &amp;= -\frac{\partial \mathcal{H}(x, p, t)}{\partial x(t)}\\
	\frac{d x(t)}{d t} &amp;= \frac{\partial \mathcal{H}(x, p, t)}{\partial p(t)}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Hamiltonian $\mathcal{H}(x, p, t)$ corresponds in our simple case to the energy of the system.
The two differential equations above describes how the energy of the system is allocated to either the momentum $p(t)$ or the position $x(t)$ as time progresses.&lt;/p&gt;

&lt;p&gt;The energy of the system $\mathcal{H}(x, p, t)$ consists of the potential energy $E_p(x(t))$ and the kinetic energy $E_k(p(t)) = p(t)^2/2m$ for a particle of mass $m$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
	\mathcal{H}(x, p, t) = E_p(x(t)) + E_k(p(t)) = E_p(x(t)) + \frac{p(t)^2}{2m}
\end{align*}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can apply the Hamiltonian mechanics from above to our function $-\log f(x | \mu ,\sigma)$ and simulate the trajectory of a particle with $m=1$.
For that to happen we first derive both $dp(t)/dt$ and $dx(t)/dt$ for the potential energy provided by $ E_p(x(t)) = - \log f(x(t) | \mu, \sigma)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\frac{d p(t)}{d t} &amp;= -\frac{\partial \mathcal{H}(x, p, t)}{\partial x(t)} \\
	&amp;= \frac{\partial }{\partial x(t)} \log f(x(t) | \mu, \sigma) \\
	&amp;= \frac{1}{f(x(t) | \mu, \sigma)} \frac{\partial}{\partial x(t)} f(x(t) | \mu, \sigma) \\
	\frac{d x(t)}{d t} &amp;= \frac{\partial \mathcal{H}(x, p, t)}{\partial p(t)} \\
	&amp;= \frac{\partial }{ \partial p(t)} \frac{p(t)^2}{2} \\
	&amp;= p(t)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now use the derivations above to simulate the trajectory of a particle on the surface of $-\log f(x(t) | \mu, \sigma)$ applying the following update steps to the position $x(t)$ and momentum $p(t)$ of the particle:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	x(t+\epsilon) &amp;= x(t) + \epsilon \frac{d p(t)}{d t} \\
	&amp;= x(t) + \epsilon p(t) \\
	p(t+ \epsilon) &amp;= p(t) - \epsilon \frac{d p(t)}{dt} \\
	&amp;= p(t) - \epsilon \frac{\partial }{\partial x(t)} \log f(x(t)|\mu, \sigma)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;These two update rules have very intuitive explanations once we have look at the shape of $-\log f(x(t) | \mu, \sigma)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_03.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The particle starts at $x(t_0)$ with a high potential energy and some initial momentum $p(t_0)$.
The gradient of $-\log f(x(t_0)|\mu, \sigma)$ is very steep and thus a lot of extra momentum is added to the particles kinetic energy.
The direction of steepest descent $-\partial \log f(x(t)|\mu, \sigma)/dx(t)$ points in the negative direction on the x-axis and thus extra momentum is added due to the minus sign.
The potential energy  $-\log f(x(t)|\mu, \sigma)$ decreases as the particle moves to the lower area.
Once the particle moved past $x(t)=0$ the trend reverses and the gradient $\partial \log f(x(t)|\mu, \sigma)/dx(t)$ is positive, thus decreasing the momentum.&lt;/p&gt;

&lt;p&gt;While the one dimensional case above is nice to gain an intuition, this whole simulation looks significantly cooler in two dimensions.
I plotted the $- \log f(x(t)|\mu, \sigma)$ surface with the resulting gradients of the surface as arrows.
We can see nicely how the particle runs up the slope at the edge of the low energy basins.
After all its momentum has been converted into potential energy, it makes a u-turn and heads back down the slope.
(That’s actually something we want to prevent which lead to the development of the No-U-Turn-Sampler (NUTS)).&lt;/p&gt;

&lt;p&gt;Here’s some code with PyTorch:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	import numpy as np
	import matplotlib
	import matplotlib.pyplot as plt
	import torch

	FloatTensor = torch.FloatTensor

	pi = FloatTensor([np.pi])


	class Potential:

		#GMM with three different clusters
		def __init__(self):
			self.mean1 = FloatTensor([-2, -2])
			self.covar1 = FloatTensor([[2, -0.5],[-0.5, 1]])
			self.mean2 = FloatTensor([1.4, 1.6])
			self.covar2 = FloatTensor([[2, 0],[0, 1]])
			self.mean3 = FloatTensor([2, -2])
			self.covar3 = FloatTensor([[2, 0],[0, 2]])

		def log_eval(self, x):

			dist1 = torch.distributions.MultivariateNormal(self.mean1, self.covar1)
			dist2 = torch.distributions.MultivariateNormal(self.mean2, self.covar2)
			dist3 = torch.distributions.MultivariateNormal(self.mean3, self.covar3)

			return torch.log(1/3*torch.exp(dist1.log_prob(x))
					 + 1/3*torch.exp(dist2.log_prob(x))
					 +1/3*torch.exp(dist3.log_prob(x)))

	#Initialize potential
	potential = Potential()

	#Data for plotting the surface  
	res = 50
	x = np.linspace(-5,5,res)
	y = np.linspace(-5,5,res)
	X,Y = np.meshgrid(x, y)
	points = FloatTensor(np.stack((X.ravel(),Y.ravel())).T).requires_grad_()
	probs = FloatTensor([potential.log_eval(point).exp() for point in points]).view(res,res)

	#Data for plotting the gradients
	res_coarse = 25
	x_coarse = np.linspace(-5, 5, res_coarse)
	y_coarse = np.linspace(-5, 5, res_coarse)
	X_coarse,Y_coarse = np.meshgrid(x_coarse, y_coarse)
	points_coarse = FloatTensor(np.stack((X_coarse.ravel(),Y_coarse.ravel())).T).requires_grad_()
	grads = [torch.autograd.grad(outputs=potential.log_eval(point), inputs=point)[0] for point in points_coarse]
	grads = torch.stack(grads, dim=0).numpy()

	#Plotting both surface and gradients
	plt.figure(figsize=(10,10))
	plt.contourf(X, Y, probs.numpy(), levels=10)
	plt.quiver(X_coarse, Y_coarse, grads[:,0], grads[:,1], color='black', alpha=0.5)

	#The two lists store the trajectory data
	traj = []
	nuts_criterion = []

	#Initialize the starting position and momentum for the particle
	x = FloatTensor([-2,-2.5]).requires_grad_() # Position
	p = FloatTensor([1,0]) # Momentum

	for i in range(600):

		p = p + 0.05 * torch.autograd.grad(outputs=potential.log_eval(x), inputs=x)[0]
		x = x + 0.05 * p

		traj.append(x.detach())
		nuts_criterion.append((x - x_init).dot(p).detach())

	traj = torch.stack(traj, dim=0).numpy()
	nuts_criterion = torch.stack(nuts_criterion).numpy()
	nuts_criterion = (nuts_criterion-nuts_criterion.min())/(nuts_criterion.max() - nuts_criterion.min())

	for i in range(traj.shape[0]-1):
		plt.plot([traj[i,0], traj[i+1,0]], [traj[i,1],traj[i+1,1]], c=(1-nuts_criterion[i], 0, nuts_criterion[i]))

	plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_2D01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The total energy of the particle is conserved between the potential and the momentum.
We show this by initializing the particle with a very high potential energy and a high momentum and watch it slide over the surface of the distribution in wide arcs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_HighEnergy.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively we could initialize the particle with little potential energy and little momentum.
Since the total energy of the particle is low, it will remain in the low energy (=high probability) basin and sample amply from there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_LowEnergy.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The total energy initialization of the particle at the start of each trajectory can be drawn from a distribution.
The distribution can easily be incorporated into the detailed balance equation and marginalized out such that the sampler is unbiased.&lt;/p&gt;

&lt;p&gt;Here are two links to fully implemented Hamiltonian Monte Carlo Samplers with crazy awesome animations:
&lt;a href=&quot;http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/&quot;&gt;here&lt;/a&gt;
and
&lt;a href=&quot;http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A closing remark about the geometry of high-dimensional spaces:&lt;/p&gt;

&lt;p&gt;While sampling methods are asymptotically correct in their estimation of the posterior distribution, they do not scale well due to the curse of dimensionality.
In a nutshell it refers to the geometric properties of high dimensional spaces.
For example the distance between two points increases as we move into higher and higher dimensional spaces.
A quick example is the Euclidean distance between two points which have a distance of 1 in every of their shared dimensions $\mathbb{R}^N$.
Depending on the dimensionality $N$ we get a distance $\sqrt{\sum_{n=0}^N 1^2}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\mathbb{R}^1 &amp;: \quad \sqrt{1^2}=1 \\
	\mathbb{R}^2 &amp;: \quad \sqrt{1^2+1^2}=1.41 \ldots \\
	\mathbb{R}^3 &amp;: \quad \sqrt{1^2+1^2 +1^2}=1.73 \ldots \\
	\vdots	&amp;
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_Distance.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The more dimensions we add, the larger the space between two points with equal distance in every dimension.
Secondly this creates the problem of requiring an increasing number of samples for higher and higher dimensional spaces.
Let’s say we want to estimate a function by sampling in the Euclidean space spanned between two points with unit distance, ergo distance of 1.
We want to estimate the function and want a resolution of 0.1, i.e. we need 10 samples per dimension.
In $\mathbb{R}^1$ we only require 10 samples to estimate the pdf wit equally spaced sampling points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_04.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But in $\mathbb{R}^2$ we suddenly require 100 samples to estimate the function with the desired resolution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_05.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^3$ we finally need 1000 samples to estimate the function to our liking.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_06.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The required number of samples grows exponentially with the number of dimension, i.e. $10^N$ in $\mathbb{R}^N$ for our specific setup of 10 samples per dimension.
By just adding two dimensions we need $100 \times$ more samples than in one dimension.
That might not sound like a lot, but it quickly accumulates when working in high-dimensional spaces.
This effectively restricts sampling algorithms to applications where we require very precise posteriors as in finance or medicine or where the run-time isn’t a problem.&lt;/p&gt;</content><summary type="html">Sampling with the help of physics</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/HMC/HMC_2D01.png" /></entry><entry><title type="html">Bayesian Non-Negative Matrix Factorization</title><link href="http://localhost:4000/blog/BayesNMF/" rel="alternate" type="text/html" title="Bayesian Non-Negative Matrix Factorization" /><published>2019-02-18T00:00:00+01:00</published><updated>2019-02-18T00:00:00+01:00</updated><id>http://localhost:4000/blog/BayesNMF</id><content type="html" xml:base="http://localhost:4000/blog/BayesNMF/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;non-negative-matrix-factorization&quot;&gt;Non-Negative Matrix Factorization&lt;/h2&gt;

&lt;p&gt;In deterministic NMF each entry in the parameters is a fixed weight or point estimate.
But in Bayesian NMF each entry in the parameters is a distribution which will be the trusty Normal distribution $\mathcal{N}(\mu, \sigma)$.
Now each entry in the matrices $P, Q$, vectors $u, i$ and scalar $b$ will be a distribution, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{uk} \sim \mathcal{N}(\mu_{uk}, \sigma_{uk}) \\
  Q_{ki} \sim \mathcal{N}(\mu_{ki}, \sigma_{ki}) \\
  u_u \sim \mathcal{N}(\mu_{u}, \sigma_{u}) \\
  i_i \sim \mathcal{N}(\mu_{i}, \sigma_{i}) \\
  b \sim \mathcal{N}(\mu_{b}, \sigma_{b}) \\&lt;/script&gt;

&lt;p&gt;The aim is now to find the optimal $\mu’s$ and $\sigma’s$ for all the parameters in $P, Q, u, i$ and $b$.
In the previous post we optimized the mean squared error between our predicted matrix $\hat{R}$ and the real matrix $R$ with gradient descent.
As it turns out we can train the distributions in the same way by applying a trick which is one of the many advantages of the Normal distribution: the reparameterization trick.
I explained the reparameterization trick in detail in another blog post and will simply refer to it &lt;a href=&quot;https://ludwigwinkler.github.io/blog/Reparam/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to allow PyTorch (shout out to the devs!) to backpropagate to the variational parameters $\mu, sigma$ we have to reparameterize the every entry in the parameters.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  P_{uk} &amp;= \mu_{uk} + \mathcal{E} \cdot \sigma_{uk} \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  Q_{ki} &amp;= \mu_{ki} + \mathcal{E} \cdot \sigma_{ki} \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  u_u &amp;= \mu_u + \mathcal{E} \cdot \sigma_u \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  i_i &amp;= \mu_i + \mathcal{E} \cdot \sigma_i \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  b &amp;= \mu_b + \mathcal{E} \cdot \sigma_b \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can rewrite this in matrix notation to write it more compactly with the element-wise multiplication operator $\odot$ and combining the $\mu’s$ and $\sigma’s$ into matrices, i.e. $P_\mu$ and $P_\sigma$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  P &amp;= P_\mu + \mathcal{E} \odot P_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  Q &amp;= Q_\mu + \mathcal{E} \odot Q_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  u &amp;= u_\mu + \mathcal{E} \odot u_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  i &amp;= i_\mu + \mathcal{E} \odot i_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  b &amp;= b_\mu + \mathcal{E} \odot b_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now sample predictions by first sampling the above matrices and vector and then combining them into:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \hat{R} &amp;= P \ Q + u^T + i + b
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\hat{R} \in \mathbb{R}^{U \times I}, P \in \mathbb{R}^{U \times K}, Q \in \mathbb{R}^{K \times I}, u \in \mathbb{R}^{U}, i \in \mathbb{R}^I$ and $b \in \mathbb{R}$.
Special attention for adding the vector $u \in \mathbb{R}^U$ column-wise to the matrix product $PQ$, the vector $i \in \mathbb{R}^I$ row-wise to $PQ$ and b simply to every entry in $PQ$.
The vector $u$ is a combination of scalar biases for every user and is added to every of her/his recommendations in the columns of $\hat{R}$.
The entries in the vector $i$ on the other hand are biases for the items and are added to each row in $\hat{R}$.&lt;/p&gt;

&lt;p&gt;We can sample multiple prediction matrices $\hat{R}$ by first sampling the corresponding parameters and then combining the into a prediction.
From these predictions we can estimate the mean and standard deviation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \mu_{\hat{R}} &amp;= \mathbb{E}[ \ \hat{R} \ \ ] \approx \frac{1}{N} \sum_{i=0}^N \hat{R}_i \\\\
  \sigma_{\hat{R}} &amp;= \sqrt{\mathbb{V}[ \ \hat{R} \ \ ]} \approx \sqrt{ \frac{1}{N} \sum_{i=0}^N ( \hat{R}_i - \mu_{\hat{R}})^2} \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Once we have the mean and standard deviation of the prediction we can construct a cost function through the negative log-likelihood of the real prediction matrix $R$ under the estimated distribution $\hat{R}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(\hat{R}, R) = - \sum_{u,i}^{U,I} \delta_{ui} \left( \log \frac{1}{\sqrt{2\pi \sigma_{\hat{R}_{ui}}^2}} - \frac{1}{2 \sigma_{\hat{R}_{ui}}^2} (R_{ui} - \mu_{\hat{R}_{ui}})^2 \right)
\end{align*}&lt;/script&gt;

&lt;p&gt;where $\delta_{ui}$ is a delta function which is 1 if the entry $R_{ui}$ is in the training data and 0 if the entry $R_{ui}$ is to be predicted.
$\delta_{ui}$ serves as a filter to remove the non-existent data points from the cost function.
Depending on the dimensionality and whether or not we’re estimating covariances as well (which we are not in this case), $N=10$ or $N=20$ is enough to stabilize the cost function.
An important coding tweak is to predict the samples of $\hat{R}$ in parallel by creating a three-dimensional tensor $\mathcal{E}$ with a third ‘sample’ dimension.
That third dimension will be used to compute the $\mu_{\hat{R}}$ and $\sigma_{\hat{R}}$ efficiently without having to resort to for-loops which can be very slow in Python.&lt;/p&gt;

&lt;p&gt;After computing the cost function we can let PyTorch do its automatic differentiation magic and we optimize the whole thing with an optimizer such as Adam.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    import numpy as np
    import torch
    import torch.nn.functional as F

    FloatTensor = torch.FloatTensor

    np.set_printoptions(precision=4)

    class BayesNMF(torch.nn.Module):

            def __init__(self, R, K, lr, iterations):

                    &quot;&quot;&quot;
                    Perform matrix factorization to predict empty
                    entries in a matrix.

                    Arguments
                    - R (ndarray)   : user-item rating matrix
                    - K (int)       : number of latent dimensions
                    - alpha (float) : learning rate
                    - beta (float)  : regularization parameter
                    &quot;&quot;&quot;
                    super().__init__()

                    self.R = R #The recommendation matrix
                    self.data_mask = torch.zeros_like(R) #Mask with 1's where we have data points
                    self.data_mask[R!=0] = 1
                    self.missing_mask = torch.zeros_like(self.R)
                    self.missing_mask[R==0]=1 #Mask with 1's where we don't have data points
                    self.num_users, self.num_items = R.shape #Number of users and items
                    self.K = K #Number of latent dimensions
                    self.lr = lr #Learning rate for the optimizer
                    self.iterations = iterations #Number of gradient descent steps

                    exponential_dist_lambda = 5 #Parameter for exponential distribution used to initialize parameters with small positive values
                    init_std = 0

                    #Parameters with mu and untransformed variances: \sigma = log(1+exp(rho))
                    self.P_mu = torch.nn.Parameter(FloatTensor(self.num_users, self.K).exponential_(exponential_dist_lambda))
                    self.P_rho = torch.nn.Parameter(FloatTensor(self.num_users, self.K).fill_(init_std))
                    self.Q_mu = torch.nn.Parameter(FloatTensor(self.num_items, self.K).exponential_(exponential_dist_lambda))
                    self.Q_rho = torch.nn.Parameter(FloatTensor(self.num_items, self.K).fill_(init_std))

                    self.b_u_mu = torch.nn.Parameter(FloatTensor(self.num_users, 1).exponential_(exponential_dist_lambda))
                    self.b_u_rho = torch.nn.Parameter(FloatTensor(self.num_users, 1).fill_(init_std))
                    self.b_i_mu = torch.nn.Parameter(FloatTensor(self.num_items, 1).exponential_(exponential_dist_lambda))
                    self.b_i_rho = torch.nn.Parameter(FloatTensor(self.num_items, 1).fill_(init_std))
                    self.b_mu = torch.nn.Parameter(FloatTensor([0.1]))
                    self.b_rho = torch.nn.Parameter(FloatTensor([init_std]))

                    #Optimizer for gradient descent
                    self.optim = torch.optim.Adam(self.parameters(), lr=self.lr)

                    #Data for stochastic gradient descent updates instead of full matrix updates
                    self.samples = [(i, j, self.R[i,j]) for i in range(self.num_users) for j in range(self.num_items) if R[i,j]&amp;gt;0]
                    np.random.shuffle(self.samples)
                    self.forward_type = 1

                    #Number of prediction samples computed in parallel
                    self.num_MC_samples = 20

            def forward2(self, _u, _i):

                    #Sample multiple parameters with the reparameterization trick in parallel
                    b = self.b_mu + torch.randn(self.num_MC_samples, *self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu[_i] + torch.randn(self.num_MC_samples, *self.b_i_rho[_i].shape)* F.softplus(self.b_i_rho[_i])
                    b_u = self.b_u_mu[_u] + torch.randn(self.num_MC_samples, *self.b_u_rho[_u].shape)* F.softplus(self.b_u_rho[_u])
                    P = self.P_mu[_u] + torch.randn(self.num_MC_samples, *self.P_rho[_u].shape)*F.softplus(self.P_rho[_u])
                    Q = self.Q_mu[_i] + torch.randn(self.num_MC_samples, *self.Q_rho[_i].shape)*F.softplus(self.Q_rho[_i])

                    prediction = torch.bmm(P.view(self.num_MC_samples, 1, self.K), Q.view(self.num_MC_samples, self.K, 1)).squeeze(-1)
                    prediction += b_u
                    prediction += b_i
                    prediction += b

                    return prediction

            def forward1(self):
                    '''
                    Sample multipile parameters in parallel
                    :return: Prediction matrix of shape [N_MC, U, I]
                    '''


                    b = self.b_mu + torch.randn(self.num_MC_samples, *self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu + torch.randn(self.num_MC_samples, *self.b_i_rho.shape)* F.softplus(self.b_i_rho)
                    b_u = self.b_u_mu + torch.randn(self.num_MC_samples, *self.b_u_rho.shape)* F.softplus(self.b_u_rho)
                    P = self.P_mu + torch.randn(self.num_MC_samples, *self.P_rho.shape)*F.softplus(self.P_rho)
                    Q = self.Q_mu + torch.randn(self.num_MC_samples, *self.Q_rho.shape)*F.softplus(self.Q_rho)

                    pred = torch.bmm(P, Q.transpose_(1,2))
                    pred += b_u
                    pred += b_i.transpose_(1,2)
                    pred += b.unsqueeze(-1)
                    pred *= self.data_mask

                    # Add a little of noise in order to prevent nan's during backpropagation; NaN's are caused by masking and the resulting std=0 for missing values
                    pred += 1e-10*torch.randn(*pred.shape)*self.missing_mask

                    return pred

            def criterion(self, _pred, _label):
                    '''

                    :param _pred: prediction matrix of shape [N_MC, U, I]
                    :param _label: _label matrix of shape [U, I]
                    :return: scalar loss for gradient descent
                    '''
                    mu = _pred.mean(dim=0)
                    sigma = _pred.std(dim=0)

                    loss = (-torch.sum(torch.log(torch.sqrt(1/(2*np.pi*sigma.pow(2))))-1./(2*sigma**2)*(_label-mu)**2))
                    mse_loss = F.mse_loss(_pred, _label) # MSE loss for interpretable loss

                    return loss, mse_loss

            def train_params(self):
                    # Perform stochastic gradient descent for number of iterations
                    for e in range(self.iterations):
                            loss_ = 0
                            for (i, j, r) in self.samples:
                                    self.zero_grad()

                                    pred = self.forward1()
                                    loss, mse_loss = self.criterion(pred, self.R)

                                    #Alternative prediction by sampling a single data point instead of the entire prediction matrix
                                    # pred= self.forward2(i, j)
                                    # loss, mse_loss = self.criterion(pred, r)

                                    loss.backward()
                                    self.optim.step()
                                    loss_+=loss.detach().numpy()

                            if e%1000==0 and e&amp;gt;0:
                                    mf.print_variational_matrix()

                            if (e+1) % 100 == 0:
                                    print(&quot;Iteration: %d ; loss = %.4f ; mse_loss=%.4f&quot; % (e+1, loss_, mse_loss.detach().cpu().numpy().squeeze()))

            def full_matrix(self):
                    &quot;&quot;&quot;
                    Computer a single full matrix using the resultant biases, P and Q
                    &quot;&quot;&quot;
                    b = self.b_mu + torch.randn(*self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu + torch.randn(*self.b_i_rho.shape)* F.softplus(self.b_i_rho)
                    b_u = self.b_u_mu + torch.randn(*self.b_u_rho.shape)* F.softplus(self.b_u_rho)
                    P = self.P_mu + torch.randn(*self.P_rho.shape)* F.softplus(self.P_rho)
                    Q = self.Q_mu + torch.randn(*self.Q_rho.shape)* F.softplus(self.Q_rho)

                    return_ = torch.matmul(P, Q.t())
                    return_ += b_u
                    return_ += b_i.t()
                    return_ += b

                    return return_.detach()

            def print_variational_matrix(self):
                    '''
                    Print the mean and variance of the prediction matrix
                    '''
                    #Sample 100 different prediction matrices
                    Rs = torch.stack([mf.full_matrix() for _ in range(100)])

                    print('Mean:')
                    print(torch.mean(Rs, dim=0).detach().numpy())
                    print('Std')
                    print(np.around(torch.std(Rs, dim=0).detach().numpy(),3))



    R = FloatTensor([       [5, 3, 5, 0],
                            [4, 5, 5, 5],
                            [3, 0, 0, 1],
                            [0, 0, 0, 0],
                            [0, 1, 0, 4],])

    mf = BayesNMF(R, K=3, lr=0.001, iterations=10000)

    mf.train_params()
    mf.print_variational_matrix()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The recommendation matrix $R$ in the code snippet above has a tweak: the fourth row does not contain any entries so every item has to be recommended to user 4.
Upon inspection of the mean $\mu_{\hat{R}}$ and $\sigma_{\hat{R}}$ we can that the variance in the fourth row in markedly increased:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_{\sigma} =
 \left(
\begin{array}{cccc}
 0.000 &amp; 0.001 &amp; 0.000 &amp; 0.000 \\
 0.000 &amp; 0.000 &amp; 0.000 &amp; 0.000 \\
0.918 &amp; 1.358 &amp; 0.833 &amp; 0.974 \\
0.001 &amp; 0.001 &amp; 0.000 &amp; 0.000 \\
 \end{array}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;Because the user hasn’t reviewed anything, we cannot leverage any latent structure through the Bayesian Non-Negative Matrix Factorization.&lt;/p&gt;

&lt;p&gt;Similarly, when predicting the matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R =
\left(
\begin{array}{cccc}
5&amp; 3&amp; 5&amp; 0 \\
                        4&amp; 5&amp; 5&amp; 0\\
                        3&amp; 0&amp; 0&amp; 0\\
                        0&amp; 0&amp; 0&amp; 1\\
                        0&amp; 1&amp; 0&amp; 0\\
\end{array} \right) %]]&gt;&lt;/script&gt;

&lt;p&gt;we will obtain a standard deviation matrix $P_{\sigma}$ which has an increased standard deviation for the fourth row and the last column:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_{\sigma} =
\left(
\begin{array}{cccc}
0  &amp;  0  &amp;  0 &amp;   0.003 \\
0  &amp;  0  &amp;  0 &amp;   0.005 \\
0  &amp;  0.  &amp;  0 &amp;   0.001 \\
0.003 &amp; 0.006 &amp; 0.002 &amp;   0.001    \\
0  &amp;  0  &amp;  0 &amp;   0.001 \\
\end{array} \right) %]]&gt;&lt;/script&gt;

&lt;p&gt;Since user 4 does not share any common likes with any other user the standard deviation is increased for all other movies in the fourth row.
Similarly, movie 4 which user 4 reviewed has an increased standard deviation for the other users as not a single other user has seen it.&lt;/p&gt;

&lt;p&gt;As a closing remark I just want to say that the gradient descent based variational inference approach is not the best approach.
Shinichi Nakajima, a member of my lab published an &lt;a href=&quot;http://www.jmlr.org/papers/volume14/nakajima13a/nakajima13a.pdf&quot;&gt;analytic solution&lt;/a&gt; which can be somewhat involved.
The core motivation of this post was to see whether we could use the reparameterization trick and whether the factorization would exhibit the behaviour which was expected of it with regards to independent entries in $R$.&lt;/p&gt;</content><summary type="html">Being Unsure About What To Recommend</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry></feed>
