<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-10-22T20:23:08+02:00</updated><id>http://localhost:4000/</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">RSA Cryptography:  The Math</title><link href="http://localhost:4000/blog/RSA/" rel="alternate" type="text/html" title="RSA Cryptography: &lt;br&gt; The Math" /><published>2021-10-04T00:00:00+02:00</published><updated>2021-10-04T00:00:00+02:00</updated><id>http://localhost:4000/blog/RSA</id><content type="html" xml:base="http://localhost:4000/blog/RSA/">&lt;head&gt;
&lt;style&gt;
.MathJax_Display, .MJXc-display, .MathJax_SVG_Display {
    overflow-x: auto;
    overflow-y: hidden;
}
&lt;/style&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             processEscapes: true,
           },
		   TeX: {extensions:[&quot;autoload-all.js&quot;]}
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h3 id=&quot;the-advantages-of-public-private-key-encryption&quot;&gt;The advantages of public private key encryption&lt;/h3&gt;

&lt;p&gt;We are all used to having access to secure communication that we seldomly ask ourselves how the encryption that keeps prying eyes from reading our messages actually works.&lt;/p&gt;

&lt;p&gt;What is in fact the best way to &lt;strong&gt;establish&lt;/strong&gt; secure communications?&lt;/p&gt;

&lt;p&gt;In most secret agent movies, there exists a cypher which allows the hero, or the villain, to decode a hidden or encrypted text into its original meaning.
Either it’s selecting specific letters from the newspaper or it’s running a tremendously exciting looking algorithm over some text files on a computer.
But in both cases the hero, or the villain, is already in possession of the knowledge how to decrypt the encrypted text.&lt;/p&gt;

&lt;p&gt;But how did he obtain it knowledge to decrypt the encrypted text?&lt;/p&gt;

&lt;p&gt;Certainly, it’s very dangerous to simply send the decryption key, whatever form it may take, through the plain mail.
It could get intercepted and copied, rendering any further encrypted communication worthless, as the copied decryption key allows the interceptor to decrypt of the secret message.&lt;/p&gt;

&lt;p&gt;The answer lies in asymmetric cryptography.&lt;/p&gt;

&lt;p&gt;The best analogy that I can think of is, that the person who wants to receive encrypted messages passes out publicly an abundance of &lt;strong&gt;open&lt;/strong&gt; locks to which only he has the key to unlock them.
So another person who wants to send you encrypted messages can simply pick up a publicly available open lock, but the message inside, lock it and send you the message.
Since only you have the key to all of these locks, you’re the only one who can open the lock and actually look at what you were sent.
If you were to reply, you would simply pick up a lock that the other persons has publicly on offer, lock you message up and send him or her your message.&lt;/p&gt;

&lt;p&gt;This concept is nice in theory, but how would it work technically and provable mathematically?
Let us establish some basics first which will be important down the road.&lt;/p&gt;

&lt;h3 id=&quot;modulo-the-secret-sauce-of-rsa-imho&quot;&gt;Modulo: The Secret Sauce of RSA IMHO&lt;/h3&gt;

&lt;p&gt;The modulo operator $x \ mod \ n$ takes any number $x$ and returns the remainder after the division with $n$.
Let’s say you want to know $x \ mod \ n$, then we can represent $x$ as $x= k n + r$ for an integer $k$ and the remainder $r$.
The modulo operator is in fact not very complicated and it should be clear after a few examples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  x \mod \ n &amp;= r \quad \text{since}  \quad &amp;&amp;  x = k * n + r    &amp;&amp; \quad \text{with} \quad k \in \mathbb{N}, r \in \mathbb{N} \\ \\
  9 \mod \ 2 &amp;= 1 \quad \text{since}  \quad &amp;&amp;  9 = 4 * 2 + 1    &amp;&amp; \quad \text{with} \quad k=4, r=1 \\
  13 \mod \ 2 &amp;= 1 \quad \text{since} \quad &amp;&amp; 13 = 6 * 2 + 1    &amp;&amp; \quad \text{with} \quad k=6, r=1 \\ \\
  5 \mod \ 3 &amp;= 2 \quad \text{since}  \quad &amp;&amp; 5  = 1 * 3 + 2    &amp;&amp; \quad \text{with} \quad k=1, r=2 \\
  8 \mod \ 3 &amp;= 2 \quad \text{since}  \quad &amp;&amp; 8  = 2 * 3 + 2    &amp;&amp; \quad \text{with} \quad k=2, r=2 \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;I chose the examples with care to highlight a natural property of the modulo operator which makes it so appealing to cryptography.
Namely, that we apply the modulo operator on different numbers and will obtain the same remainder.
If you’re only left with the number $n$ and the remainder $r$, there is no way to obtain the original number $x$.
The modulo operator is basically a one way operation which hides the original number $x$!
The properties of the modulo function that will be important to us only consider large integer values.&lt;/p&gt;

&lt;h3 id=&quot;coprime-integers&quot;&gt;Coprime Integers&lt;/h3&gt;

&lt;p&gt;The next ingredient that is required are coprime integers.
‘Coprime’ might sound threatening and complicated but in fact the definition is quite simple.
Two integer numbers are coprime if their greatest common divisor is 1, $\text{gcd}(a, b)=1$ for $a, b \in \mathbb{N}$.&lt;/p&gt;

&lt;p&gt;In laymen’s terms, this means that both numbers are divisible into an integer only by the number of one.
So for example, 3 and 5 are coprime because 1 is the largest integer divisor of both of them.
The numbers 3 and 15 and not coprime because the greatest common denominator is 3.
The latter example is still divisible by 1 as well, but we’re talking about the &lt;em&gt;greatest&lt;/em&gt; common divisor of two integer numbers.&lt;/p&gt;

&lt;h3 id=&quot;eulers-totient&quot;&gt;Eulers Totient&lt;/h3&gt;

&lt;p&gt;For $n \in \mathbb{N}$, Euler’s totient is given as the number of integer numbers $m$ that are in the set&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\phi(n) = \{m \in \mathbb{N} \ | \ 1 \leq m \leq n, \text{gcd}(m, n) = 1 \}
\end{align}&lt;/script&gt;

&lt;p&gt;which essentially means that we want the number of integers that are smaller than $n$ and are coprime to $n$.
As an example let’s look at &lt;script type=&quot;math/tex&quot;&gt;\phi(9) = | \{1, 2, 4, 5, 7, 8 \} |  = 6&lt;/script&gt;.
The numbers 3, 6 and 9 are not members of $\phi(9)$ because their greatest common divisor is 3, so the restriction $\text{gcd}(\{ 3,6,9 \}, 9) =1$ does not hold.&lt;/p&gt;

&lt;p&gt;More importantly for our application, Euler’s totient is a multiplicative function for two numbers which are relatively prime, or coprime, to each other.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
  \phi(n) = \phi(pq) = \phi(p) \phi(q) \quad ; \text{gcd}(p, q)=1
\end{align}&lt;/script&gt;

&lt;p&gt;Also, taking Euler’s totient of a prime number $p \in \mathbb{P}$ returns $\phi(p) = p-1$ since a prime number is only divisible by 1 or itself and thus every number before $p$ falls into the category of Euler’s totient. This will be important later for the RSA algorithm …&lt;/p&gt;

&lt;h3 id=&quot;eulers-theorem-the-bedrock-of-rsa&quot;&gt;Eulers Theorem: The Bedrock of RSA&lt;/h3&gt;

&lt;p&gt;The impressive Leonhard Euler ( I mean come on, the guy lived 250 years ago and so much of our modern technological applications are based on his math) found out that if $n$ and $a$ are coprime, the following identity holds:
&lt;script type=&quot;math/tex&quot;&gt;\left[ a^{\phi(n)} = 1 \right] \mod \ n.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The notation $[ \ldots ] \mod n$ means that we apply the modulo equally to both sides. The notation $\ldots = \ldots (\mod n)$ is more common but I like my paranthesis to clearly delineate what operator we apply to what.&lt;/p&gt;

&lt;p&gt;Now it turns out that we can massage Euler’s Theorem into a very useful form, namely
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  \Big[ a^{\phi(n)}       &amp;= 1 \Big] \mod \ n \\
  \Big[ a^{k\phi(n)}      &amp;= 1 \Big] \mod \ n \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Right here we smuggled in the integer number $k$ in the exponentiation.
In order to prove the validity of this step, we can construct the following ‘proof’ (Hey, I ain’t no mathematician, but at least the math checks out),&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
  \Big[ a^{k\phi(n)}      = 1 \Big] \mod \ n \\
  \Big[ {\big( a^{\phi(n)} \big) }^k      = 1 \Big] \mod \ n \\
  \Big[ \prod_i^k {\big( a^{\phi(n)} \big) }      = 1 \Big] \mod \ n \\
\end{align}&lt;/script&gt;
for which we can now employ the transitivity rule which states that
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp; \Big[ ab \Big] \mod n \\
  =&amp; \Big[ [a] \ \text{mod} \ n \cdot [b] \ \text{mod} \ n \Big] \mod n \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
  \Big[ \prod_i^k {\big( a^{\phi(n)} \big) }      = 1 \Big] \mod \ n \\
  \Bigg[ \prod_i^k \Big( \Big[ \underbrace{a^{\phi(n)} \Big] \ \text{mod} \ n}_{=1} \Big)      = 1 \Bigg] \mod \ n \\
  \Bigg[ \prod_i^k \Big( 1 \Big)      = 1 \Bigg] \mod \ n \\
  \Bigg[  1   = 1 \Bigg] \mod \ n.
\end{align}&lt;/script&gt;

&lt;p&gt;which proves that we can add an integer multiplier in the exponent.
Carrying on we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp; \Big[ a^{\phi(n)}       = 1 \Big] \mod \ n \\
  &amp; \Big[ a^{k\phi(n)}      = 1 \Big] \mod \ n \quad \Big| \cdot a \\
  &amp; \Big[ a^{k \phi(n)+ 1}  = a \Big] \mod \ n \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The equation above tells us that for some integer number $k$ about which we don’t really care and the number $n$ we can exponentiate the number $a$ with $k \phi(n)+1$ and under the $\text{mod} \ n$ regime we will obtain the same number $a$.
The cool thing that Euler’s Theorem allows us to do is that we can obviously factorize, respectively construct, the exponent $k \phi(n)+1$ in any way we want.&lt;/p&gt;

&lt;p&gt;And this is where we finally get to the main RSA algorithm.&lt;/p&gt;

&lt;h3 id=&quot;rsa-encryption&quot;&gt;RSA Encryption&lt;/h3&gt;

&lt;p&gt;The Rivest-Shamir-Adleman cryptosystem leans heavily, and almost exclusively, on Euler’s theorem to enable an asymmetric cryptographic system.
It starts out by taking a message, which are just numbers for computers, and exponentiating it with the secret key $e$ and applying the modulo operator $\text{mod} \ n $ to it to generate an encrypted cypher text $c$,
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\text{Encryption}: \quad c = [ x^e ] \mod n.
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can conclude that we only need the encryption key $e$ (which is a number) and the modulo basis $n$ (which is also just a number) to be public to enable anybody to encrypt a message for us.
The almost identical operation, albeit with the secret key $d$, is performed to decrypt the cypher message $c$,
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\text{Decryption}: \quad x = [ c^d ] \mod n
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The big question is obviously whether there is a strict one-to-one correspondence between the encryption and the decryption.
Because a cryptographic system which maps multiple messages into the same decrypted message is absolutely worthless.
To show this, we can again employ Euler’s theorem to guarantee the needed one-to-one correspondence, such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
[ c^d &amp;= x ] \mod n \\
\Big[ \big( [ x^e ] \mod n \big)^d &amp;= x \Big] \mod n \\
\Big[ \prod^d_i \big( [ x^e ] \mod n \big) &amp;= x \Big] \mod n \quad \\
\Big[ \prod^d_i x^e &amp;= x \Big] \mod n \quad \text{(Transitivity Rule)} \\
\Big[ \big( x^e \big)^d &amp;= x \Big] \mod n \\
\Big[ x^{ed} &amp;= x \Big] \mod n \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus if we are able to find a corresponding exponent in our encryption/decryption scheme that fulfills Euler’s theorem, we have a wonderful asymmetric cryptographic system which checkmarks all the requirements we wanted:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\Big[ x^{ed} &amp;= x \Big] \mod \ n \quad \Leftrightarrow \quad \Big[ a^{k \phi(n)+ 1}  = a \Big] \mod \ n 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The beauty of the modulo operator is that it’s really hard to guess the correct $d$ since there are an infinite number of possible $d$’s due to the arbitrary $k$ for which the modulo operator returns a possible correct $x$.&lt;/p&gt;

&lt;p&gt;The only remaining question is now how to choose the modulo basis $n$ of which we want to compute Euler’s totient &lt;strong&gt;efficiently&lt;/strong&gt;. 
Naturally, we want $n$ to be very large such that an interceptor can’t simply try out all the $k$’s and Euler’s totient $\phi(n)$’s by brute force, since the RSA key generation scheme for $e$ and $d$ is public.
Instead we want a very, very large number $n$ for which it is almost impossible to determine the totient $\phi(n)$ by brute force, but which we construct in such a way that we know the totient in single evaluation during the key generation.&lt;/p&gt;

&lt;p&gt;For this we will use the previousuly stated property of the totient that if two numbers are coprime to each other, we can simply factorize the totient into the multiplicative components.
Furthermore we can leverage the fact, and this is extremely important for the RSA cryptographic system, that Euler’s totient of a prime number $p$ is just $\phi(p)= p-1$ since a prime number is integer divisible only by itself and one.
This means that Euler’s totient for prime numbers is just the respective prime number reduced by one, which means that every number before the prime number is coprime to it as the greatest common divisor of a prime number is 1.&lt;/p&gt;

&lt;p&gt;Thus if we construct $n=pq$ from two very large, but distinct, prime numbers $p, q \in \mathbb{P}$, we can factorize the practically incalculable totient of $n$ into a fairly easily computable product,
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\phi(n) = \phi(p)\phi(q) = (p-1)(q-1).
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;That means that the main effort of the RSA cryptographic system lies with the generation of two very large prime numbers which have to remain secret as they are used during the generation of the private key.
Just to reiterate, computing Euler’s totient $\phi(n)$ for a very large $n$ is practically impossible at the moment (but let’s maybe wait for quantum computers), while the private knowledge of the two prime numbers $p$ and $q$ which constitute $n$ multiplicatively allows us to generate a correct private key quite easily.
In fact after having generated $p$ and $q$ we can simply pick a super large $k$ and determine $d$ quite easily as $d=(k(p-1)(q-1)+1)/e$.&lt;/p&gt;

&lt;p&gt;In order to get the mathematical equivalency that the encryption and subsequent decryption under the modulo $n$ operation is valid, all we have to do is find a $d$ for which holds&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
e d &amp;= k \phi(n)+1 \\
e d &amp;= k \phi(pq)+1 \\
e d &amp;= k \phi(p) \phi(q) + 1 \\
e d &amp;= k (p-1)(q-1) + 1 \\
\Big[ ed &amp;= 1 \Big] \mod (p-1)(q-1) \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which tells us that we are indeed generating the private key $d$ with the private ingredients of $\phi(p)$ and $\phi(q)$
of our two secret prime numbers $p$ and $q$.&lt;/p&gt;

&lt;p&gt;The last thing to check is that $a$ and $n$ are indeed coprime (not necessarily prime themselves) which happens with overwhelming probability as $n$ becomes increasingly large.&lt;/p&gt;

&lt;h3 id=&quot;signatures-in-rsa&quot;&gt;Signatures in RSA&lt;/h3&gt;

&lt;p&gt;An intriguing property of the public private key encryption is its ability to sign messages as a sender through the use of a cryptographic hash function.&lt;/p&gt;

&lt;p&gt;A cryptographic hash maps data of an arbitrary size to a bit array of fixed size.
It is by all practical means a seemingly random function, although it is in fact deterministic but people have spent a lot of time perfecting the ostensible randomness.
Importantly, hash functions should be quick to compute, infeasible to reverse and to find two messages with the same hash and finally changing the input message slightly should return a completely different hash.
Illustratively, hash functions permute input, cycle over it a couple of times and do all other crazy things to map the variable input length array into a fixed one.&lt;/p&gt;

&lt;p&gt;But let’s get back to RSA …&lt;/p&gt;

&lt;p&gt;A sender would send the message through a hash function $h = hash(x)$ and subsequently exponentiate it with the private key $d$.
The receiver can then exponetiate the ‘decrypted’ hash again with the public key $e$ and take the modulo of it and compare it to his or her own hash of the sent message.&lt;/p&gt;

&lt;p&gt;Mathematically, we can employ Euler’s theorem again, having defined the product of the private and public key as $ed = k \phi(n)+1$, to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \Big[ {\big(h^d \big)}^e &amp;= h \Big] \mod n \\
  \Big[  h^{de} &amp;= h \Big] \mod n \quad ed = k\phi(n)+1 \\
  \Big[  h^{k\phi(n)+1} &amp;= h \Big] \mod n \quad \text{True via Euler's theorem} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Cha-Ching Baby …&lt;/p&gt;</content><summary type="html">The math that ensures that it's none of your business</summary></entry><entry><title type="html">Berlin Over The Years</title><link href="http://localhost:4000/blog/Berlin2021/" rel="alternate" type="text/html" title="Berlin Over The Years" /><published>2021-08-31T00:00:00+02:00</published><updated>2021-08-31T00:00:00+02:00</updated><id>http://localhost:4000/blog/Berlin2021</id><content type="html" xml:base="http://localhost:4000/blog/Berlin2021/">&lt;!-- ## Berlin Over The Years --&gt;
&lt;style&gt;
    .image-gallery {overflow: auto; margin-left: -1%!important;}
    .image-gallery li {float: left; float: top; display: block; margin: 0 0 1% 1%; width: 99%;}
    .image-gallery li a {text-align: top; text-decoration: none!important; color: #777;}
    .image-gallery li a span {display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0;}
    .image-gallery li a img {width: 100%; height: 100%; display: flex; vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-11.jpg&quot; title=&quot;Berlin-11&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-11.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-11&quot; title=&quot;Berlin-11&quot; /&gt;&lt;span&gt;Berlin-11&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-11.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-11.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-13.jpg&quot; title=&quot;Berlin-13&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-13.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-13&quot; title=&quot;Berlin-13&quot; /&gt;&lt;span&gt;Berlin-13&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-13.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-13.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-15.jpg&quot; title=&quot;Berlin-15&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-15.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-15&quot; title=&quot;Berlin-15&quot; /&gt;&lt;span&gt;Berlin-15&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-15.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-15.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-170.jpg&quot; title=&quot;Berlin-170&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-170.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-170&quot; title=&quot;Berlin-170&quot; /&gt;&lt;span&gt;Berlin-170&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-170.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-170.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-180.jpg&quot; title=&quot;Berlin-180&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-180.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-180&quot; title=&quot;Berlin-180&quot; /&gt;&lt;span&gt;Berlin-180&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-180.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-180.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-61.jpg&quot; title=&quot;Berlin-61&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/Berlin-61.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-61&quot; title=&quot;Berlin-61&quot; /&gt;&lt;span&gt;Berlin-61&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/Berlin-61.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/Berlin-61.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/DSC4686.jpg&quot; title=&quot;DSC4686&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/DSC4686.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC4686&quot; title=&quot;DSC4686&quot; /&gt;&lt;span&gt;DSC4686&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/DSC4686.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/DSC4686.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/DSC4952.jpg&quot; title=&quot;DSC4952&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2021/DSC4952.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC4952&quot; title=&quot;DSC4952&quot; /&gt;&lt;span&gt;DSC4952&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2021/DSC4952.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2021/DSC4952.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;</content><category term="photography" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/berlin2021/Berlin-15.jpg" /></entry><entry><title type="html">Spain</title><link href="http://localhost:4000/blog/Spain21/" rel="alternate" type="text/html" title="Spain" /><published>2021-07-01T00:00:00+02:00</published><updated>2021-07-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/Spain21</id><content type="html" xml:base="http://localhost:4000/blog/Spain21/">&lt;style&gt;
    .image-gallery {overflow: auto; margin-left: -1%!important;}
    .image-gallery li {float: left; float: top; display: block; margin: 0 0 1% 1%; width: 99%;}
    .image-gallery li a {text-align: top; text-decoration: none!important; color: #777;}
    .image-gallery li a span {display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0;}
    .image-gallery li a img {width: 100%; height: 100%; display: flex; vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2100.jpg&quot; title=&quot;DSC2100&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2100.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2100&quot; title=&quot;DSC2100&quot; /&gt;&lt;span&gt;DSC2100&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2100.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2100.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2101.jpg&quot; title=&quot;DSC2101&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2101.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2101&quot; title=&quot;DSC2101&quot; /&gt;&lt;span&gt;DSC2101&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2101.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2101.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2102.jpg&quot; title=&quot;DSC2102&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2102.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2102&quot; title=&quot;DSC2102&quot; /&gt;&lt;span&gt;DSC2102&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2102.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2102.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2102_.jpg&quot; title=&quot;DSC2102_&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2102_.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2102_&quot; title=&quot;DSC2102_&quot; /&gt;&lt;span&gt;DSC2102_&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2102_.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2102_.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2103.jpg&quot; title=&quot;DSC2103&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2103.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2103&quot; title=&quot;DSC2103&quot; /&gt;&lt;span&gt;DSC2103&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2103.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2103.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2260.jpg&quot; title=&quot;DSC2260&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2260.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2260&quot; title=&quot;DSC2260&quot; /&gt;&lt;span&gt;DSC2260&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2260.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2260.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2439.jpg&quot; title=&quot;DSC2439&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2439.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2439&quot; title=&quot;DSC2439&quot; /&gt;&lt;span&gt;DSC2439&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2439.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2439.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2453.jpg&quot; title=&quot;DSC2453&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2453.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2453&quot; title=&quot;DSC2453&quot; /&gt;&lt;span&gt;DSC2453&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2453.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2453.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2492.jpg&quot; title=&quot;DSC2492&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2492.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2492&quot; title=&quot;DSC2492&quot; /&gt;&lt;span&gt;DSC2492&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2492.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2492.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2748.jpg&quot; title=&quot;DSC2748&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/spain21/DSC2748.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2748&quot; title=&quot;DSC2748&quot; /&gt;&lt;span&gt;DSC2748&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/spain21/DSC2748.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/spain21/DSC2748.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
&lt;/ul&gt;</content><category term="photography" /><summary type="html">Granada</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/spain21/DSC2103.jpg" /></entry><entry><title type="html">Greece</title><link href="http://localhost:4000/blog/Greece21/" rel="alternate" type="text/html" title="Greece" /><published>2021-06-01T00:00:00+02:00</published><updated>2021-06-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/Greece21</id><content type="html" xml:base="http://localhost:4000/blog/Greece21/">&lt;style&gt;
    .image-gallery {overflow: auto; margin-left: -1%!important;}
    .image-gallery li {float: left; float: top; display: block; margin: 0 0 1% 1%; width: 99%;}
    .image-gallery li a {text-align: top; text-decoration: none!important; color: #777;}
    .image-gallery li a span {display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0;}
    .image-gallery li a img {width: 100%; height: 100%; display: flex; vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1780.jpg&quot; title=&quot;DSC1780&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1780.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1780&quot; title=&quot;DSC1780&quot; /&gt;&lt;span&gt;DSC1780&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1780.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1780.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1783.jpg&quot; title=&quot;DSC1783&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1783.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1783&quot; title=&quot;DSC1783&quot; /&gt;&lt;span&gt;DSC1783&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1783.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1783.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1784.jpg&quot; title=&quot;DSC1784&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1784.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1784&quot; title=&quot;DSC1784&quot; /&gt;&lt;span&gt;DSC1784&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1784.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1784.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1888.jpg&quot; title=&quot;DSC1888&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1888.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1888&quot; title=&quot;DSC1888&quot; /&gt;&lt;span&gt;DSC1888&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1888.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1888.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1918.jpg&quot; title=&quot;DSC1918&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1918.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1918&quot; title=&quot;DSC1918&quot; /&gt;&lt;span&gt;DSC1918&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1918.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1918.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1924.jpg&quot; title=&quot;DSC1924&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC1924.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1924&quot; title=&quot;DSC1924&quot; /&gt;&lt;span&gt;DSC1924&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC1924.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC1924.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2009.jpg&quot; title=&quot;DSC2009&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC2009.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2009&quot; title=&quot;DSC2009&quot; /&gt;&lt;span&gt;DSC2009&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2009.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC2009.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2023.jpg&quot; title=&quot;DSC2023&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC2023.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2023&quot; title=&quot;DSC2023&quot; /&gt;&lt;span&gt;DSC2023&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2023.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC2023.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2127.jpg&quot; title=&quot;DSC2127&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/greece21/DSC2127.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC2127&quot; title=&quot;DSC2127&quot; /&gt;&lt;span&gt;DSC2127&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/greece21/DSC2127.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/greece21/DSC2127.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;</content><category term="photography" /><summary type="html">Thessaloniki &amp; Mount Olympus</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/greece21/DSC1783.jpg" /></entry><entry><title type="html">Italy</title><link href="http://localhost:4000/blog/Italy21/" rel="alternate" type="text/html" title="Italy" /><published>2021-05-01T00:00:00+02:00</published><updated>2021-05-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/Italy21</id><content type="html" xml:base="http://localhost:4000/blog/Italy21/">&lt;!-- ## Berlin Over The Years --&gt;
&lt;style&gt;
    .image-gallery {overflow: auto; margin-left: -1%!important;}
    .image-gallery li {float: left; float: top; display: block; margin: 0 0 1% 1%; width: 99%;}
    .image-gallery li a {text-align: top; text-decoration: none!important; color: #777;}
    .image-gallery li a span {display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0;}
    .image-gallery li a img {width: 100%; height: 100%; display: flex; vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0927.jpg&quot; title=&quot;DSC0927&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC0927.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC0927&quot; title=&quot;DSC0927&quot; /&gt;&lt;span&gt;DSC0927&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0927.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC0927.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0939.jpg&quot; title=&quot;DSC0939&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC0939.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC0939&quot; title=&quot;DSC0939&quot; /&gt;&lt;span&gt;DSC0939&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0939.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC0939.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0940.jpg&quot; title=&quot;DSC0940&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC0940.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC0940&quot; title=&quot;DSC0940&quot; /&gt;&lt;span&gt;DSC0940&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0940.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC0940.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0950.jpg&quot; title=&quot;DSC0950&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC0950.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC0950&quot; title=&quot;DSC0950&quot; /&gt;&lt;span&gt;DSC0950&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0950.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC0950.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0966.jpg&quot; title=&quot;DSC0966&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC0966.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC0966&quot; title=&quot;DSC0966&quot; /&gt;&lt;span&gt;DSC0966&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC0966.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC0966.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1065.jpg&quot; title=&quot;DSC1065&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1065.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1065&quot; title=&quot;DSC1065&quot; /&gt;&lt;span&gt;DSC1065&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1065.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1065.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1169.jpg&quot; title=&quot;DSC1169&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1169.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1169&quot; title=&quot;DSC1169&quot; /&gt;&lt;span&gt;DSC1169&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1169.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1169.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1181.jpg&quot; title=&quot;DSC1181&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1181.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1181&quot; title=&quot;DSC1181&quot; /&gt;&lt;span&gt;DSC1181&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1181.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1181.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1447.jpg&quot; title=&quot;DSC1447&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1447.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1447&quot; title=&quot;DSC1447&quot; /&gt;&lt;span&gt;DSC1447&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1447.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1447.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1614.jpg&quot; title=&quot;DSC1614&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1614.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1614&quot; title=&quot;DSC1614&quot; /&gt;&lt;span&gt;DSC1614&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1614.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1614.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1719.jpg&quot; title=&quot;DSC1719&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1719.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1719&quot; title=&quot;DSC1719&quot; /&gt;&lt;span&gt;DSC1719&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1719.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1719.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1722-Pano.jpg&quot; title=&quot;DSC1722-Pano&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/italy21/DSC1722-Pano.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;DSC1722-Pano&quot; title=&quot;DSC1722-Pano&quot; /&gt;&lt;span&gt;DSC1722-Pano&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/italy21/DSC1722-Pano.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/italy21/DSC1722-Pano.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;</content><category term="photography" /><summary type="html">Lake Garda, Milan, Bergamo, Brescia</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/italy21/DSC0940.jpg" /></entry><entry><title type="html">Reverse Time Stochastic Differential Equations</title><link href="http://localhost:4000/blog/ReverseTimeAnderson/" rel="alternate" type="text/html" title="Reverse Time Stochastic Differential Equations" /><published>2021-04-09T00:00:00+02:00</published><updated>2021-04-09T00:00:00+02:00</updated><id>http://localhost:4000/blog/ReverseTimeAnderson</id><content type="html" xml:base="http://localhost:4000/blog/ReverseTimeAnderson/">&lt;head&gt;
&lt;style&gt;
.MathJax_Display, .MJXc-display, .MathJax_SVG_Display {
    overflow-x: auto;
    overflow-y: hidden;
}
&lt;/style&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             processEscapes: true
           },
		   TeX: {extensions:[&quot;autoload-all.js&quot;]}
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;What follows is a derivation of the main result of ‘Reverse-Time Diffusion Equation Models’ by Brian D.O. Anderson (1982).
Earlier on this blog we learned that a stochastic differential equation of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	dX_t = \mu(X_t, t) dt + \sigma(X_t, t) dW_t
\end{align}&lt;/script&gt;

&lt;p&gt;with the derivative of Wiener process $W_t$ admits two types of equations, called the forward Kolmogorov or Fokker-Planck equation and the backward Kolmogorov equation.
For notational brevity we will use the term $\mu(x_t)$ for the drift and $\sigma(x_t)$ as the diffusion parameter and omit the explicit time dependency.&lt;/p&gt;

&lt;p&gt;The Kolmogorov forward equation is identical to the Fokker Planck equation and states&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	\partial_t p(x_t) = -\partial_{x_t} \left[ \mu(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right].
\end{align}&lt;/script&gt;

&lt;p&gt;It describes the evolution of a probability distribution $p(x_t)$ forward in time.
We can quite frankly think of it as, for example, a Normal distribution being slowly transformed into an arbitrary complex distribution according to the drift and diffusion parameters $\mu(x_t)$ and $\sigma(x_t)$.&lt;/p&gt;

&lt;p&gt;The Kolmogorov backward equation for $t \leq s$ is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
	- \partial_t p(x_s | x_t) = \mu(x_t) \ \partial_{x_t} p(x_s|x_t) + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t)
\end{align}&lt;/script&gt;

&lt;p&gt;and it basically answers the question how the probability of $x_s$ at a later point in time changes as we change $x_t$ at an earlier point in time.
The Kolmogorov backward equation is somewhat confounding with respect to time as we’re taking the partial derivative with respect to the earlier time step $t$ on which we are also coniditoning.
But we can think of it as asking ‘How does the probability of $x_s$ at the later point in time $s$ change, as we slowly evolve the probability distribution backwards through time and condition on $x_t$’.&lt;/p&gt;

&lt;p&gt;Taking inspiration from our crude example earlier, the backward equation offers a partial differential equation which we can solve backward in time, which would correspond to evolving the arbitrarily complex distribution backwards to our original Normal distribution. 
Unfortunately there is no corresponding stochastic differential equation with a drift and diffusion term that describes the evolution of a random variable backwards through time in terms of a stochastic differential equation.&lt;/p&gt;

&lt;p&gt;This is where the remarkable result from Anderson (1982) comes into play.&lt;/p&gt;

&lt;p&gt;The granddaddy of all probabilistic equations, Bayes theorem, tells us that a joint distribution can be factorized by conditioning: $p(x_s , x_t) = p(x_s|x_t) p(x_t)$ with the time ordering $t \leq s$.
Why do we invoke the joint probability $p(x_s, x_t)$ we might ask?
What we’re trying to achieve is to derive a stochastic differential equation that tells us from what values of $x_t$ we can arrive at $x_s$.
We can ask ourselves what the partial differential equation would be that describes the evolution of the joint distribution over time.
First multiplying both sides of Bayes theorem with minus one and taking the derivative with respect to time $t$, we obtain via the product rule&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	- \partial_t p(x_s, x_t) &amp;= - \partial_t \left[ p(x_s| x_t) p(x_t) \right] \\
	&amp;= \underbrace{-\partial_t p(x_s|x_t)}_{\text{KBE}} p(x_t) - p(x_s | x_t) \underbrace{\partial_t p(x_t)}_{\text{KFE}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;into which we can plug in the Kolmogorov forward (KFE) and Kolmogorov backward (KBE) equations,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	&amp; -\partial_t p(x_s|x_t) p(x_t) - p(x_s | x_t) \partial_t p(x_t) \\
	&amp;= \left( \mu(x_t) \ \partial_{x_t} p(x_s|x_t) + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \right) p(x_t) \\
	&amp; + p(x_s| x_t) \left( \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] - \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The derivative occuring in the backward Kolmogorov equation are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_{x_t} p(x_s|x_t) &amp;= \partial_{x_t} \left[ \frac{p(x_s, x_t)}{p(x_t)} \right] \\
	&amp; = \frac{\partial_{x_t} p(x_s, x_t) p(x_t) - p(x_s, x_t) \partial_{x_t} p(x_t)}{p^2(x_t)} \\
	&amp; = \frac{\partial_{x_t} p(x_s, x_t)}{p(x_t)} - \frac{p(x_s, x_t) \partial_{x_t} p(x_t)}{p^2(x_t)}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is to evaluate the derivative of the products in the forward Kolmogorov equation.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_{x_t} \left[ \mu(x_t) p(x_t) \right] &amp; = \partial_{x_t} \mu(x_t) \ p(x_t) + \mu(x_t) \ \partial_{x_t} p(x_t) \\
	\partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] &amp; = \partial_{x_t}^2 \sigma^2(x_t) \ p(x_t) + 2 \ \partial_{x_t} \sigma^2(x_t) \ \partial_{x_t} p(x_t) + \sigma^2(x_t) \ \partial_{x_t}^2 p(x_t)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Plugging in the derivatives of the probability distributions we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	- \partial_t p(x_s, x_t)
	= &amp; - \partial_t \left[ p(x_s| x_t) p(x_t) \right] \\
	= &amp; -\partial_t p(x_s|x_t) p(x_t) - p(x_s | x_t) \partial_t p(x_t) \\
	= &amp; \left( \mu(x_t) \ \partial_{x_t} p(x_s|x_t) + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \right) p(x_t) \\
	&amp; + p(x_s| x_t) \left( \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] - \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \right) \\
	= &amp; \mu(x_t) \ \partial_{x_t} p(x_s|x_t) \ p(x_t) 
	+ \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) \\
	&amp; + p(x_s| x_t) \partial_{x_t} \mu(x_t) \ p(x_t) + p(x_s| x_t) \mu(x_t) \ \partial_{x_t} p(x_t) \\
	&amp; - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \mu(x_t) \ \left(\frac{\partial_{x_t} p(x_s, x_t)}{\cancel{p(x_t)}} - \frac{p(x_s, x_t) \partial_{x_t} p(x_t)}{p^{\cancel{2}}(x_t)} \right) \ \cancel{p(x_t)} \\
	&amp; + p(x_s| x_t) \partial_{x_t} \mu(x_t) \ p(x_t) + p(x_s| x_t) \mu(x_t) \ \partial_{x_t} p(x_t) \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \mu(x_t) \ \left(\partial_{x_t} p(x_s, x_t) - \frac{p(x_s, x_t) \partial_{x_t} p(x_t)}{p(x_t)} \right) \\
	&amp; + p(x_s| x_t) \partial_{x_t} \mu(x_t) \ p(x_t) + p(x_s| x_t) \mu(x_t) \ \partial_{x_t} p(x_t) \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \mu(x_t) \ \left(\partial_{x_t} p(x_s, x_t) - \cancel{p(x_s| x_t) \partial_{x_t} p(x_t)} \right) \\
	&amp; + p(x_s, x_t) \partial_{x_t} \mu(x_t) + \cancel{p(x_s| x_t) \mu(x_t) \ \partial_{x_t} p(x_t)} \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \underbrace{\mu(x_t) \ \partial_{x_t} p(x_s, x_t) + p(x_s, x_t) \partial_{x_t} \mu(x_t)}_{\text{product rule}} \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now see that the drift term already fulfills the requirements of the forward Kolmogorov equation.
What we’re left with and which we have to take care of are the two terms with the diffusion terms $\sigma^2(x_t)$.
The goal is to fuse the two terms into one which resembles the diffusion term in the forward Kolmogorov equation.
Just to reiterate it one more time, if we can massage the partial differential equation into the functional form of the Kolmogorov forward equation, we have a one-to-one correspondence to a stochastic differential equation that can be solved backward in time.&lt;/p&gt;

&lt;p&gt;Following the gracious help from Brian Anderson himself in an email from down under, we can simplify the terms with $\sigma^2(x_t)$ by expanding the last term.
The important step, that Brian pointed out to me, is to factorize the joint distribution $p(x_s, x_t) = p(x_s| x_t) p(x_t)$ and invoke the product rule to match the terms,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	&amp; \frac{1}{2} \partial_{x_t}^2 \left[ p(x_s, x_t) \sigma^2(x_t) \right] \\
	= &amp; \frac{1}{2} \partial_{x_t}^2 \left[ p(x_s | x_t) p(x_t) \sigma^2(x_t) \right] \\
	= &amp; \frac{1}{2} \partial_{x_t}^2 p(x_s | x_t) p(x_t) \sigma^2(x_t) + \partial_{x_t} \left[ p(x_t) \sigma^2(x_t) \right] \partial_{x_t} p(x_s| x_t)
	 + \frac{1}{2} \partial_{x_t}^2 \left[ p(x_t) \sigma^2(x_t) \right] p(x_s| x_t)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can see from the expansion of the derivative above that we can combine the terms in our derivation if we expand the “center term”.
Furthermore we can employ the identity $-\frac{1}{2} X = -X + \frac{1}{2} X$ to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	-\partial_t p(x_s, x_t)
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t) - \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] \\
	&amp; + \frac{1}{2} \ \sigma^2(x_t) \ p(x_t) \ \partial_{x_t}^2 p(x_s | x_t) - \underbrace{ \frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] }_{-\frac{1}{2} X = -X + \frac{1}{2} X} \\
	&amp; \pm \partial_{x_t} p(x_s | x_t) \partial_{x_t} \left[ p(x_t) \sigma^2(x_t) \right] \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] \\
	&amp; \textcolor{red}{+ \frac{1}{2} \ \sigma^2(x_t) \ \partial_{x_t}^2 p(x_s | x_t) \ p(x_t)} \\
	&amp; \underbrace{ - p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] + \textcolor{red}{\frac{1}{2} p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right]} }_{-\frac{1}{2} X = -X + \frac{1}{2} X} \\
	&amp; \textcolor{red}{\pm \partial_{x_t} p(x_s | x_t) \partial_{x_t} \left[ p(x_t) \sigma^2(x_t) \right]} \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] + \textcolor{red}{\frac{1}{2} \partial_{x_t}^2 \left[ p( x_s | x_t) p(x_t) \sigma^2(x_t) \right]} \\
	&amp; \underbrace{- p(x_s| x_t) \partial_{x_t}^2 \left[ \sigma^2(x_t) \ p(x_t) \right] - \partial_{x_t} p(x_s | x_t) \partial_{x_t} \left[ p(x_t) \sigma^2(x_t) \right]}_{
		- \partial_{x_t} \left[ p(x_s| x_t) \partial_{x_t} \left[ \sigma^2(x_t) \ p(x_t) \right] \right] \text{ (product rule) }
		} \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ p( x_s , x_t) \sigma^2(x_t) \right] \\
	&amp; - \partial_{x_t} \left[ p(x_s| x_t) \partial_{x_t} \left[ \sigma^2(x_t) \ p(x_t) \right] \right] \\
	= &amp; \partial_{x_t} \left[ \mu(x_t) \ p(x_s, x_t) - p(x_s| x_t) \partial_{x_t} \left[ \sigma^2(x_t) \ p(x_t) \right] \right] \\
	&amp; + \frac{1}{2} \partial_{x_t}^2 \left[ p( x_s , x_t) \sigma^2(x_t) \right] \\
	= &amp; \partial_{x_t} \left[ p(x_s, x_t) \left( \mu(x_t) - \frac{1}{p(x_t)} \partial_{x_t} \left[ \sigma^2(x_t) \ p(x_t) \right] \right) \right] \\
	&amp; + \frac{1}{2} \partial_{x_t}^2 \left[ p( x_s , x_t) \sigma^2(x_t) \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which finally gives us a stochastic differential equation that we can solve backward in time:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
d\tilde{X}_t = \left(\mu(x_t) - \frac{1}{p(x_t)} \partial_{x_t} \left[ \sigma^2(x_t) \ p(x_t) \right] \right) dt + \sigma(x_t) d\tilde{W}_t
\end{align}&lt;/script&gt;
where $\tilde{W}_t$ is a Wiener process that flows backward in time.&lt;/p&gt;

&lt;p&gt;The common application of the reverse time stochastic differential equations is in generative modelling.
In these applications the noise $\sigma(x_t)$ is made independent of the input such that we can pull it out of the partial derivative $\partial_{x_t}$.
By additionally using the log-derivative trick we obtain a term which is dependent on the score&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
d\tilde{X}_t &amp;= \left(\mu(x_t) - \sigma_t^2 \frac{1}{p(x_t)} \partial_{x_t} \ p(x_t) \right) dt + \sigma_t d\tilde{W}_t \\
&amp;= \Big(\mu(x_t) - \sigma_t^2 \underbrace{\partial_{x_t} \log p(x_t)}_{\text{ML Model}} \Big) dt + \sigma_t d\tilde{W}_t \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Score based generative modelling proceeds by defining a, what I creatively call, ‘noising’ process which takes data and turns it into garbage by iteratively adding noise along an analytically known SDE.
The data dependent drift $\mu(x_t) = \alpha_t x_0$ with $\alpha_s &amp;gt; \alpha_t, s&amp;lt;t$ and the data independent diffusion $\sigma_t$ with $\sigma_s &amp;lt; \sigma_t, s&amp;lt; t$ are known beforehand.
In essence, the drift reduces the data $x_0$ slowly to zero while we heap more and noise on the data via the diffusion term.
More importantly, by defining an analytical SDE we can ‘jump’ to any point in time and evaluate how noisy the data $x_t$ has become without having to solve SDE explicitely.
By defining a schedule for both $\alpha_t$ and $\sigma_t$ we can evaluate the ‘noisiness’ sort of point-wise in time.&lt;/p&gt;

&lt;p&gt;The remarkable reverse time stochastic differential equation then tells us that all we have to do is to cancel out the drift which turns our data into noise.
If we now take some noise and iteratively subtract the $\sigma_t^2 \partial_{x_t} \log p(x_t)$ from it we are reducing integration step by integration step (remember it’s a SDE) the noise and recover the true data.
Voila, you have generative model that turns noise into data.&lt;/p&gt;</content><summary type="html">'If I Could Turn Back Time' by Cher (1989)</summary></entry><entry><title type="html">Fokker, Planck &amp;amp; Kolmogorov</title><link href="http://localhost:4000/blog/Kramers/" rel="alternate" type="text/html" title="Fokker, Planck &amp; Kolmogorov" /><published>2021-02-04T00:00:00+01:00</published><updated>2021-02-04T00:00:00+01:00</updated><id>http://localhost:4000/blog/Kramers</id><content type="html" xml:base="http://localhost:4000/blog/Kramers/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;At the core of the partial differential equations that will describe the change of a distribution both forward and backward in time lies the Chapman-Kolmogorov equation
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t + \tau}) &amp; = \int p(x_{t + \tau} , x'_{t}) \ dx'_t \\
	&amp; = \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t
\end{align} %]]&gt;&lt;/script&gt;
which simply expands over an auxiliary variable $x’_t$ while simultaneously marginalizing it out and factorizing the joint distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau}, x'_t)&lt;/script&gt; into a conditional distribution.
The conditional distribution above states that we can start from any &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; and by moving to &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt; with the right transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x_{t + \tau} | x'_{t})&lt;/script&gt; we will obtain the correct marginal distribution &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We will assume a stochastic differential equation the first two orders of which can be estimated with
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	M^{(n)} (x'_t) = \int (x_{t+\tau} - x'_t)^n  p(x_{t + \tau} | x'_t) dx_{t+\tau}
\end{align}&lt;/script&gt;
such that the dynamics are described by the Ito drift-diffusion process
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	dX'_t = &amp; M^{(1)}(X'_t) dt + M^{(2)}(X'_t) dW_t \\
	= &amp; \mu(X'_t, t) dt + \sigma(X'_t, t) dW_t \\
\end{align} %]]&gt;&lt;/script&gt;
with the Wiener process &lt;script type=&quot;math/tex&quot;&gt;W_t&lt;/script&gt;.
The important part is to note the ‘direction’ of the differential which is evaluated strictly forward in time.
We take &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; as a sort of origin point which doesn’t change and weight the differential &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau} - x'_t&lt;/script&gt; by the appropriate transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau} | x'_t)&lt;/script&gt; over every possible &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;forward-equation&quot;&gt;Forward Equation&lt;/h3&gt;

&lt;p&gt;The Chapman-Kolmogorov equation fro the forward Kramers-Moyal expansion can be rewritten with the help of an auxilliary variable as
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t + \tau}) = &amp; \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t \\
	= &amp; \int_{X'} \int_{Y} \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} p(x'_t) dx'_t
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can then first expand the delta function $\delta ( y_{t+\tau} - x_{t+\tau} )$ with $\pm x_t$ and subsequently expand the Taylor series to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\delta(y_{t+\tau} - x_{t+\tau}) = &amp; \delta(y_{t+\tau} - x'_t + x'_t - x_{t+\tau}) = \delta(y_{t+\tau} - x'_t) \delta(x'_t - x_{t+\tau}) \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'_t}^{n} \ \delta(x'_t - x_{t+\tau})
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can plug the expanded Taylor series back in to get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t + \tau}) 
= &amp; \int_{X'} \int_{Y} \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
= &amp; \int_{X'} \int_{Y} \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \underbrace{ \partial_{x'_t}^{n} \ \delta(x'_t - x_{t+\tau})}_{!} p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) \underbrace{dx'_t}_{!} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;But now we’re in a sort of a pickle, since we’re integrating over &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; but the Dirac function &lt;script type=&quot;math/tex&quot;&gt;\delta(x'_t - x_{t+\tau})&lt;/script&gt; will serve as a sort of selector for the integral discarding anything for which the value for &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; does not correspond to the future value &lt;script type=&quot;math/tex&quot;&gt;x_{t+\tau}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Since for any subtraction &lt;script type=&quot;math/tex&quot;&gt;x'_{t} - x_{t+\tau}&lt;/script&gt; and function &lt;script type=&quot;math/tex&quot;&gt;f(x'_{t} - x_{t+\tau})&lt;/script&gt; we can employ the relation &lt;script type=&quot;math/tex&quot;&gt;\partial_{x'_t} f(x'_{t} - x_{t+\tau}) = - \partial_{x_{t}} f(x'_{t} - x_{t+\tau})&lt;/script&gt; via the change of variables, we switch the derivative to 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'_{t}}^{n} \ \delta(x'_t - x_{t+\tau}) = &amp; \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ (- \partial_{x_{t}})^{n} \ \delta(x'_t - x_{t+\tau})
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The question that remains to be answered is what motivates us to do the derivative switch in the first place.
In terms of algebra and calculus, the switch is mathematically valid, yet the holistic reason for it is still a mystery.
It turns out that due to the Chapman-Kolmogorov equation, we will integrate out the variable &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt;, so a derivative with respect to a latent variable is not of much use.
More holistically, we want to obtain the change in the probability from &lt;script type=&quot;math/tex&quot;&gt;p(x_t)&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau})&lt;/script&gt; for which the values of &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; are not of much use.&lt;/p&gt;

&lt;!-- The Taylor expansion of the Dirac impulse above is used for the entire distribution of $p(y_{t+\tau} | x_t)$ since $y$ originates, so to say, from the distribution $p(y_{t + \tau}|x'_t)$ such that we obtain
$$
\begin{align}
	p(x_{t + \tau} | x'_t) = &amp; \int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_{t})^n \ (- \partial_{x_{t+\tau}})^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t+\tau} | x'_t) dy \\
\end{align}
$$ --&gt;

&lt;p&gt;For the special case of $n=0$ where the factorial, powers and derivatives evaluate to 1 and we can marginalize out over $y$, the sum simplifies to 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	&amp;\int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ (- \partial_{x_{t}})^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t+\tau} | x'_t) dy_{t+\tau} \\
	= &amp; \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} \underbrace{\int ( y_{t+\tau} - x'_{t})^n \ p(y_{t+\tau} | x'_t) \ dy_{t+\tau}}_{M^{(n)}(x'_t)} \Big) \ \delta(x'_t - x_{t+\tau}) \\
	= &amp; \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} M^{(n)}(x'_t) \Big) \ \delta(x'_t - x_{t+\tau})
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Plugging the expanded transition probability back into the Chapman-Kolmogorov equation and noting that the Dirac impulse &lt;script type=&quot;math/tex&quot;&gt;\delta(x'_t-x_{t+\tau})&lt;/script&gt; eliminates the integral with respect to &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; by eliminating every value of &lt;script type=&quot;math/tex&quot;&gt;x'&lt;/script&gt; different from &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; irrespective of time, we get
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t + \tau}) = &amp; \int p(x_{t + \tau} | x'_{t}) \ p(x'_t) \ dx'_t \\
	= &amp; \int \int \delta(y_{t+\tau} - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) dy_{t+\tau} \ p(x'_t) \ dx'_{t} \\
	= &amp; \int \int \sum_{n=0}^\infty \frac{1}{n!} ( y_{t+\tau} - x'_t)^n \ \partial_{x'_t}^{n} \ \delta(x'_t - x_{t+\tau}) p(y_{t + \tau} | x'_{t}) \ dy_{t+\tau} \ p(x'_t) dx'_t \\
	= &amp; \int_{X'} \Big( 1 + \sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} M^{(n)}(x'_t) \Big) \ \delta(x'_t - x_{t+\tau}) \ p(x'_t) \ dx'_t \\
	= &amp; p(x_t) + \underbrace{\sum_{n=1}^\infty \frac{1}{n!} (- \partial_{x_{t}})^{n} \left[ M^{(n)}(x_t) \ p(x_t) \right]}_{\lim_{\tau \rightarrow 0} : \partial_t p(x_t) \tau}
\end{align} %]]&gt;&lt;/script&gt;
where we note that the evaluation of $n=0$ applies in the same way to any value or function that we multiply into the simplified Taylor expansion.
Pulling &lt;script type=&quot;math/tex&quot;&gt;p(x'_t)&lt;/script&gt; to the left side and finding that the change between &lt;script type=&quot;math/tex&quot;&gt;p(x_{t+\tau})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(x'_t)&lt;/script&gt; should be proportional to &lt;script type=&quot;math/tex&quot;&gt;\partial_t p(x_t) \tau&lt;/script&gt; for a small step size &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; analogously to th Euler discretization, we obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_{t+\tau}) - p(x_t) = &amp; \partial_t p(x_t) \tau \\
	\frac{p(x_{t+\tau}) - p(x_t)}{\tau} = &amp; \partial_t p(x_t).
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Finally we can note that we can could cut off the Taylor expansion after the second order and realize that Taylor expansion is equivalent to the time derivative in the limit of time, i.e. &lt;script type=&quot;math/tex&quot;&gt;\lim_{\tau \rightarrow 0}&lt;/script&gt; and we can proclaim that
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\partial_t p(x_t) = &amp; - \partial_x \left[ M^{(1)}(x_t) p(x_t) \right] + \frac{1}{2} \partial_x^2 \left[M^{(2)}(x_t) p(x_t) \right] \\
	= &amp; - \partial_{x_t} \left[ \mu(x_t) p(x_t) \right] + \frac{1}{2} \partial_{x_t}^2 \left[ \sigma^2(x_t) p(x_t) \right] \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;backward-equation&quot;&gt;Backward Equation&lt;/h3&gt;

&lt;p&gt;The Kolmogorov backward equation (KBE) can be derived in the same way while paying attention to the derivatives.&lt;/p&gt;

&lt;p&gt;Again we start with the Chapman-Kolmogorov equation:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	p(x_T | x'_t) = \int p(x_T | x''_{t+\tau}) p(x''_{t+\tau} | x'_t) dx''_{t+\tau}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We expand the transition probability &lt;script type=&quot;math/tex&quot;&gt;p(x'_t | x''_{t+\tau})&lt;/script&gt; again with a Dirac function 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	p(x''_{t+\tau} | x'_t) = \int \delta(y_{t+\tau} - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then we expand the Dirac function and expand it with the Taylor series to obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\delta(y_{t+\tau} - x''_{t+\tau}) = &amp; \delta(y_{t+\tau} - x''_{t+\tau} + x'_t - x'_t) \\
	= &amp; \delta (y_{t+\tau} - x'_t)  \delta(x'_t - x_{t + \tau}) \\
	= &amp; \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau})
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Plugging the expanded Dirac function back into the transition probability we obtain
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x''_{t+\tau} | x'_t) = &amp; \int \delta(y_{t+\tau} - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau} \\
	= &amp; \int \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;When we compare the derivation of the forward Kramers-Moyals expansion with the backwards Kramers-Moyals expansion we should immediately detect that the only difference is the partial derivative.
The subtle but important difference lies in direction to which we differentiate.
For the forward expansion we are interested how the PDF changes with respect to the future values &lt;script type=&quot;math/tex&quot;&gt;x''_{t+\tau}&lt;/script&gt; whereas for the backward expansion we want to ultimately know how the PDF changes backward in time, ergo &lt;script type=&quot;math/tex&quot;&gt;\partial_{x'_t}&lt;/script&gt; and not &lt;script type=&quot;math/tex&quot;&gt;\partial_{x''_{t+\tau}}&lt;/script&gt; since obviously &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
t &lt; t + \tau %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This is consequential for whether we include the moments &lt;script type=&quot;math/tex&quot;&gt;M^{(n)}(x'_t)&lt;/script&gt; in the differentiation or not.
Remember that the moments are defined with a fixed value at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and that they are defined as as a forward differentiation &lt;script type=&quot;math/tex&quot;&gt;y_{t+\tau} - x'_t&lt;/script&gt;.
If we now differentiate with respect to time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, the moments will not be differentiated as the value &lt;script type=&quot;math/tex&quot;&gt;x'_t&lt;/script&gt; is assumed fixed in the moments.&lt;/p&gt;

&lt;p&gt;Thus substituting the expanded transition probability back into the Chapman-Kolmogorov equation we obtain,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	p(x_T | x'_t) = &amp; \int p(x_T | x''_{t+\tau}) \ \int \sum_{n=0}^\infty \frac{1}{n!} (y_{t + \tau} - x'_t)^n \ \partial_{x'_{t}}^n \ \delta(x'_t - x''_{t+\tau}) p(y_{t+\tau} | x_{t}) dy_{t+\tau} dx''_{t+\tau} \\
	= &amp; \int p(x_T | x''_{t+\tau}) \ \sum_{n=0}^\infty \frac{1}{n!} \underbrace{\int (y_{t + \tau} - x'_t)^n p(y_{t+\tau} | x'_t) dy_{t+\tau}}_{M^{(n)}(x'_t)} \ \partial_{x'_t}^n \ \delta(x'_t - x''_{t+\tau}) dx''_{t+\tau} \\
	= &amp; p(x_T | x'_{t+\tau}) + M^{(1)}(x'_t) \partial_{x'_t} p(x_T | x'_{t+\tau}) + \frac{1}{2} M^{(2)}(x'_t) \partial_{x'_t}^2 p(x_T | x'_{t+\tau})
	% = &amp; p(x_T | x'_{t+\tau}) + \mu(x'_t) \partial_{x'_{t}} p(x_T | x'_t) + \frac{1}{2} \sigma^2(x'_t) \partial^2_{x'_{t}} p(x_T | x'_{t+\tau}).
\end{align} %]]&gt;&lt;/script&gt;
Dividing both sides by $\tau$ and evaluating in the limit of $\lim_{\tau \rightarrow 0}$, we get 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	- \partial_t p(x_T| x'_t) = &amp; \mu(x'_t) \partial_{x'_t} p(x_T | x'_t) + \frac{1}{2} \sigma^2(x'_t) \partial^2_{x'_{t}} p(x_T | x'_{t}).
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The equation above is a partial differential equation which characterizes how the probability of $x_T$ changes as we go backwards in time.
It essentially answers the question of how much the probability $p(x_T)$ changes after conditioning the process on $x’_t$ and at earlier point in time $t$.&lt;/p&gt;</content><summary type="html">Distributions as partial differential equations over time</summary></entry><entry><title type="html">Berlin Over The Years</title><link href="http://localhost:4000/blog/Berlin2020/" rel="alternate" type="text/html" title="Berlin Over The Years" /><published>2020-12-31T00:00:00+01:00</published><updated>2020-12-31T00:00:00+01:00</updated><id>http://localhost:4000/blog/Berlin2020</id><content type="html" xml:base="http://localhost:4000/blog/Berlin2020/">&lt;!-- ## Berlin Over The Years --&gt;
&lt;style&gt;
    .image-gallery {overflow: auto; margin-left: -1%!important;}
    .image-gallery li {float: left; float: top; display: block; margin: 0 0 1% 1%; width: 99%;}
    .image-gallery li a {text-align: top; text-decoration: none!important; color: #777;}
    .image-gallery li a span {display: block; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; padding: 3px 0;}
    .image-gallery li a img {width: 100%; height: 100%; display: flex; vertical-align: top;
    }
&lt;/style&gt;

&lt;ul class=&quot;image-gallery&quot;&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-111.png&quot; title=&quot;Berlin-111&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-111.png&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-111&quot; title=&quot;Berlin-111&quot; /&gt;&lt;span&gt;Berlin-111&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-111.png&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-111.png&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-120.jpg&quot; title=&quot;Berlin-120&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-120.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-120&quot; title=&quot;Berlin-120&quot; /&gt;&lt;span&gt;Berlin-120&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-120.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-120.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-160.jpg&quot; title=&quot;Berlin-160&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-160.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-160&quot; title=&quot;Berlin-160&quot; /&gt;&lt;span&gt;Berlin-160&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-160.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-160.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-30.jpg&quot; title=&quot;Berlin-30&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-30.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-30&quot; title=&quot;Berlin-30&quot; /&gt;&lt;span&gt;Berlin-30&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-30.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-30.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-70.jpg&quot; title=&quot;Berlin-70&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-70.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-70&quot; title=&quot;Berlin-70&quot; /&gt;&lt;span&gt;Berlin-70&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-70.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-70.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    &lt;!-- &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-90.jpg&quot; title=&quot;Berlin-90&quot;&gt;&lt;img src=&quot;//images.weserv.nl/?url=localhost:4000/photo_gallery/berlin2020/Berlin-90.jpg&amp;w=300&amp;h=300&amp;output=jpg&amp;q=50&amp;t=square&quot; alt=&quot;Berlin-90&quot; title=&quot;Berlin-90&quot; /&gt;&lt;span&gt;Berlin-90&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;li&gt;&lt;a href=&quot;/photo_gallery/berlin2020/Berlin-90.jpg&quot;&gt;&lt;img src=&quot;/photo_gallery/berlin2020/Berlin-90.jpg&quot; /&gt;&lt;/a&gt;&lt;/li&gt;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
&lt;/ul&gt;</content><category term="photography" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/photo_gallery/berlin2020/Berlin-70.jpg" /></entry><entry><title type="html">From Bernoulli Distributions to Poisson Point Processes</title><link href="http://localhost:4000/blog/Poisson/" rel="alternate" type="text/html" title="From Bernoulli Distributions to Poisson Point Processes" /><published>2020-11-10T00:00:00+01:00</published><updated>2020-11-10T00:00:00+01:00</updated><id>http://localhost:4000/blog/Poisson</id><content type="html" xml:base="http://localhost:4000/blog/Poisson/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;!-- Recently I got interested in jump processes as the natural extension to Wiener processes. --&gt;
&lt;!-- While Wiener process model continuous processes, jump processes are defined by instantaneous, discrete jumps.  --&gt;

&lt;p&gt;Recently I got interested in Poisson point processes which model the probabilities of phenomena or objects in some type of space.
These types of spaces can be anything from a real line to a Cartesian plane.&lt;/p&gt;

&lt;p&gt;The most direct application of point processes is queueing theory with which we can model how many packages will arrive at a certain node in a network over time.
Mathematically it looks like this:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\mathbb{P}\left[ N(t) = k \right] = \frac{( \ \lambda t \ )^k}{k!} e^{-\lambda t}
\end{align}&lt;/script&gt;
where $t$ is the time, $N(t)$ is a counting process and $\lambda$ is the intensity of the Poisson point process.
The first time I saw this probability, I frankly didn’t know what I was supposed to make of this probability so I started reading.
And the following paragraphs are the trip I took to understand the probability above.&lt;/p&gt;

&lt;h3 id=&quot;counting-process&quot;&gt;Counting Process&lt;/h3&gt;

&lt;p&gt;Let us first start with the notion of discrete jumps in stochastic processes.&lt;/p&gt;

&lt;p&gt;A counting process is a stochastic process &lt;script type=&quot;math/tex&quot;&gt;\{ N(t), t \in \mathbb{R}_+ \}&lt;/script&gt; with values that are non-negative, integer and non-decreasing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Poisson/CountingProcess.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gist of the figure above is that a counting process increases monotonically over time $t \in \mathbb{R}_+$ with integer steps of 1.
Mathematically, a counting process $N(t)$ is defined in the following way
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	N(t) &amp;= \sum_{k \geq 1} \mathbb{1}_{(T_k, \infty)} (t) \\
	\mathbb{1}_{(T_k, \infty)} (t) 
	&amp; = 
	\begin{cases}
	1, &amp; \text{if } \ t \geq T_k \\
	0, &amp; \text{if } \ 0 \leq t \leq T_k \\
	\end{cases}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In order to determine $N(t)$ for some specific value of $t$, all you have to do is to compare $t$ to each $T_k$ that you have recorded, and if $t\geq T_k$, add it to $N(t)$.
The stochastic part comes from the fact that the jump times $T_k$ are stochastic and follow some distribution.&lt;/p&gt;

&lt;h3 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h3&gt;

&lt;p&gt;A binary random variable &lt;script type=&quot;math/tex&quot;&gt;X \in \{0,1\}&lt;/script&gt; follows a Bernoulli distribution if it takes on the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.
More succinctly, we can write
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{P}\left[X=0 \right] &amp;= 1-p = q \\
	\mathbb{P}\left[X=1 \right] &amp;= p \\
	\mathbb{P}\left[X=k \right] &amp;= p^k (1-p)^{1-k} \quad \text{with} \quad k \in \{0,1\}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For a single binary experiment, the Bernoulli distribution models the probability that each outcome has.
But what if we would like to conduct multiple experiments?
Enter the Binomial distribution.&lt;/p&gt;

&lt;h3 id=&quot;binomial-distribution&quot;&gt;Binomial Distribution&lt;/h3&gt;

&lt;p&gt;Let’s say you want to conduct $n$ experiments with a binary outcome (yes/no, true/false, 1/0 etc) given some probability of success $p$ and some probability of defeat $q=1-p$.
Obviously, you can run the $n$ separate experiments and count the number of total successes from your $n$ experiments.
The expected number of successes for $n$ trials with a success probability $p$ amounts to $np$.
For example, if $n=100$ and $p=0.6$, on average we will obtain $k=60$ successful trials.&lt;/p&gt;

&lt;p&gt;But a more interesting question looms in the background.
It is certainly nice to know that if we run those $n=100$ trials a million times, we will eventually converge to $k=60$ succesfull runs per $n=100$ trials with a success probability of $p=0.6$.
But we shouldn’t forget that each time we run the $n=100$ experiments, we will obtain a different result for &lt;script type=&quot;math/tex&quot;&gt;k \in \{0, n\}&lt;/script&gt;.
So it could certainly happen that one run has exactly $k=60$ while the other run has $k=43$.&lt;/p&gt;

&lt;p&gt;In the grander scope of things, we would like to know what the probabilities are for each success rate $k$.
More specifically, we are interested in the probability of $\mathbb{P}\left[k=60 \right]$ trials versus $\mathbb{P}\left[k=43 \right]$, for example.&lt;/p&gt;

&lt;p&gt;This appears to be similar to the Bernoulli distribution except that we do $n$ trials instead of just one trial,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{P}\left[k,n, p \right] &amp;\propto p^k (1-p)^{n-k} \quad \text{with} \quad k \in \{0,n\}.
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The proportional sign was chosen on purpose to denote that this does not denote a valid distribution.
We made no assumption with regards to the ordering of the successful trials.
All we are interested in is the total number of successful trials and not the order.
Naturally, one could simply assume an ordering in which all successful trials occur subsequently and the final $n-k$ trials are the unsuccessful trials.
But since the trials are inherently stochastic, it could also occur that the first $n-k$ trials are unsuccessful and only the last $k$ trials are successful or any possible other ordering with $k$ successful trials and $n-k$ unsuccessful trials.
Since we are not interested in the ordering, we have to basically sum up all the possibilities of ordering.&lt;/p&gt;

&lt;p&gt;We can derive the number of possible orderings by thinking of how the successful trials $k$ are drawn from the number of trials $n$.
First of all we know that for $k$ successful draws, any combination of exactly $k$ successful draws is permissible.
So we could have &lt;script type=&quot;math/tex&quot;&gt;(47, 1, 13, \ldots, 16 )&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;(1, 2, 3, \ldots, 98 )&lt;/script&gt; as long as number of draws in those tuples is exactly $k$.
The first time we draw, we have 100 options, but the second time we draw we only have 99 options left and the the third time we draw we only have 98 left and so on.
In order to express this mathematically, we can conclude the following with regards to “k out of n” orderings:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\text{&quot;60 out of 100&quot;} &amp;= 100 \cdot 99 \cdot 98 \cdot \ldots \cdot 42 \cdot 41\\
	&amp;= \frac{100 \cdot 99 \cdot 98 \cdot \ldots \cdot 2 \cdot 1 }{40 \cdot 49 \cdot \ldots \cdot 2 \cdot 1} \\
	&amp;= \frac{100!}{(100-60)!}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So in general we can conclude that if we want to draw $k$ ordered, successful trials out of $n$ total trials we have
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\text{&quot;k out of n&quot;} &amp;= \frac{n!}{(n-k)!}.
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;But there is a catch.
So far we have only considered cases of unique orderings, such as &lt;script type=&quot;math/tex&quot;&gt;(47, 1, 13, 16)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(1, 47, 13, 16 )&lt;/script&gt; which have the same elements just in a different order.
But we are only interested whether the trials are successful, not how the successful trials are ordered.
The notion of interest is whether the draws occur in the set such as &lt;script type=&quot;math/tex&quot;&gt;\{(47, 1, 13, 16), (1, 47, 13, 16 ) \} \subset \{1, 13, 16, 47 \}&lt;/script&gt; (which I ordered for convenience’s sake) and not their order.
Fortunately, there is a quick fix to this: Simply divide by the number of possible permutations of the set $k!$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\binom{n}{k} = \underbrace{\frac{1}{k!}}_{\text{unordered correction}} \underbrace{\frac{n!}{(n-k)!}}_{\text{# of unique orderings}}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The binomial coefficient is the normalization constant which corrects the original probability $p^k (1-p)^{n-k}$ by the number of possible permutations.&lt;/p&gt;

&lt;p&gt;Thus we obtain the binomial distribution which states
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{P}\left[k,n, p \right] &amp;= \binom{n}{k} p^k (1-p)^{n-k} \\
	&amp;= \frac{n!}{k! (n-k)!} p^k (1-p)^{n-k}
\end{align} %]]&gt;&lt;/script&gt;
which tells us what the probability of a certain number of successful trials $k$ is, if we run $n$ trials with a success rate of $p$.&lt;/p&gt;

&lt;h3 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h3&gt;

&lt;p&gt;The binomial distribution defines the probability of $k$ successful trials for a fixed set of trials $n$ for a success probability $p$.
For example, we can observe a logistics network and can ask every second whether a package has arrived or not.
Remember that the binomial distribution is only defined for binary outcomes.
We can then model how many packages arrive per minute or even hour by multiplying the number of seconds per minute or hour by the probability $p$ of a package arriving.&lt;/p&gt;

&lt;p&gt;That time partition is still discrete, though, and the real world is continuous.
We can let $\lim n \rightarrow \infty$ to model an infinitely fine partition of time.
But now we’re confronted with the inconvenient &lt;script type=&quot;math/tex&quot;&gt;\lim_{n \rightarrow \infty} np = \infty&lt;/script&gt; which we can’t really work with.
instead we can redefine the probability &lt;script type=&quot;math/tex&quot;&gt;p = \frac{\lambda}{n}&lt;/script&gt; with an intensity $\lambda$ which will be invariant to time, so to speak, because &lt;script type=&quot;math/tex&quot;&gt;\lim_{n \rightarrow \infty} np = \lim_{n \rightarrow \infty} n \frac{\lambda}{n} = \lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can then substitute $p = \frac{\lambda}{n}$ into the binomial distribution and see where that takes us:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\lim_{n \rightarrow \infty} \mathbb{P} \left[ n, k, p \right]
&amp;= \lim_{n \rightarrow \infty} \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} \\
&amp;= \lim_{n \rightarrow \infty} \frac{n!}{k!(n-k)!} \left( \frac{\lambda}{n} \right)^k \left( 1-\frac{\lambda}{n} \right)^{n-k} \\
&amp;= \lim_{n \rightarrow \infty} \frac{n!}{k!(n-k)!} \left( \frac{\lambda}{n} \right)^k \left(1-\frac{\lambda}{n} \right)^{n-k} \\
&amp;= \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \frac{n!}{k!(n-k)!} \frac{1}{n^k} \left(1-\frac{\lambda}{n} \right)^{n-k} \\
&amp;= \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \frac{n!}{(n-k)!} \frac{1}{n^k} \left(1-\frac{\lambda}{n} \right)^n \left(1-\frac{\lambda}{n} \right)^{-k} \\
&amp;= \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \frac{n \cdot (n-1) \cdot (n-1) \cdot \ldots \cdot (n-k)}{n^k} \left(1-\frac{\lambda}{n} \right)^n \left(1-\frac{\lambda}{n} \right)^{-k} \\
&amp;= \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \underbrace{\frac{n}{n} \cdot \frac{(n-1)}{n} \cdot \frac{(n-1)}{n} \cdot \ldots \cdot \frac{(n-k)}{n}}_{1 \text{ for } \lim_{n \rightarrow \infty}}  \underbrace{\left(1+\frac{1}{\frac{n}{-\lambda}} \right)^{-\lambda \frac{n}{-\lambda}}}_{=e^{-\lambda}} \underbrace{\left(1+\frac{\lambda}{n} \right)^{-k}}_{1 \text{ for } \lim_{n \rightarrow \infty}} \\
&amp;= \frac{\lambda^k}{k!} e^{-\lambda} \\
&amp;= \text{Poisson} [ \lambda]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The Poisson distribution is a discrete distribution with support $k \in \mathbb{N}_+$ that quantifies the probability of $k$ events happening in an interval of time or space with $\lambda$ denoting the intensity of events (like how often they occur).&lt;/p&gt;

&lt;h3 id=&quot;poisson-point-process&quot;&gt;Poisson Point Process&lt;/h3&gt;

&lt;p&gt;The Poisson distribution above allows us to compute the probability of $k$ events for a given fixed interval of time or space.
We can generalize the Poisson distribution to a Poisson point process over varying intervals by integrating over the (possibly changing) intensities that the space exhibits.
For a Poisson point process defined over a time interval $(t_0, t_1]$, we could compute the number of expected events by integrating the intensity of $\lambda(t)$ which is a function of time $t$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\mathbb{P}[N(t_0, t_1]=k, \lambda(s)] = \frac{\left( \int_{t_0}^{t_1} \lambda(s) ds \right)^k}{k!} e^{-\int_{t_0}^{t_1} \lambda(s) ds}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can recover the original Poisson distribution through a homogeneous Poisson point process which has a constant $\lambda$ and for which the, in our case time, integral evaluates to $\int_{t_0}^{t_1} ds = 1$.
In fact, a meaningful intensity $\lambda$ for a Poisson distribution can only be determined for a-priori defined, fixed interval of time or space.
The Poisson point process is then the natural extension to varying intervals of time or space by defining a varying intensity $\lambda(s)$.
A homogeneous Poisson point process for a fixed interval $\int_{t_0}^{t_1} = 1$ can be obtained through
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{P}[N(t_0, t_1]=k, \lambda] &amp;= \frac{\left( \int_{t_0}^{t_1} \lambda ds \right)^k}{k!} e^{-\int_{t_0}^{t_1} \lambda ds} \\
&amp;= \frac{\left( \lambda \int_{t_0}^{t_1} ds \right)^k}{k!} e^{-\lambda \int_{t_0}^{t_1} ds} \\
&amp;= \frac{\left( \lambda \right)^k}{k!} e^{-\lambda}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The definition above unfortunately only tells us the number of events occuring over the entire interval $(t_0, t_1]$.
What if we wanted to now the probability of an event happening at a singular moment $t$?
We should remember that a Poisson point process can be interpreted as a counting process $N(t)$ since it starts at zero, is monotonically increasing and the probability of an event happening is randomly distributed with $\int_{t_0}^{t_1} \lambda(s) ds$ which simplifies to just $\lambda t$ for a constant intensity and $t_0 = 0$, $t_1 = t$.
Our derivation showed us that if we expand the Bernoulli distribution to a Binomial distribution with an infinite number of trials we arrive at the Poisson distribution which lies at the heart of a Poisson point process.
Nevertheless the counting process always increases by a single integer step.
Thus we can ask the question what the probability is of an event at an arbitrary point in the interval.&lt;/p&gt;

&lt;p&gt;We can therefore inquire about the probability of an event not happening, ergo $N(t_0, t_1]=0$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{P}[N(t_0, t_1]=0, \lambda(s)] &amp;= \frac{\left( \int_{t_0}^{t_1} \lambda(s) ds \right)^0}{0!} e^{-\int_{t_0}^{t_1} \lambda(s) ds} \\
&amp;= e^{-\int_{t_0}^{t_1} \lambda(s) ds}
\end{align} %]]&gt;&lt;/script&gt;
which is the probability that no event will occur until $t_1$.
As the intensity is a strictly positive quantity, the integral $\int_{t_0}^{t_1} \lambda(s) ds$ increases monotonically over time until an event occurs.
Naturally since the integral is monotonically increasing, the probability of no event is monotonically decreasing through the negated exponential.
This is quite intuitive as it states that the probability of no event happening decreases monotonically as time progresses.
On the flip side, this means that the probability of an event occuring is monotonically increasing as time progresses:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{P}[N(t_0, t_1]=1, \lambda(s)] &amp;= 1 - e^{-\int_{t_0}^{t_1} \lambda(s) ds}.
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The reason why we analyze only a single event $k=1$ lies with the fact that the events are i.i.d. distributed which in case for the Poisson distribution means that the integral is reset after each event.
This is the reason why we only integrate to $t_1$.
Since $\mathbb{P}[N(t_0, t_1]=1, \lambda(s)]$ is a cumulative probability distribution as it approaches 1 in infinity, we call simply derive it with respect to $t_1$ to obtain the probability density function:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{P}[N(t_1)=1, \lambda(s)] &amp;= \frac{d}{dt_1} \left[ 1 - e^{-\int_{t_0}^{t_1} \lambda(s) ds} \right] \\
&amp;= \lambda(t_1) e^{-\int_{t_0}^{t_1} \lambda(s) ds}.
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Another fun fact of the Poisson process is that its derivative is in fact a Bernoulli distribution.
To see this we inspect the Poisson process for an extremely small time frame $\lim_{\Delta t \rightarrow 0} [t, t+ \Delta t]$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\lim_{\Delta t \rightarrow 0} \mathbb{P}[N(t, t + \Delta t]=k, \lambda(s)] = \lim_{\Delta t \rightarrow 0} \frac{\left( \int_{t}^{t + \Delta t} \lambda(s) ds \right)^k}{k!} e^{-\int_{t}^{t + \Delta t} \lambda(s) ds}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which for &lt;script type=&quot;math/tex&quot;&gt;k=\{0, 1\}&lt;/script&gt; and a first order exponential series expansion &lt;script type=&quot;math/tex&quot;&gt;e^x = \sum_{k=0}^\infty \frac{x^k}{k!}&lt;/script&gt; yields
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\lim_{\Delta t \rightarrow 0} \mathbb{P}[N(t, t + \Delta t]=0, \lambda(s)] &amp;= \lim_{\Delta t \rightarrow 0} \frac{\left( \int_{t}^{t + \Delta t} \lambda(s) ds \right)^0}{0!} e^{-\int_{t}^{t + \Delta t} \lambda(s) ds} \\
&amp; \approx e^{- \lambda(t)\Delta t} \\
&amp; \approx 1 - \lambda(t) \Delta t \\
\lim_{\Delta t \rightarrow 0} \mathbb{P}[N(t, t + \Delta t]=1, \lambda(s)] &amp; = \lim_{\Delta t \rightarrow 0} \frac{\left( \int_{t}^{t + \Delta t} \lambda(s) ds \right)^k}{k!} e^{-\int_{t}^{t + \Delta t} \lambda(s) ds} \\
&amp; \approx \lambda(t) \Delta t e^{- \lambda(t)\Delta t} \\
&amp; \approx \lambda \Delta t - \underbrace{\lambda(t)^2 \Delta t^2}_{=0, \Delta t \rightarrow 0} \\
&amp;= \lambda \Delta t
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;which are precisely the probabilities for a Bernoulli distribution.&lt;/p&gt;</content><summary type="html">From binary to fishy if you're francophil</summary></entry><entry><title type="html">(Importance Weighted) Variational Autoencoders Derived</title><link href="http://localhost:4000/blog/VAE/" rel="alternate" type="text/html" title="(Importance Weighted) Variational Autoencoders Derived" /><published>2020-08-10T00:00:00+02:00</published><updated>2020-08-10T00:00:00+02:00</updated><id>http://localhost:4000/blog/VAE</id><content type="html" xml:base="http://localhost:4000/blog/VAE/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h3 id=&quot;variational-autoencoders&quot;&gt;Variational Autoencoders&lt;/h3&gt;

&lt;p&gt;Let $x$ denote a random variable which is generated by a random process.
This random process first samples a random latent variable $z$ and subsequently generates $x|z \sim p(x|z)$ by conditioning the random process $p(x|z)$ on the random variable $z$.
Thus we are dealing with a generative model which can generate valid samples $x$ from some random $z$.
We are interested in the latent distribution of $p(z |x)$ given the data $x$ and wish to learn it.
Intuitively, we want to know for a given $x$ what the latent variables $z$ were that generated them.
This is akin to observing some observation $x$ and being able to say: I know the $z$ ‘s that generated that!.&lt;/p&gt;

&lt;p&gt;By Bayes rule we now that for a distribution $ p ( x | z ) $ there also exists the distribution $ p ( z | x ) $, the distribution we are interested in.
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}
\end{align}&lt;/script&gt;
The crux of the problem is that we can only observe the data distribution $p(x)$ through a data set $\mathcal{D}= \{ x_i \}_{i=0}^N$.
So we neither know what form the data generating process $p(x|z)$ has nor what the true latent distribution $p(z)$ is.
Additionally, the data probability $p(x)$ is even more obscure.
How would you even answer the question of how probable your data set is?&lt;/p&gt;

&lt;p&gt;What we do know is the following: We want to find a variational distribution, let’s name it $q_\phi(z |x)$ with the optimizable parameters $\phi$, which we want to be as close as possible to the true distribution $p(z |x)$.
The motivation behind this formulation is that the true latent conditional distribution $p(z|x)$ could be very complicated, but we will choose a simpler variational distribution $q_\phi(z |x)$ that we can conveniently work with.
It might not be able to represent all the modes and fat tails that could potentially occur in $p(z |x)$ but better than nothing, right?&lt;/p&gt;

&lt;p&gt;Information theory gives us the right tools to measure the difference between $q_\phi(z|x)$ and $p(z|x)$ through the Kullback-Leibler divergence:
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{KL} \left[ q_\phi(z|x) \ || \ p(z|x) \right]&lt;/script&gt;
The state of affairs sofar is that we have an easy to work with distribution $q_\phi(z|x)$ with the trainable parameters $\phi$ and that we wish to minimize the divergence to the true latent distribution $p(z|x)$.
We can also rewrite $p(z|x)$ according to Bayes rule to maybe make the computations a bit more tractable.
We can now write out the Kullback-Leibler divergence and inspect the terms that arise from some algebraic manipulation:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{KL} \left[ q_\phi(z \| x) \ || \ p(z \| x) \right] &amp;= \mathbb{E}_{q_\phi(z \|x)} \left[ \log \frac{q_\phi(z \| x)}{p(z \| x)} \right] \\
	&amp;= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z|x)\right] \\
	&amp;= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log \frac{p(z,x)}{p(x)}\right] \\
	&amp;= \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z,x) + \log p(x)\right]
\end{align} %]]&gt;&lt;/script&gt;
From earlier we know, that the data marginal probability $p(x)$ is almost surely intractable so we might want to avoid working with it directly.
But by applying Bayes’ rule we suddenly see that we are working with the joint probability $p(z,x)$.
Given the fact that $0 \leq \mathbb{KL}$ we can deduce
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	0 &amp;\leq \mathbb{KL} \left[ q_\phi(z|x) \ || \ p(z|x) \right] \\
	0 &amp;\leq \mathbb{E}_{q_\phi(z|x)} \left[ \log q_\phi(z|x) - \log p(z,x) + \log p(x) \right] \\
	0 &amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ -\log q_\phi(z|x) + \log p(z,x) - \log p(x) \right] \\
	\log p(x) &amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(z,x)\right] \\
	\log p(x) &amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(x|z) p(z)\right] \\
	\log p(x) &amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ - \log q_\phi(z|x) + \log p(x|z) + \log p(z)\right] \\
	\log p(x) &amp;\geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(z)}{q_\phi(z|x)} + \log p(x|z) \right] \\
	\log p(x) &amp;\geq -\mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{q_\phi(z|x)}{p(z)} \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p(x | z) \right] \\
	\log p(x) &amp;\geq -\mathbb{KL} \left[ q_\phi(z|x) || p(z) \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p(x|z) \right]
\end{align} %]]&gt;&lt;/script&gt;
What does the inequality above tell us?
It says that if we want to maximize the probability of the data we must minimize the KL divergence in the first term and maximize the probability of the generative model $p(x|z)$.
So for any given $z$, we want the generative model $p(x|z)$.
If we optimize the two terms on the right, we will obtain an inference model $q_\phi(z|x)$ which inverts the generative model $p(x|z)$.&lt;/p&gt;

&lt;p&gt;The problem, though, is that we have no clue what either $p(z)$ nor the true generative model $p(x|z)$ actually is.
Here comes the fun part: Let’s just assume stuff and parameterize both $p(z)$ and $p(x|z)$ such that we can easily and conveniently work with them.
Since $p(z)$ is a latent distribution we will enforce a strong simplicity by assuming that it follows a standard normal distribution $\mathcal{N}(0, I)$.
We could assume any other family of distributions but the standard normal distribution has lots of nice perks and properties.
This might seem bold but if the generative model $p(x|z)$ is flexible enough it can generate any $x$ from this comparatively simpel $z$.
Now let’s turn our attention to $p(x|z)$: We will change the unknown $p(x|z)$ to a parameterized and differentiable $p_\theta(x|z)$ such that we can maximize the probability of the data $x$ for a given $z$.&lt;/p&gt;

&lt;p&gt;Now we have the following objective function:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	\log p(x) \geq -\mathbb{KL} \left[ q_\phi(z|x) || p(z) \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z) \right]
\end{align}&lt;/script&gt;
which, upon closer inspection, has a lot of similarities to an autoencoder, except that it’s probabilistic!
We use the distribution $q_\phi(z|x)$ to infer some latent code from a given sample.
Through the KL divergence we enforce that the latent representation should be close to the simplified assumption of $p(z) = \mathcal{N}(0,I)$.
The same latent code $z$ should be reconstructed to the true sample $x$ by the generative model $p_\theta(x|z)$.
So we can actually interpret $q_\phi(z|x)$ as a probabilistic encoder and $p_\theta(x|z)$ as a decoder.&lt;/p&gt;

&lt;p&gt;It is important to note that the prior and data loglikelihood are not balanced with respect to the data set size as it is done in Bayesian neural network and the parameter prior.
The KL divergence between the latent code $q_\phi(z|x)$ and the prior $p_\theta(z)$ is computed for each data point independently and is equally balanced.&lt;/p&gt;

&lt;h3 id=&quot;jensens-inequality&quot;&gt;Jensen’s Inequality&lt;/h3&gt;

&lt;p&gt;The core idea of variational autoencoders and, to a larger extent, variational inference is that we optimize the $ \mathbb{E} [ \log p(x|z) ]- \mathbb{KL} [ q(\theta) || (p(\theta) ] $ in the hopes that we are able to push the bound as close as possible to the true value $\log p(x)$.
The ELBO can thus be understood as a surrogate criterion.&lt;/p&gt;

&lt;p&gt;The derivation of the ELBO originates from the minimization of the Kullback-Leibler divergence.
It turns out that we can also derive an alternative criterion in order to maximize $\log p(x)$, the term we ultimately want to see maximized.
In order to derive this alternative bound we will first have to understand Jensen’s inequality.&lt;/p&gt;

&lt;p&gt;Let’s assume that we have the quadratic function $f(x) = x^2, x,y \in \mathbb{R}$.
The quadratic function is convex which is essential for Jensen’s inequality.
For any convex function $f(x)$, Jensen’s inequality for $t \in (0,1)$ states:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	f(tx_1 + (1-t) x_2 ) \ \leq \ t \ f(x_1) + (1-t) \ f(x_2)
\end{align}&lt;/script&gt;
which is the definition of a convex function and quintessentially asks the question of whether we interpolate the function values $f(x)$ or interpolate the arguments $x_1$ and $x_2$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/VAE/JensensInequalityConvex.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;20%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interpolating the function values $f(x)$ results in a straight line from $f(x_1)$ to $f(x_2)$.
Interpolating the arguments results in tracing the original function $f(x)$.
Since the quadratic function in its vanilla form is convex, we can conclude that interpolating the arguments will always be below the interpolation of the function values.&lt;/p&gt;

&lt;p&gt;Jensen’s inequality becomes useful for deriving bounds when applied to probability theory.
We can expand the interpolation beyond the two values $x_1$ and $x_2$ by weighting the values uniformly:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	f\left( \frac{1}{2} x_1 + \frac{1}{2} x_2 \right) \ \leq \ \frac{1}{2} \ f\left( x_1 \right) + \frac{1}{2} \ f \left( x_2 \right)
\end{align}&lt;/script&gt;
We can then extend the interpolation to $x_n \in \{x_1, \ldots, x_N \}$ via
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	f\left( \frac{1}{N} x_1 + \ldots + \frac{1}{N} x_N \right) \ 
	&amp; \leq \ 
	\frac{1}{N} \ f\left( x_1 \right) + \ldots + \frac{1}{N} \ f \left( x_N \right) \\
	f\left( \frac{1}{N} \sum_n^N x_n \right) \ 
	&amp; \leq \ 
	\frac{1}{N} \sum_n^N \ f\left( x_n \right) \\
\end{align} %]]&gt;&lt;/script&gt;
which gives us the probabilistic version of Jensen’s inequality for a convex function $f(x)$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	f \left( \mathbb{E}[x_n] \right) \ 
	&amp; \leq \ 
	\mathbb{E} \left[ f(x_n) \right] \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For a concave function, we simply have to flip the inequality sign, as the function value interpolation $\mathbb{E}[f(x)]$ will always be equal or below the argument interpolation $f(\mathbb{E})[x]$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	f \left( \mathbb{E}[x_n] \right) \ 
	&amp; \geq \ 
	\mathbb{E} \left[ f(x_n) \right] \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/VAE/JensensInequalityConcave.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;20%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;importance-weighted-variational-autoencoders&quot;&gt;Importance Weighted Variational Autoencoders&lt;/h3&gt;

&lt;p&gt;To derive the loss for the Importance Weighted Variational Autoencoder we will use two tricks: the fact that the logarithm is a concave function and utilizing importance sampling.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\log p(x) 
&amp;= \log \left[ \int p(x| z) p(z) dz \right] \\
&amp;= \log \left[ \int q(z|x) \frac{p(x|z) p(z)}{q(z|x)} dz \right] \qquad \Leftarrow \text{expanding with $\frac{q(z|x)}{q(z|x)}$} \\
&amp;= \log \left[ \mathbb{E}_{q(z|x)} \left[ \frac{p(x|z) p(z)}{q(z|x)} \right] \right] \\
&amp;\geq \mathbb{E}_{q(z|x)} \left[ \log \left[ \frac{p(x|z) p(z)}{q(z|x)} \right] \right]
\qquad \Leftarrow \text{pulling in $\log$ with Jensen's inequality = ELBO}
\end{align} %]]&gt;&lt;/script&gt;
and we arrive at the original ELBO derived from the Kullback-Leibler divergence from the original VAE formulation.&lt;/p&gt;

&lt;p&gt;The important step is applying Jensen’s inequality where we introduce the bound for the first time.
If we leave the expectation in the log we are still working with marginal data log-likelihood instead of the bound.
Only after applying Jensen’s inequality do we loosen the equality to an inequality which is the ELBO.&lt;/p&gt;

&lt;p&gt;The remedy to fending off the loose ELBO is given by evaluating the term before applying Jensen’s inequality by pulling in the logarithm.
This gives us an importance sampling algorithm which repeatedly samples $z \sim q(z|x)$ and subsequently evaluating $p(x|z)$.
This tightens the bound and we have better criterion.&lt;/p&gt;

&lt;h3 id=&quot;squeezing-jensen&quot;&gt;Squeezing Jensen&lt;/h3&gt;

&lt;p&gt;We stated earlier, that if we apply Jensen’s inequality and sample $z$ once, we have the original training procedure of the VAE.
But the latent representation is a distribution so why should we only sample once?
Probably because we’re lazy and stochastic optimization theory guarantees an unbiased gradient, so if we train long enough we will arrive at the optimum.
But there is no real argument against sampling multiple times from the latent distribution and averaging the reconstruction $p(x|z)$ to integrate out the randomness that is injected into the optimization through the latent distribution $q(z|x)$.&lt;/p&gt;

&lt;p&gt;The more samples we draw, the tighter our bound becomes.
Why is this the case we might ask?
We take our toy example with the quadratic function again to visualize what’s happening when we draw more samples from the latent distribution
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	f\left( \mathbb{E}[x] \right) &amp;\leq f \left( \frac{1}{N} \sum_n x_n \right) \\
	f\left( \mu \right) &amp;\leq f \left( \frac{1}{N} \sum_n^N x_n \right)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/VAE/TighteningJensens.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;20%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we evaluate a finite number of samples $x_n$, the mean estimator will have a higher variance.
There might be occurences where we draw the perfect sample right or very, very close to $\mu$, but on average the mean estimator will have the estimator variance
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
	\mathbb{V} \left[ \frac{1}{N} \sum_n^N x_n \right]
	&amp;= \frac{1}{N^2} \sum_n^N \mathbb{V}[x] \\
	&amp;= \frac{1}{N} \mathbb{V}[x] \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;with means that 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
	f\left( \mu \right) \leq f \left( \frac{1}{N+1} \sum_n^{N+1} x_n \right) \leq f \left( \frac{1}{N} \sum_n^N x_n \right)
\end{align}&lt;/script&gt;
for a convex function $f(\cdot)$.
Conversely for a concave function, we would have to flip the inequality sign.&lt;/p&gt;

&lt;p&gt;By simply drawing more samples $N$ we reduce the variance and thus tighten the bound of the estimator.
Visually, sampling more samples $x_n$ in the figure above asymptotically lets the estimated mean converge on the true mean.
The closer the estimated mean is, the tighter the bound.
This is precisely whats happening when we draw more samples in the expectation of the importance weighted VAE.&lt;/p&gt;

&lt;p&gt;Finally, for we can use the probabilistic version of Jensen’s inequality to derive the positive constraint of the variance of a random variable $x$:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{E}[x]^2 &amp;\leq \mathbb{E}[x^2] \\
0 &amp;\leq \mathbb{E}[x^2] - \mathbb{E}[x]^2 = \mathbb{V}[x]
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;</content><summary type="html">How to get to the objective function of VAEs ...</summary></entry></feed>
