<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-03-14T18:56:37+01:00</updated><id>/</id><title type="html">Ludwig Winkler</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">Hamiltonian Mechanics in Monte Carlo Samplers</title><link href="/blog/CurseOfDimensionality/" rel="alternate" type="text/html" title="Hamiltonian Mechanics in Monte Carlo Samplers" /><published>2019-03-14T00:00:00+01:00</published><updated>2019-03-14T00:00:00+01:00</updated><id>/blog/CurseOfDimensionality</id><content type="html" xml:base="/blog/CurseOfDimensionality/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;Here’s a common problem: You want to estimate some function $f$.
The function $f$ takes some input $x$ and outputs some value $y$ thus giving you $y=f(x)$.
The important part is that you don’t have access to the global shape of $f$.&lt;/p&gt;

&lt;p&gt;This could be due to $f(x)$ being an absolute black-box model such that you can only interact with it by handing it some value $x$ and receiving some value $y=f(x)$.
Something that is practically very similar is that the analytical form of $f(x)$ is very, very complicated but can be evaluated in a pointwise manner.
This means that we can’t evaluate the global shape of $f(x)$ but only the single points for some specific value of $x$.&lt;/p&gt;

&lt;p&gt;The lack of knowledge of the global shape can be encountered in different scenarios which all boil down to computing a complicated integral.
In order to offer some intuition let’s have a look at the univariate normal distribution $p(x|\mu, \sigma)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	p(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\end{align*}&lt;/script&gt;

&lt;p&gt;The univariate normal distribution is a true and tested distribution in machine learning with many convenient mathematical properties.
One property is the normalization of the exponential term $\smash{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}$ with the scaling term $1/\sqrt{2\pi\sigma^2}$ such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	\int_{- \infty }^{\infty} p(x | \mu, \sigma) dx = 1.
\end{align*}&lt;/script&gt;

&lt;p&gt;Conversely we know that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\int_{-\infty}^{\infty} p(x|\mu, \sigma) dx
	&amp;= \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \\
	&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \underbrace{\int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx}_{=\sqrt{2\pi\sigma^2}} \\
	&amp;= 1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Obviously we now the exact form of the scaling parameter through the standard deviation $\sigma$ … but what if we didn’t?&lt;/p&gt;

&lt;p&gt;In that case we would encounter the distribution as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	p(x|\mu, \sigma) &amp;= \frac{1}{Z} f(x|\mu, \sigma)\\
	&amp;= \frac{1}{Z} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
	\text{with} \ Z  = \int_{-\infty}^{\infty} f(x| \mu, \sigma) dx &amp;= \int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx = \sqrt{2\pi\sigma^2}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the case above we could evaluate the distribution $p(x | \mu, \sigma)$ correctly up to the scaling parameter $1/Z$ by evaluating $f(x | \mu, \sigma)$.
The value of $Z$ is given by the shape of the distribution $p(x | \mu, \sigma)$ which we can’t determine globally but only point-wise for some value $x$.&lt;/p&gt;

&lt;p&gt;The example above is supposed to illustrate the more general problem of estimating probability distributions with intractable partition functions $Z$ while still being intuitive by working with a well known distribution such as the univariate normal distribution.&lt;/p&gt;

&lt;p&gt;The most naive idea for computing $Z$ is to evaluate random values of $x$ and compute an approximate integral.
More precisely we would sample $N$ different $x$ from a uniform distribution $U(x_{min}, x_{max})$ with equal probability.
This set $\{ x_i \}_{i=0}^N$ serves as an approximation of the true function which becomes better and better, the more samples $x_i$ we obtain.
If we were to choose $N=\infty$ then the sample set would replicate the true shape perfectly.
This is practically impossible due to computational and time constraints.&lt;/p&gt;

&lt;p&gt;Once we have a set which we think contains enough samples, we could approximate the integral by simply summing over our set&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	Z = \int_{-\infty}^{\infty} f(x|\mu, \sigma) dx
	=\int_{-\infty}^{\infty} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \approx \sum_{i=0}^N \frac{1}{\Delta x} e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
\end{align*}&lt;/script&gt;

&lt;p&gt;The $\Delta x$ is required since we approximating the function $f( x | \mu, \sigma)$ with little columns where the exponential term tells us the height and $\Delta x$ the width of the column (look at the animations at Wikipedias entry for Riemann integral).&lt;/p&gt;

&lt;p&gt;This would get the job done as our approximation would asymptotically approach the true value of $Z$ for large enough sample sizes.
Yet, upon closer examination we can see that the univariate normal distribution has a specific shape which concentrates a lot of information (or more precisely probability) in the area around $\mu$.&lt;/p&gt;

&lt;p&gt;Remember that a the bulk of the integral $\smash{\int_{-\infty}^{\infty} f(x|\mu, \sigma) dx}$ is evaluated around $\mu$.
If we chose $x_{min}$ and $x_{max}$ at the very extreme ends of the number line, our $N$ samples would be very dispersed and our approximation of $Z$ would be very poor.&lt;/p&gt;

&lt;p&gt;Let’s examine a concrete problem.
We choose a standard normal distribution $\mathcal{N}(0,1)$ and sample $200$ values of $x$ from the uniform distribution $\mathcal{U}(-10,10)$.
Furthermore we consider all probabilities of $\mathcal{N}(0,1)$ interesting which have a probability above $p(x|\mu, \sigma) \geq 0.01$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print()
print('###########################################')
print('Uniform Sampler')
print('###########################################')

import numpy
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import torch
import math

FloatTensor = torch.FloatTensor

class Gaussian:

	def __init__(self, _mu, _sigma):
		self.mu = _mu
		self.std = _sigma
		self.dist = torch.distributions.Normal(self.mu, self.std)

	def eval(self, _x):
		return self.dist.log_prob(_x).exp()

	def unnorm_eval(self, _x):
		return torch.exp(-1/(2*self.std**2)*(_x - self.mu)**2)
std = 1
print('True Z:', np.sqrt(2*np.pi*std**2))
P = Gaussian(FloatTensor([0]), FloatTensor([std]))
U = torch.distributions.Uniform(0,1)
num_samples=1000

res = 100
x_min=-10
x_max=10
points = torch.from_numpy(np.linspace(x_min,x_max,res))
probs = FloatTensor([P.eval(point) for point in points])

plt.figure(figsize=(10,10))
plt.plot(points.numpy(), probs.numpy())
sample_points = torch.distributions.Uniform(x_min, x_max).sample((num_samples,))
sample_probs = FloatTensor([P.unnorm_eval(point) for point in sample_points])

# Compute Z
Z = sample_probs.sum()/(num_samples/(x_max - x_min))
sample_probs /= Z
print('Estimated Z:', Z.numpy())

print('1D Uniform Sampler: P(x)&amp;gt;=0.01: ', sample_probs[sample_probs&amp;gt;=0.01].shape[0]/sample_points.shape[0]*100, '%')

plt.scatter(sample_points.numpy(), sample_probs.numpy())
plt.show()

###########################################
Uniform Sampler
###########################################
True Z: 2.5066282746310002
Estimated Z: 2.5824146
1D Uniform Sampler: P(x)&amp;gt;=0.01:  26.1 %
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As it turns out $ \approx 74\%$ of the values $x \in [-10,10]$ are below $0.01$.
That means that by sampling randomly from a uniform distribution $\mathcal{U}(-10,10)$ we would evaluate $72\%$ of our randomly selected values $x$ with a probability below $0.01$ thus effectively wasting a large number of samples.
Conversely only $28\%$ of the samples would lie in probability regions around the mean $\mu$ with a probability higher than $0.01$.
The evaluation of the area $\mu$ would therefore be highly inefficient as we waste a considerable amount of samples in areas which do not contribute to the integral of $Z$.&lt;/p&gt;

&lt;p&gt;Obviously we could restrict the uniform distribution $\mathcal{U}(x_{min}, x_{max})$ to better values but we could only do this in this easy toy problem and not on more complex distributions.
Similarly, the area with probabilities below 0.01 decreases if we increase the variance of the Normal distribution.
Again, this is just a toy example to visualize the problems when sampling from high-dimensional spaces.
The main take way point is that one has to pay close attention to what the sampler is doing and how sampling algorithms are designed to make efficient use of computational resources.
I’ll get into the geometric details of high dimensional spaces later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Can we do any better? … In fact we can with the help of two physicists from the Manhattan Project and a chain.&lt;/p&gt;

&lt;p&gt;In the example above we want to sample as often as possible in the areas which greatly contribute to the integral $Z = \int_{-\infty}^{\infty} f(x|\mu, \sigma) dx$.
So we could say that once we happen to sample close to $\mu$ we would like to sample as often as possible in the same area, effectively staying in the area around $\mu$.&lt;/p&gt;

&lt;p&gt;The achievement of Mr Metropolis and Mr Hastings (is the Netflix-Founder related to him?) was to elaborate precisely that idea and introduce the concept of a sampling chain to our problem.&lt;/p&gt;

&lt;p&gt;The sampling chain is a sequence of samples ${x_0, x_1, \ldots, x_N}$ which is constructed iteratively by sampling a proposal point $x_t’$ and accepting it as the next sample $x_{t+1}$ based on an acceptance condition.
The proposal sample $x’_t$ can be obviously be rejected and in that case a new proposal sample $x’_t$ has to be proposed.
The important part is how the proposed samples in the chain are generated, accepted and rejected.&lt;/p&gt;

&lt;p&gt;In order to stay in areas which contribute to the value of Z, we can propose a sample $x’_t$ which is close to our current sample $x_t$ and evaluate $f(x’_t|\mu, \sigma)$.
We can then compare the new evaluated sample $f(x’_t|\mu, \sigma)$ and the current evaluated sample $f(x_t|\mu, \sigma)$ and accept the proposed sample $x’_t$ as the next sample if $f(x’_t|\mu, \sigma) \geq f(x|\mu, \sigma)$.&lt;/p&gt;

&lt;p&gt;Before long the straight-forward acceptance rule $f(x’_t|\mu, \sigma) \geq f(x_t|\mu, \sigma)$ would take us straight to $x = \mu$.
The problem is that we would be stuck in $x = \mu$ as every value $x \neq \mu$ has a lower evaluation $f(x\neq\mu |\mu, \sigma)$ than $f(x = \mu|\mu, \sigma)$.
You can verify that by simply looking at a plot of a normal distribution.&lt;/p&gt;

&lt;p&gt;In order to stay in the high value area around $\mu$ but also not get stuck in it, a proposal distribution $q(x’|x)$ is evaluated, the ratio $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma)$ is used which is then compared to an auxiliary sample: $U \sim \mathcal{U}(0,1)$.
This is done in the following way:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_MHAlgo.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The acceptance ratio $\alpha$ is computed in a smart way, namely that $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \in (0,\infty)$.
If $f(x’_t|\mu, \sigma)$ is larger than $f(x_t|\mu, \sigma)$, then $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \geq 1$ and the proposal sample will always be accepted.
If on the other hand $f(x’_t|\mu, \sigma) \leq f(x_t|\mu, \sigma)$, then $f(x’_t|\mu, \sigma)/f(x_t|\mu, \sigma) \in (0,1)$ there is a chance that we accept the proposed sample $x’_t$ anyway if $u &amp;gt; \alpha$.
By sampling $u \sim \mathcal{U}(0,1)$, occasional steps into areas with lower values $f(x)$ are allowed.&lt;/p&gt;

&lt;p&gt;The final component is the proposal distribution $q(x’|x)$ which is chosen in its simplest form as a standard normal distribution around $x$, namely $x’ \sim \mathcal{N}(x, 1)$.
The chain $\{ x_t \}_ { i=0 }^N$ that is constructed during sampling exhibits the Markov property such that each state $x_t$ is only dependent on the previous state $x_{t-1}$ through the proposal distribution $q(x’|x)$ (and after the proposal was accepted).
The Markov chain property and the Monte Carlo sampling process together define the broader class of sampling algorithms called ‘Markov Chain Monte Carlo’ (MCMC).&lt;/p&gt;

&lt;p&gt;An important criterion while constructing such a Markovian chain of samples is that the sampler can move through the state space in an unbiased way.
The acceptance probability and the proposal distribution should therefore guarantee that the sampler can reach every state if we sample long enough.
For the Metropolis-Hastings algorithm it is required that the proposal density is symmetric such that $q(x’|x) = q(x|x’)$.
This can realized easily with a Normal distribution with constant standard deviation.
Mathematically, it is required that the Markov chain is reversible which simply states that there should be equal probability when being in state $x$ and moving to $x’$ and reverse:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	p(x',x) &amp;= p(x, x') \\
	p(x' | x) \ p(x) &amp;= p(x | x) \ p(x').
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now decompose the transition probability $p(x’|x)$ into the proposal distribution $q(x’|x)$ and the acceptance probability $\alpha(x’|x)$.
\begin{align&lt;em&gt;}
	\alpha(x’|x) q(x’|x) p(x) = \alpha(x|x’) q(x|x’) p(x’)
\end{align&lt;/em&gt;}
The equation above simply states that the probability of being in state $x$, proposing to go to state $x’$ and finally accepting to go the state $x’$ should be the same as doing the three steps in reverse.
This guarantees that no state $x$ is favored in any particular way and that the final Markov chain asymptotically approaches the true value of $Z$.&lt;/p&gt;

&lt;p&gt;We can now plug in the acceptance probability of the Metropolis-Hastings sampler and pull the transition and stationary probabilities into the minimum operator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\min\left(1, \frac{p(x')}{p(x)} \right) q(x'|x) p(x) &amp;= \min \left(1, \frac{p(x)}{p(x')} \right) q(x|x') p(x') \\
	\min\left( q(x'|x) p(x), \frac{p(x') q(x'|x) p(x)}{p(x)} \right) &amp;= \min \left(q(x|x') p(x'), \frac{p(x)q(x|x') p(x')}{p(x')} \right) \\
	\min\big( q(x'|x) p(x), p(x') \underbrace{q(x'|x)}_{=q(x|x')} \big) &amp;= \min \big(q(x|x') p(x'), p(x) \underbrace{q(x|x')}_{=q(x'|x)} \big) \\
	% \min\big( q(x'|x) p(x), p(x') \underbrace{q(x'|x)}_{=q(x|x')} \big) &amp;= \min \big(q(x|x') p(x'), p(x) \underbrace{q(x|x')}_{=q(x'|x)} \big)\\
	\min\big( q(x'|x) p(x), p(x') q(x|x') \big) &amp;= \min \big( q(x|x') p(x'), p(x) q(x'|x) \big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above equation is valid and states that the ‘detailed balance’ property holds for the acceptance ratio of the Metropolis-Hastings sampler.
Thus in statistical lingo, we have an unbiased estimator of the true $Z$ which will asymptotically approach the true of value of $Z$!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	print()
	print('###########################################')
	print('Metropolis-Hastings')
	print('###########################################')


	import numpy
	import numpy as np
	import scipy as sp
	import matplotlib.pyplot as plt
	import torch
	import math

	FloatTensor = torch.FloatTensor

	class Gaussian:

		def __init__(self, _mu, _sigma):
			self.mu = _mu
			self.std = _sigma
			self.dist = torch.distributions.Normal(self.mu, self.std)

		def eval(self, _x):
			return self.dist.log_prob(_x).exp()

		def unnorm_eval(self, _x):
			return torch.exp(-1/(2*self.std**2)*(_x - self.mu)**2)

	std = 1
	print('True Z:', np.sqrt(2*np.pi*std**2))
	P = Gaussian(FloatTensor([0]), FloatTensor([std]))
	Q = torch.distributions.Normal(FloatTensor([0]), FloatTensor([1]))
	U = torch.distributions.Uniform(0,1)
	num_samples=100

	res = 200
	x_min=-10
	x_max=10

	#Deterministic evaluation for plotting purposes
	points = torch.from_numpy(np.linspace(x_min,x_max,res))
	probs = FloatTensor([P.eval(point) for point in points])
	plt.figure(figsize=(10,10))
	plt.plot(points.numpy(), probs.numpy())

	# Initialize starting point of chain randomly in U(-10,10)
	x_init = torch.distributions.Uniform(x_min, x_max).sample((1,))
	x = x_init

	accepted_x = []
	accepted_y = []
	rejected_x = []

	num_samples=1000
	burn_in = 200 # Burn_in to make up for random initialization
	for i in range(num_samples+burn_in):

		x_ = Q.sample()
		a = P.unnorm_eval(x+x_)/P.unnorm_eval(x)
		if a &amp;gt;= U.sample():
			if i &amp;gt;= burn_in:
				accepted_x.append(x+x_)
				accepted_y.append(P.unnorm_eval(x+x_))
			x = x+ x_
		else:
			if i &amp;gt;= burn_in: rejected_x.append(x+x_)

	accepted_x = torch.stack(accepted_x, dim=0)
	accepted_y = torch.stack(accepted_y, dim=0)
	rejected_x = torch.stack(rejected_x, dim=0)
	print('Accepted/Rejected: ', accepted_x.shape[0], '/', rejected_x.shape[0])

	#Computing Z
	num_bins = 200
	bins = np.linspace(x_min, x_max, num_bins)
	index = np.digitize(accepted_x, bins)


	bin_counts = np.array([np.sum([index == i]) for i in range(1, len(bins))])
	bin_y_sums = np.array([accepted_y.numpy()[index == i].sum() for i in range(1, len(bins))])

	tmp = bin_y_sums/bin_counts
	tmp[np.isnan(tmp)]=0

	print('Estimated Z:', sum(tmp)/(num_bins/(x_max-x_min)))

	accepted_probs = FloatTensor([P.eval(point) for point in accepted_x])
	print('1D MH Sampler: P(x)&amp;gt;=0.01: ', accepted_probs[accepted_probs&amp;gt;=0.01].shape[0]/num_samples*100, '%')

	plt.scatter(accepted_x.numpy(), accepted_probs.numpy())
	plt.show()

	###########################################
	Metropolis-Hastings
	###########################################
	True Z: 2.5066282746310002
	Accepted/Rejected:  701 / 299
	Estimated Z: 2.4467572424682826
	1D MH Sampler: P(x)&amp;gt;=0.01:  69.69999999999999 %
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The uniform sampled used $26 \%$ of its samples in useful areas whereas the Metropolis-Hastings sampler uses almost $70 \%$!
We can visualize the two samplers next to each other to visualize the increased efficiency of the Metropolis-Hastings Sampler.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/Uniform_SamplingDist.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/blog/HMC/MH_SamplingDist.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can even improve our sampler even further by treating the surface of the distribution as a physical model.
In order to illustrate we should first take the $-\log$ of the unnormalized function $f(x|\mu, \sigma)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_02.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By working with $-\log f(x | \mu, \sigma)$ we get rid of the pesky exponential term, effectively working with a linear function in case of the Normal distribution.
Additionally, we have transformed the function $f(x|\mu, \sigma)$ into a nice representation where the bottom of the curve $-\log f(x|\mu, \sigma)$ represents the areas in $f(x|\mu, \sigma)/Z$ with the highest probabilities.&lt;/p&gt;

&lt;p&gt;Remember that the Metropolis-Hastings sampler was developed by two physicists at the Manhattan Project to tackle the problem in statistical mechanics of estimating the partition function $Z$.
In statistical mechanics, the function $f(x)$ is the energy function $E(x)$ of a system in state $x$ and the probability of the system being in state $x$ is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
	p(x) = \frac{1}{Z} e^{-E[x]}
\end{align*}&lt;/script&gt;

&lt;p&gt;The probability of a system being in state $x$ increases if the energy of that state $x$ is small.
This corresponds to the basic physical behavior, that physical systems want to be in a minimal energy state.
If a piece of metal is hot, it emanates heat and cools down and decreases its energy.
If a ball has high potential energy, it will decrease this energy by dropping down.
If a coil is being compressed, it will expand once the force relents to enter into a low energy state.&lt;/p&gt;

&lt;p&gt;We will now introduce a second idea from physics: Hamiltonian Mechanics.
Let’s say you have physical system $\mathcal{H}(x, p, t)$ in which two properties describes everything you know.
These two properties are the position $x(t)$ and the momentum $p(t)$.
For some position $x(0)$ and the momentum $p(0)$, the physical system $\mathcal{H}(x, p, t)$ will be able to predict every future position $x(t)$ and momentum $p(t)$.&lt;/p&gt;

&lt;p&gt;This is possible due to the following Hamiltonian mechanics:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\frac{d p(t)}{d t} &amp;= -\frac{\partial \mathcal{H}(x, p, t)}{\partial x(t)}\\
	\frac{d x(t)}{d t} &amp;= \frac{\partial \mathcal{H}(x, p, t)}{\partial p(t)}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Hamiltonian $\mathcal{H}(x, p, t)$ corresponds in our simple case to the energy of the system.
The two differential equations above describes how the energy of the system is allocated to either the momentum $p(t)$ or the position $x(t)$ as time progresses.&lt;/p&gt;

&lt;p&gt;The energy of the system $\mathcal{H}(x, p, t)$ consists of the potential energy $E_p(x(t))$ and the kinetic energy $E_k(p(t)) = p(t)^2/2m$ for a particle of mass $m$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
	\mathcal{H}(x, p, t) = E_p(x(t)) + E_k(p(t)) = E_p(x(t)) + \frac{p(t)^2}{2m}
\end{align*}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can apply the Hamiltonian mechanics from above to our function $-\log f(x | \mu ,\sigma)$ and simulate the trajectory of a particle with $m=1$.
For that to happen we first derive both $dp(t)/dt$ and $dx(t)/dt$ for the potential energy provided by $ E_p(x(t)) = - \log f(x(t) | \mu, \sigma)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\frac{d p(t)}{d t} &amp;= -\frac{\partial \mathcal{H}(x, p, t)}{\partial x(t)} \\
	&amp;= \frac{\partial }{\partial x(t)} \log f(x(t) | \mu, \sigma) \\
	&amp;= \frac{1}{f(x(t) | \mu, \sigma)} \frac{\partial}{\partial x(t)} f(x(t) | \mu, \sigma) \\
	\frac{d x(t)}{d t} &amp;= \frac{\partial \mathcal{H}(x, p, t)}{\partial p(t)} \\
	&amp;= \frac{\partial }{ \partial p(t)} \frac{p(t)^2}{2} \\
	&amp;= p(t)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now use the derivations above to simulate the trajectory of a particle on the surface of $-\log f(x(t) | \mu, \sigma)$ applying the following update steps to the position $x(t)$ and momentum $p(t)$ of the particle:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	x(t+\epsilon) &amp;= x(t) + \epsilon \frac{d p(t)}{d t} \\
	&amp;= x(t) + \epsilon p(t) \\
	p(t+ \epsilon) &amp;= p(t) - \epsilon \frac{d p(t)}{dt} \\
	&amp;= p(t) - \epsilon \frac{\partial }{\partial x(t)} \log f(x(t)|\mu, \sigma)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;These two update rules have very intuitive explanations once we have look at the shape of $-\log f(x(t) | \mu, \sigma)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_03.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The particle starts at $x(t_0)$ with a high potential energy and some initial momentum $p(t_0)$.
The gradient of $-\log f(x(t_0)|\mu, \sigma)$ is very steep and thus a lot of extra momentum is added to the particles kinetic energy.
The direction of steepest descent $-\partial \log f(x(t)|\mu, \sigma)/dx(t)$ points in the negative direction on the x-axis and thus extra momentum is added due to the minus sign.
The potential energy  $-\log f(x(t)|\mu, \sigma)$ decreases as the particle moves to the lower area.
Once the particle moved past $x(t)=0$ the trend reverses and the gradient $\partial \log f(x(t)|\mu, \sigma)/dx(t)$ is positive, thus decreasing the momentum.&lt;/p&gt;

&lt;p&gt;While the one dimensional case above is nice to gain an intuition, this whole simulation looks significantly cooler in two dimensions.
I plotted the $- \log f(x(t)|\mu, \sigma)$ surface with the resulting gradients of the surface as arrows.
We can see nicely how the particle runs up the slope at the edge of the low energy basins.
After all its momentum has been converted into potential energy, it makes a u-turn and heads back down the slope.
(That’s actually something we want to prevent which lead to the development of the No-U-Turn-Sampler (NUTS)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_2D01.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The total energy of the particle is conserved between the potential and the momentum.
We show this by initializing the particle with a very high potential energy and a high momentum and watch it slide over the surface of the distribution in wide arcs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_HighEnergy.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively we could initialize the particle with little potential energy and little momentum.
Since the total energy of the particle is low, it will remain in the low energy (=high probability) basin and sample amply from there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_LowEnergy.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The total energy initialization of the particle at the start of each trajectory can be drawn from a distribution.
The distribution can easily be incorporated into the detailed balance equation and marginalized out such that the sampler is unbiased.&lt;/p&gt;

&lt;p&gt;Here are two links to fully implemented Hamiltonian Monte Carlo Samplers with crazy awesome animations:
&lt;a href=&quot;http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/&quot;&gt;here&lt;/a&gt;
and
&lt;a href=&quot;http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A closing remark about the geometry of high-dimensional spaces:&lt;/p&gt;

&lt;p&gt;While sampling methods are asymptotically correct in their estimation of the posterior distribution, they do not scale well due to the curse of dimensionality.
In a nutshell it refers to the geometric properties of high dimensional spaces.
For example the distance between two points increases as we move into higher and higher dimensional spaces.
A quick example is the Euclidean distance between two points which have a distance of 1 in every of their shared dimensions $\mathbb{R}^N$.
Depending on the dimensionality $N$ we get a distance $\sqrt{\sum_{n=0}^N 1^2}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\mathbb{R}^1 &amp;: \quad \sqrt{1^2}=1 \\
	\mathbb{R}^2 &amp;: \quad \sqrt{1^2+1^2}=1.41 \ldots \\
	\mathbb{R}^3 &amp;: \quad \sqrt{1^2+1^2 +1^2}=1.73 \ldots \\
	\vdots	&amp;
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_Distance.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The more dimensions we add, the larger the space between two points with equal distance in every dimension.
Secondly this creates the problem of requiring an increasing number of samples for higher and higher dimensional spaces.
Let’s say we want to estimate a function by sampling in the Euclidean space spanned between two points with unit distance, ergo distance of 1.
We want to estimate the function and want a resolution of 0.1, i.e. we need 10 samples per dimension.
In $\mathbb{R}^1$ we only require 10 samples to estimate the pdf wit equally spaced sampling points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_04.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But in $\mathbb{R}^2$ we suddenly require 100 samples to estimate the function with the desired resolution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_05.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In $\mathbb{R}^3$ we finally need 1000 samples to estimate the function to our liking.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/HMC/HMC_06.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The required number of samples grows exponentially with the number of dimension, i.e. $10^N$ in $\mathbb{R}^N$ for our specific setup of 10 samples per dimension.
By just adding two dimensions we need $100 \times$ more samples than in one dimension.
That might not sound like a lot, but it quickly accumulates when working in high-dimensional spaces.
This effectively restricts sampling algorithms to applications where we require very precise posteriors as in finance or medicine or where the run-time isn’t a problem.&lt;/p&gt;</content><summary type="html">Sampling with the help of physics</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/blog/HMC/HMC_2D01.png" /></entry><entry><title type="html">Bayesian Non-Negative Matrix Factorization</title><link href="/blog/BayesNMF/" rel="alternate" type="text/html" title="Bayesian Non-Negative Matrix Factorization" /><published>2019-02-18T00:00:00+01:00</published><updated>2019-02-18T00:00:00+01:00</updated><id>/blog/BayesNMF</id><content type="html" xml:base="/blog/BayesNMF/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;non-negative-matrix-factorization&quot;&gt;Non-Negative Matrix Factorization&lt;/h2&gt;

&lt;p&gt;In deterministic NMF each entry in the parameters is a fixed weight or point estimate.
But in Bayesian NMF each entry in the parameters is a distribution which will be the trusty Normal distribution $\mathcal{N}(\mu, \sigma)$.
Now each entry in the matrices $P, Q$, vectors $u, i$ and scalar $b$ will be a distribution, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{uk} \sim \mathcal{N}(\mu_{uk}, \sigma_{uk}) \\
  Q_{ki} \sim \mathcal{N}(\mu_{ki}, \sigma_{ki}) \\
  u_u \sim \mathcal{N}(\mu_{u}, \sigma_{u}) \\
  i_i \sim \mathcal{N}(\mu_{i}, \sigma_{i}) \\
  b \sim \mathcal{N}(\mu_{b}, \sigma_{b}) \\&lt;/script&gt;

&lt;p&gt;The aim is now to find the optimal $\mu’s$ and $\sigma’s$ for all the parameters in $P, Q, u, i$ and $b$.
In the previous post we optimized the mean squared error between our predicted matrix $\hat{R}$ and the real matrix $R$ with gradient descent.
As it turns out we can train the distributions in the same way by applying a trick which is one of the many advantages of the Normal distribution: the reparameterization trick.
I explained the reparameterization trick in detail in another blog post and will simply refer to it &lt;a href=&quot;https://ludwigwinkler.github.io/blog/Reparam/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to allow PyTorch (shout out to the devs!) to backpropagate to the variational parameters $\mu, sigma$ we have to reparameterize the every entry in the parameters.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  P_{uk} &amp;= \mu_{uk} + \mathcal{E} \cdot \sigma_{uk} \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  Q_{ki} &amp;= \mu_{ki} + \mathcal{E} \cdot \sigma_{ki} \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  u_u &amp;= \mu_u + \mathcal{E} \cdot \sigma_u \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  i_i &amp;= \mu_i + \mathcal{E} \cdot \sigma_i \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  b &amp;= \mu_b + \mathcal{E} \cdot \sigma_b \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can rewrite this in matrix notation to write it more compactly with the element-wise multiplication operator $\odot$ and combining the $\mu’s$ and $\sigma’s$ into matrices, i.e. $P_\mu$ and $P_\sigma$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  P &amp;= P_\mu + \mathcal{E} \odot P_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  Q &amp;= Q_\mu + \mathcal{E} \odot Q_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  u &amp;= u_\mu + \mathcal{E} \odot u_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  i &amp;= i_\mu + \mathcal{E} \odot i_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
  b &amp;= b_\mu + \mathcal{E} \odot b_\sigma \quad ; \mathcal{E} \sim \mathcal{N}(0,1) \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can now sample predictions by first sampling the above matrices and vector and then combining them into:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \hat{R} &amp;= P \ Q + u^T + i + b
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\hat{R} \in \mathbb{R}^{U \times I}, P \in \mathbb{R}^{U \times K}, Q \in \mathbb{R}^{K \times I}, u \in \mathbb{R}^{U}, i \in \mathbb{R}^I$ and $b \in \mathbb{R}$.
Special attention for adding the vector $u \in \mathbb{R}^U$ column-wise to the matrix product $PQ$, the vector $i \in \mathbb{R}^I$ row-wise to $PQ$ and b simply to every entry in $PQ$.
The vector $u$ is a combination of scalar biases for every user and is added to every of her/his recommendations in the columns of $\hat{R}$.
The entries in the vector $i$ on the other hand are biases for the items and are added to each row in $\hat{R}$.&lt;/p&gt;

&lt;p&gt;We can sample multiple prediction matrices $\hat{R}$ by first sampling the corresponding parameters and then combining the into a prediction.
From these predictions we can estimate the mean and standard deviation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \mu_{\hat{R}} &amp;= \mathbb{E}[ \ \hat{R} \ \ ] \approx \frac{1}{N} \sum_{i=0}^N \hat{R}_i \\\\
  \sigma_{\hat{R}} &amp;= \sqrt{\mathbb{V}[ \ \hat{R} \ \ ]} \approx \sqrt{ \frac{1}{N} \sum_{i=0}^N ( \hat{R}_i - \mu_{\hat{R}})^2} \\\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Once we have the mean and standard deviation of the prediction we can construct a cost function through the negative log-likelihood of the real prediction matrix $R$ under the estimated distribution $\hat{R}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(\hat{R}, R) = - \sum_{u,i}^{U,I} \delta_{ui} \left( \log \frac{1}{\sqrt{2\pi \sigma_{\hat{R}_{ui}}^2}} - \frac{1}{2 \sigma_{\hat{R}_{ui}}^2} (R_{ui} - \mu_{\hat{R}_{ui}})^2 \right)
\end{align*}&lt;/script&gt;

&lt;p&gt;where $\delta_{ui}$ is a delta function which is 1 if the entry $R_{ui}$ is in the training data and 0 if the entry $R_{ui}$ is to be predicted.
$\delta_{ui}$ serves as a filter to remove the non-existent data points from the cost function.
Depending on the dimensionality and whether or not we’re estimating covariances as well (which we are not in this case), $N=10$ or $N=20$ is enough to stabilize the cost function.
An important coding tweak is to predict the samples of $\hat{R}$ in parallel by creating a three-dimensional tensor $\mathcal{E}$ with a third ‘sample’ dimension.
That third dimension will be used to compute the $\mu_{\hat{R}}$ and $\sigma_{\hat{R}}$ efficiently without having to resort to for-loops which can be very slow in Python.&lt;/p&gt;

&lt;p&gt;After computing the cost function we can let PyTorch do its automatic differentiation magic and we optimize the whole thing with an optimizer such as Adam.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    import numpy as np
    import torch
    import torch.nn.functional as F

    FloatTensor = torch.FloatTensor

    np.set_printoptions(precision=4)

    class BayesNMF(torch.nn.Module):

            def __init__(self, R, K, lr, iterations):

                    &quot;&quot;&quot;
                    Perform matrix factorization to predict empty
                    entries in a matrix.

                    Arguments
                    - R (ndarray)   : user-item rating matrix
                    - K (int)       : number of latent dimensions
                    - alpha (float) : learning rate
                    - beta (float)  : regularization parameter
                    &quot;&quot;&quot;
                    super().__init__()

                    self.R = R #The recommendation matrix
                    self.data_mask = torch.zeros_like(R) #Mask with 1's where we have data points
                    self.data_mask[R!=0] = 1
                    self.missing_mask = torch.zeros_like(self.R)
                    self.missing_mask[R==0]=1 #Mask with 1's where we don't have data points
                    self.num_users, self.num_items = R.shape #Number of users and items
                    self.K = K #Number of latent dimensions
                    self.lr = lr #Learning rate for the optimizer
                    self.iterations = iterations #Number of gradient descent steps

                    exponential_dist_lambda = 5 #Parameter for exponential distribution used to initialize parameters with small positive values
                    init_std = 0

                    #Parameters with mu and untransformed variances: \sigma = log(1+exp(rho))
                    self.P_mu = torch.nn.Parameter(FloatTensor(self.num_users, self.K).exponential_(exponential_dist_lambda))
                    self.P_rho = torch.nn.Parameter(FloatTensor(self.num_users, self.K).fill_(init_std))
                    self.Q_mu = torch.nn.Parameter(FloatTensor(self.num_items, self.K).exponential_(exponential_dist_lambda))
                    self.Q_rho = torch.nn.Parameter(FloatTensor(self.num_items, self.K).fill_(init_std))

                    self.b_u_mu = torch.nn.Parameter(FloatTensor(self.num_users, 1).exponential_(exponential_dist_lambda))
                    self.b_u_rho = torch.nn.Parameter(FloatTensor(self.num_users, 1).fill_(init_std))
                    self.b_i_mu = torch.nn.Parameter(FloatTensor(self.num_items, 1).exponential_(exponential_dist_lambda))
                    self.b_i_rho = torch.nn.Parameter(FloatTensor(self.num_items, 1).fill_(init_std))
                    self.b_mu = torch.nn.Parameter(FloatTensor([0.1]))
                    self.b_rho = torch.nn.Parameter(FloatTensor([init_std]))

                    #Optimizer for gradient descent
                    self.optim = torch.optim.Adam(self.parameters(), lr=self.lr)

                    #Data for stochastic gradient descent updates instead of full matrix updates
                    self.samples = [(i, j, self.R[i,j]) for i in range(self.num_users) for j in range(self.num_items) if R[i,j]&amp;gt;0]
                    np.random.shuffle(self.samples)
                    self.forward_type = 1

                    #Number of prediction samples computed in parallel
                    self.num_MC_samples = 20

            def forward2(self, _u, _i):

                    #Sample multiple parameters with the reparameterization trick in parallel
                    b = self.b_mu + torch.randn(self.num_MC_samples, *self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu[_i] + torch.randn(self.num_MC_samples, *self.b_i_rho[_i].shape)* F.softplus(self.b_i_rho[_i])
                    b_u = self.b_u_mu[_u] + torch.randn(self.num_MC_samples, *self.b_u_rho[_u].shape)* F.softplus(self.b_u_rho[_u])
                    P = self.P_mu[_u] + torch.randn(self.num_MC_samples, *self.P_rho[_u].shape)*F.softplus(self.P_rho[_u])
                    Q = self.Q_mu[_i] + torch.randn(self.num_MC_samples, *self.Q_rho[_i].shape)*F.softplus(self.Q_rho[_i])

                    prediction = torch.bmm(P.view(self.num_MC_samples, 1, self.K), Q.view(self.num_MC_samples, self.K, 1)).squeeze(-1)
                    prediction += b_u
                    prediction += b_i
                    prediction += b

                    return prediction

            def forward1(self):
                    '''
                    Sample multipile parameters in parallel
                    :return: Prediction matrix of shape [N_MC, U, I]
                    '''


                    b = self.b_mu + torch.randn(self.num_MC_samples, *self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu + torch.randn(self.num_MC_samples, *self.b_i_rho.shape)* F.softplus(self.b_i_rho)
                    b_u = self.b_u_mu + torch.randn(self.num_MC_samples, *self.b_u_rho.shape)* F.softplus(self.b_u_rho)
                    P = self.P_mu + torch.randn(self.num_MC_samples, *self.P_rho.shape)*F.softplus(self.P_rho)
                    Q = self.Q_mu + torch.randn(self.num_MC_samples, *self.Q_rho.shape)*F.softplus(self.Q_rho)

                    pred = torch.bmm(P, Q.transpose_(1,2))
                    pred += b_u
                    pred += b_i.transpose_(1,2)
                    pred += b.unsqueeze(-1)
                    pred *= self.data_mask

                    # Add a little of noise in order to prevent nan's during backpropagation; NaN's are caused by masking and the resulting std=0 for missing values
                    pred += 1e-10*torch.randn(*pred.shape)*self.missing_mask

                    return pred

            def criterion(self, _pred, _label):
                    '''

                    :param _pred: prediction matrix of shape [N_MC, U, I]
                    :param _label: _label matrix of shape [U, I]
                    :return: scalar loss for gradient descent
                    '''
                    mu = _pred.mean(dim=0)
                    sigma = _pred.std(dim=0)

                    loss = (-torch.sum(torch.log(torch.sqrt(1/(2*np.pi*sigma.pow(2))))-1./(2*sigma**2)*(_label-mu)**2))
                    mse_loss = F.mse_loss(_pred, _label) # MSE loss for interpretable loss

                    return loss, mse_loss

            def train_params(self):
                    # Perform stochastic gradient descent for number of iterations
                    for e in range(self.iterations):
                            loss_ = 0
                            for (i, j, r) in self.samples:
                                    self.zero_grad()

                                    pred = self.forward1()
                                    loss, mse_loss = self.criterion(pred, self.R)

                                    #Alternative prediction by sampling a single data point instead of the entire prediction matrix
                                    # pred= self.forward2(i, j)
                                    # loss, mse_loss = self.criterion(pred, r)

                                    loss.backward()
                                    self.optim.step()
                                    loss_+=loss.detach().numpy()

                            if e%1000==0 and e&amp;gt;0:
                                    mf.print_variational_matrix()

                            if (e+1) % 100 == 0:
                                    print(&quot;Iteration: %d ; loss = %.4f ; mse_loss=%.4f&quot; % (e+1, loss_, mse_loss.detach().cpu().numpy().squeeze()))

            def full_matrix(self):
                    &quot;&quot;&quot;
                    Computer a single full matrix using the resultant biases, P and Q
                    &quot;&quot;&quot;
                    b = self.b_mu + torch.randn(*self.b_rho.shape)* F.softplus(self.b_rho)
                    b_i = self.b_i_mu + torch.randn(*self.b_i_rho.shape)* F.softplus(self.b_i_rho)
                    b_u = self.b_u_mu + torch.randn(*self.b_u_rho.shape)* F.softplus(self.b_u_rho)
                    P = self.P_mu + torch.randn(*self.P_rho.shape)* F.softplus(self.P_rho)
                    Q = self.Q_mu + torch.randn(*self.Q_rho.shape)* F.softplus(self.Q_rho)

                    return_ = torch.matmul(P, Q.t())
                    return_ += b_u
                    return_ += b_i.t()
                    return_ += b

                    return return_.detach()

            def print_variational_matrix(self):
                    '''
                    Print the mean and variance of the prediction matrix
                    '''
                    #Sample 100 different prediction matrices
                    Rs = torch.stack([mf.full_matrix() for _ in range(100)])

                    print('Mean:')
                    print(torch.mean(Rs, dim=0).detach().numpy())
                    print('Std')
                    print(np.around(torch.std(Rs, dim=0).detach().numpy(),3))



    R = FloatTensor([       [5, 3, 5, 0],
                            [4, 5, 5, 5],
                            [3, 0, 0, 1],
                            [0, 0, 0, 0],
                            [0, 1, 0, 4],])

    mf = BayesNMF(R, K=3, lr=0.001, iterations=10000)

    mf.train_params()
    mf.print_variational_matrix()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The recommendation matrix $R$ in the code snippet above has a tweak: the fourth row does not contain any entries so every item has to be recommended to user 4.
Upon inspection of the mean $\mu_{\hat{R}}$ and $\sigma_{\hat{R}}$ we can that the variance in the fourth row in markedly increased:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_{\sigma} =
 \left(
\begin{array}{cccc}
 0.000 &amp; 0.001 &amp; 0.000 &amp; 0.000 \\
 0.000 &amp; 0.000 &amp; 0.000 &amp; 0.000 \\
0.918 &amp; 1.358 &amp; 0.833 &amp; 0.974 \\
0.001 &amp; 0.001 &amp; 0.000 &amp; 0.000 \\
 \end{array}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;Because the user hasn’t reviewed anything, we cannot leverage any latent structure through the Bayesian Non-Negative Matrix Factorization.&lt;/p&gt;

&lt;p&gt;Similarly, when predicting the matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
R =
\left(
\begin{array}{cccc}
5&amp; 3&amp; 5&amp; 0 \\
                        4&amp; 5&amp; 5&amp; 0\\
                        3&amp; 0&amp; 0&amp; 0\\
                        0&amp; 0&amp; 0&amp; 1\\
                        0&amp; 1&amp; 0&amp; 0\\
\end{array} \right) %]]&gt;&lt;/script&gt;

&lt;p&gt;we will obtain a standard deviation matrix $P_{\sigma}$ which has an increased standard deviation for the fourth row and the last column:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P_{\sigma} =
\left(
\begin{array}{cccc}
0  &amp;  0  &amp;  0 &amp;   0.003 \\
0  &amp;  0  &amp;  0 &amp;   0.005 \\
0  &amp;  0.  &amp;  0 &amp;   0.001 \\
0.003 &amp; 0.006 &amp; 0.002 &amp;   0.001    \\
0  &amp;  0  &amp;  0 &amp;   0.001 \\
\end{array} \right) %]]&gt;&lt;/script&gt;

&lt;p&gt;Since user 4 does not share any common likes with any other user the standard deviation is increased for all other movies in the fourth row.
Similarly, movie 4 which user 4 reviewed has an increased standard deviation for the other users as not a single other user has seen it.&lt;/p&gt;

&lt;p&gt;As a closing remark I just want to say that the gradient descent based variational inference approach is not the best approach.
Shinichi Nakajima, a member of my lab published an &lt;a href=&quot;http://www.jmlr.org/papers/volume14/nakajima13a/nakajima13a.pdf&quot;&gt;analytic solution&lt;/a&gt; which can be somewhat involved.
The core motivation of this post was to see whether we could use the reparameterization trick and whether the factorization would exhibit the behaviour which was expected of it with regards to independent entries in $R$.&lt;/p&gt;</content><summary type="html">Being Unsure About What To Recommend</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="" /></entry><entry><title type="html">The Reparameterization Trick</title><link href="/blog/Reparam/" rel="alternate" type="text/html" title="The Reparameterization Trick" /><published>2019-02-02T00:00:00+01:00</published><updated>2019-02-02T00:00:00+01:00</updated><id>/blog/Reparam</id><content type="html" xml:base="/blog/Reparam/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;!-- ## Non-Negative Matrix Factorization --&gt;

&lt;p&gt;Let’s assume we have normal standard distribution $\mathcal{E} \sim \mathcal{N}(0,1)$.
We can draw lots of samples from it which will be distributed just like the parameterized distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N01_0.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unsuprisingly, the distribution of the samples follows the parameterized Normal distribution $\mathcal{N}(\mu=0, \sigma=1)$.
If we were to sample millions of samples from the distribution and use a ever finer resolution of the histogram we would arrive at a perfect $\mathcal{N}(\mu=0, \sigma=1)$ distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N01_1.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s pick three samples $\epsilon_1, \epsilon_2, \epsilon_3$ from that standard normal distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_1 = -0.5 \\
  \epsilon_2=0.5 \\
  \epsilon_3=1&lt;/script&gt;

&lt;p&gt;What would happen if we were to multiply these three samples with a constant number $\sigma=3$?
Well since it is a linear transformation it should be a straight forward multiplication&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma \cdot \epsilon_1 = -1.5 \\
  \sigma \cdot \epsilon_2 = 1.5 \\
  \sigma \cdot \epsilon_3 = 3&lt;/script&gt;

&lt;p&gt;Let’s visualize this multiplication with a fixed number for thousands of samples $\epsilon$ from the standard normal distribution.
In the next image I normalized the samples such that we’re working with a distribution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N03_0.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, multiplying the samples of a $\mathcal{N}(0,1)$ distribution with a constant number $\sigma$ results in a Normal distribution with the standard deviation $\sigma$!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N03_1.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second step of the reparameterization step is to ask ourselves what would happen if we added a constant number $\mu =10$ to the samples drawn from $\mathcal{N}(0,1)$ which are already multiplied with $\sigma$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu + \sigma \cdot \epsilon_1 = 8.5 \\
  \mu + \sigma \cdot \epsilon_2 = 11.5 \\
  \mu + \sigma \cdot \epsilon_3 = 13&lt;/script&gt;

&lt;p&gt;Let’s visualize this linear transformation on all the samples we drew from $\mathcal{N}(0,1)$ and multiplied with $\sigma=3$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N103_0.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, if we plot the Normal distribution $\mathcal{N}(10,3)$ on top of our scaled and shifted standard normal samples, we obtain the same distribution!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/N103_1.png&quot; alt=&quot;&quot; class=&quot;align=&amp;quot;center&amp;quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After some eye-candy we can tackle the above-mentioned transformation analytically.
We transform any Standard-Normally distributed random variable $\mathcal{E} \sim \mathcal{N}(0,1)$ by scaling it first with $\sigma$ and shifting it afterwards with $\mu$ into a Normally distributed random variable $w \sim \mathcal{N}(\mu, \sigma)$ through the linear transformation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  w = t_1 (\mathcal{E}, \mu, \sigma) = \mu + \mathcal{E} \cdot \sigma
\end{align*}&lt;/script&gt;

&lt;p&gt;Before we can dive into the gradients and properties we have to introduce one more non-trivial trick: keeping the standard deviation strictly positive.
This can be achieved by reparameterizing the standard deviation itself with the transformation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  \sigma = t_2(\rho) = \log(1+\exp(\rho)) \quad ; \sigma \in \mathbb{R}^+, \rho \in \mathbb{R}
\end{align*}&lt;/script&gt;

&lt;p&gt;The transformation is important as it allows $\sigma$ to be optimized freely without having to check after every optimization step, whether it is still positive.
The parameter $\rho$ can be optimized freely from $-\infty$ to $+\infty$ and only the $\log-\exp$ transformation above turns it into a strictly positive number.
For readability we will only use $\rho$ when it’s absolutely necessary since most people are fairly accustomed to the $\sigma$ notation of normal distributions.&lt;/p&gt;

&lt;p&gt;The important analytical property of this reparameterization is that we can take gradients with respect to its parameters.
Let’s create a simple example of linear regression of some input $x \in \mathbb{R}$ with a weight $w \sim \mathcal{N}(\mu,\sigma)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
  \hat{y} = f(x) = w \cdot x = (\mu + \mathcal{E} \cdot \sigma) \ x
\end{align*}&lt;/script&gt;

&lt;p&gt;Since $t_1$ is nothing else than a linear transformation we can easily take the gradients with respect to it’s parameters just as we would do with any other linear function.
The objective function for the toy example which follows is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \mathcal{L}(\hat{y}, y) &amp;= \mathcal{L}( f(x, w), y ) \\\\
  &amp;= (w\cdot x - y)^2 \\\\
  &amp;= ((\mu + \mathcal{E} \cdot \sigma) \ x - y)^2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;After the double reparameterization of $w$ with $t_1$ and $t_2$ we can compute the gradient for the deterministic parameters $\mu, \rho$ for the given cost function.
This is possible because we ‘outsourced’ the stochasticity of the distribution $w \sim \mathcal{N}(\mu, \sigma)$ into the Standard Normal random variable $\mathcal{E}$.
We are not interested in the stochasticity of $\mathcal{E} \sim \mathcal{N}(0,1)$ but in the parameters $\mu, \sigma$.
All the transformations of the variational parameters above are encapsulated in the cost function below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \mathcal{L}(\hat{y}, y) &amp;= \mathcal{L} (f(x, t_1(\mathcal{E}, \mu, t_2(\rho))), y)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;All we have to do is to apply the chain rule and work from outside towards the inside of the nested functions $\mathcal{L}, f, t_1$ and $t_2$.
This is what is commonly called reverse-mode auto differentiation and is applied in neural networks in the backpropagation algorithm.
The gradient for the variational parameter $\mu$ can be computed via:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \frac{\partial \mathcal{L}}{\partial \mu} &amp;= \frac{\partial \mathcal{L}}{\partial f} \frac{\partial f}{\partial w} \frac{\partial w}{\partial t_1} \frac{\partial t_1}{\partial \mu} \\\\
  \frac{\partial \mathcal{L}(\hat{y}, y)}{\partial \mu} &amp;= \frac{\partial \mathcal{L}(\hat{y}, y)}{\partial f(x, w)} \cdot \frac{\partial f(x, w)}{\partial w} \cdot \frac{\partial w}{\partial t_1(\mathcal{E}, \mu, \sigma)} \cdot \frac{\partial t_1(\mathcal{E}, \mu, \sigma)}{\partial \mu} \\\\
  &amp;= 2(\hat{y}-y) \cdot x \cdot 1 \cdot 1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and to obtain the gradient of the cost function with respect to $\rho$ we compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \frac{\partial \mathcal{L}}{\partial \rho} &amp;= \frac{\partial \mathcal{L}}{\partial f} \cdot \frac{\partial f}{\partial w} \cdot \frac{\partial w}{\partial g} \cdot \frac{\partial g}{\partial t_2} \cdot \frac{\partial t_2}{\partial \rho} \\\\
  \frac{\partial \mathcal{L}(\hat{y}, y)}{\partial \rho} &amp;= \frac{\partial \mathcal{L}(\hat{y}, y)}{\partial f(x, w)} \cdot \frac{\partial f(x, w)}{\partial w} \cdot \frac{\partial w}{\partial t_1(\mathcal{E}, \mu, \sigma)} \cdot \frac{\partial t_1(\mathcal{E}, \mu, \sigma)}{\partial t_2(\rho)} \cdot \frac{t_2(\rho)}{\partial \rho} \\\\
  &amp;= 2(\hat{y}-y) \cdot x \cdot 1 \cdot \mathcal{E} \cdot \frac{1}{\exp(-\rho)}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In essence, it is simply an extended chain rule where we not only have to backprop to the used weight $w$ but further through the reparameterization $t_1$ to obtain the gradient for $\mu$ and even further through the reparameterization $t_2$ to obtain the gradient for $\rho$.
Below is a graphical representation of how the the gradient is first backpropagated to the linear product $w \cdot x$ and then subsequently to the variational parameters $\mu, \rho$.
Since we transform $\rho$ such that it will it be in $\mathbb{R}^+$ with $t_2( \cdot )$ we also have to backpropagate through the transformation to obtain the gradient for $\rho$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ReparamTrick/ReparamTrick.png&quot; alt=&quot;&quot; align=&quot;center&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final point is a little code snippet which can be analyzed without much plotting&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

#Generate 10 data points between 0 and 5
x = np.linspace(0,5,10)
# Scale each data point with 3 such that we get a linear function with slope 3
y = 3*x

#Initialize the variational parameters mu and rho
mu = -3
rho = 1

#Set a learning rate
lr = 0.05

# Do a couple of epochs
for epoch in range(20):
    #Iterate over the training data
    for i, (label, data) in enumerate(zip(y, x)):

                #Sample from the standard normal distribution N(0,1)
                e = np.random.randn(1)
                #Transform the unconstrained variational parameter \rho into \sigma
                std = np.log(1+np.exp(rho))

                #Make a prediction with the sampled weight w=mu + e*std
                pred = (mu + e*std) * data

                #Backprop gradients onto the variational parameters
                mu = mu - lr * (pred - label) * data
                rho = rho - 3*lr * e/(1+np.exp(-rho)) * (pred - label) * data

                print('Episode {} | Data Point {}: μ: {:.2f}, σ: {:.02f}'.format(epoch, i, mu.squeeze(), std.squeeze()))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Running the little script we can see that the values were initialized with $\mu=-3$ and $\sigma=1.31$.
After a couple of iterations the parameters converge on the correct values of $\mu=3$ and $\sigma=0$.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 0: μ: -3.00, σ: 1.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 1: μ: -2.93, σ: 1.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 2: μ: -2.77, σ: 1.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 3: μ: -1.75, σ: 2.11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 4: μ: -0.66, σ: 0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 5: μ: 0.64, σ: 1.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 6: μ: 1.62, σ: 1.67&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 7: μ: 3.62, σ: 2.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 8: μ: 3.80, σ: 0.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 0&lt;/td&gt;
      &lt;td&gt;Data Point 9: μ: 2.20, σ: 0.33&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 0: μ: 2.20, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 1: μ: 2.21, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 2: μ: 2.26, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 3: μ: 2.36, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 4: μ: 2.52, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 5: μ: 2.69, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 6: μ: 2.90, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 7: μ: 2.95, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 8: μ: 3.02, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Episode 1&lt;/td&gt;
      &lt;td&gt;Data Point 9: μ: 2.92, σ: 0.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It should be noted that the through the $\sigma = \log(1+\exp(\rho))$ transformation the gradient with respect to $\rho$ becomes very, very small for large negative values of $\rho$.
The transformation $t_2$ is referred to as the ‘Softplus’ activation function is shown below.
We can see that while $\rho$ is freely optimizable the ‘Softplus’ function flattens out for negative values of $\rho$ and the resulting gradient is almost zero.
So we’re basically at the same spot deep learning was at 10 years ago with the vanishing gradient problem in very deep neural networks.
The Glorot-Activation and ReLU activation functions solved it and maybe somebody will develop a new fancy reparameterization for the variational parameter $\rho$. =)
That’s why it will never really converge to absolute 0, but that’s a trade-off we’re willing to make if we’re able to scale variational inference to large data sets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ReparamTrick/Softplus.png&quot; alt=&quot;&quot; align=&quot;center&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;</content><summary type="html">Outsourcing Stochasticity and Making Normal Distributions Differentiable</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/blog/ReparamTrick/ReparamTrick.png" /></entry><entry><title type="html">Non-Negative Matrix Factorization</title><link href="/blog/NMF/" rel="alternate" type="text/html" title="Non-Negative Matrix Factorization" /><published>2019-01-24T00:00:00+01:00</published><updated>2019-01-24T00:00:00+01:00</updated><id>/blog/NMF</id><content type="html" xml:base="/blog/NMF/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;non-negative-matrix-factorization&quot;&gt;Non-Negative Matrix Factorization&lt;/h2&gt;

&lt;p&gt;Recently while I was browsing reddit, an article was linked about the Netflix challenge from the late 2000’s.
It sparked my interest and it was a nice article to read.
Back then large-scale machine learning was still hampered through the lack of big data sets but when Netflix dumped a large recommendation data set onto the internet a lot of people went wild.&lt;/p&gt;

&lt;p&gt;It had been the first time that such a large data set with millions of data points had been published.
And this wasn’t just some obscure data set but real-world data from a major tech company itself.
Back then Netflix proclaimed that the first group/person to develop a recommendation system which would be 10\% better than their own would get a million dollars.&lt;/p&gt;

&lt;p&gt;A method that has been popping up repeatedly with the Netflix challenge has been non-negative matrix factorization (NMF).
I always made a big detour around the topic since it sounded complicated and but after the article I became interested in it and as it turns out, NMF isn’t too hard to understand.&lt;/p&gt;

&lt;p&gt;It all starts with the aim to factorize the matrix $R \in \mathbb{R}^{U \times I}$ into two smaller matrices $P \in \mathbb{R}^{U \times K}$ and $Q \in \mathbb{R}^{K \times I}$, such that their matrix product is approximately the original matrix $R$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R \approx P \ Q = \hat{R}.&lt;/script&gt;

&lt;p&gt;The approximation results from the fact that we want the dimension $K$ to be somewhat or significantly smaller than either $U$ or $I$, namely $K \ll U, I$.&lt;/p&gt;

&lt;p&gt;Why do we want this? Well foremost to find some latent structure in the matrix which allows us to compress the information in $R$ into two smaller matrices $U, I$ with only little loss in information.&lt;/p&gt;

&lt;p&gt;The idea is very similar to auto-encoders which use a reduced dimensionality in their middle to find a meaningful, but more compact representation of the original data.
Another example would be the well-known PCA which uses the data’s principal components (directions in the data space which exhibit the highest variance) to project the data itself into a subspace which is lower dimensional and more meaningful.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;NMF_01.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the example above we factorize the matrix $R \in \mathbb{R}^{3 \times 4}$ into two matrices $P \in \mathbb{R}^{3 \times 3}, Q \in \mathbb{R}^{3 \times 4}$ which share the common latent dimensionality $K=3$.
The latent factors correspond to the latent information which we can discover between the movies and what movies the users liked.
In our case the latent factors are ‘James Cameron’, ‘Action’ and ‘Romantic’.&lt;/p&gt;

&lt;p&gt;The important intuition is that the column vectors of $P$ represent the strength of association between users and latent factors and the row vectors of $Q$ represent the strength of association between the movies and the latent factors.&lt;/p&gt;

&lt;p&gt;In the matrix ‘P’ we can see that ‘User 1’ reacts normally to ‘James Cameron’, strongly to ‘Action’ and weakly to ‘Romance’.
‘User 2’ only reacts strongly to ‘Romance’ and doesn’t care much about either ‘James Cameron’ or ‘Action’.
The final ‘User 3’ on the other hand reacts strongly to ‘James Cameron’ and weakly to both ‘Action’ and ‘Romance’.&lt;/p&gt;

&lt;p&gt;In matrix $Q$ there also some information.
Obviously ‘James Cameron’ reacts strongly to ‘Titanic’, ‘Terminator 2’ and ‘Avatar’ since he’s the director of all three movies.
The latent factor ‘Romance’ reacts somewhat strongly to ‘Titanic’ and very strongly to ‘The Notebook’.&lt;/p&gt;

&lt;p&gt;Once we have the full factorization, we can use both matrices to fill in the missing values.
Because ‘User 1’ reacts strongly to ‘Action’, he will probably only react somewhat weakly to ‘The Notebook’.
Since ‘User 2’ favours ‘Romance’ above all else, the probable review of ‘Terminator 2’ will be very weak. Better not recommend it to the user!
‘User 3’ could react normally to ‘The Notebook’ since he doesn’t care too much about any specific latent factor.&lt;/p&gt;

&lt;p&gt;The infilling, or imputation, is made possible by computing the two factorized matrices $P$ and $Q$ from all observed data points in the data matrix $R$ and subsequently generalizing to the unseen data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;NMF_02.png&quot; alt=&quot;&quot; align=&quot;center&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve seen above that we can determine clear preferences of the users.
Instead of encoding the preferences of the users implicitly in $P$ we can encode them explicitly with biases.
By using biases we can let $P$ concentrate on finding strong weights with the items and offload the user preference onto the biases.
For each user there will be a bias which we can aggregate into a vector $u \in \mathbb{R}^{U}$.
Similarly we will introduce a bias for each movie in a vector $i \in \mathbb{R}^{I}$.
And for good measure we’ll include a overall scalar bias $b \in \mathbb{R}$.
Since these are vectors and we want to compute matrices, it’s easier to write down the full reconstruction of the matrix $R$ element-wise:&lt;/p&gt;

&lt;p&gt;\begin{align}
	R_{ui} \approx \sum_{k=0}^K P_{uk} Q_{ki} + u_u + i_i + b = \hat{R}_{ui}
\end{align}&lt;/p&gt;

&lt;p&gt;The above example is a made-up example which doesn’t correspond to a true factorization.
I only created it to demonstrate the core idea of NMF.
In reality we would have to infer the names of the latent factors by looking at the data.
Furthermore the values aren’t integer values like above but float values.&lt;/p&gt;

&lt;p&gt;Surprisingly, NMF is not hard to implement when using gradient descent.
As we are living in the age of automatic differentiation frameworks implementing NMF is a breeze.&lt;/p&gt;

&lt;p&gt;The recipe to obtain a full factorization is quite easy:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize $P, Q, u, i$ and $b$ randomly and positively&lt;/li&gt;
  &lt;li&gt;Compute $\hat{R}$ and compute a loss to $R$&lt;/li&gt;
  &lt;li&gt;Take the gradient of the loss w.r.t. all entries in $P, Q, u, i$ and $b$ and do a gradient descent step&lt;/li&gt;
  &lt;li&gt;Repeat until converged&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are two approaches how we can compute the loss.
We could iterate through all existing data points in $R$ individually and compute the loss between $R_{ui}$ and $\hat{R}_{ui}$.
Alternatively we can compute the loss between $R$ and $\hat{R}$ directly and we have to pay attention on what entries we compute the loss.
The latter approach can easily be accomplished with a mask of zeros and ones which eliminates the unknown entries in $R$ from the loss function.&lt;/p&gt;

&lt;p&gt;Let’s jump into some code!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch

FloatTensor = torch.FloatTensor

class NMF(torch.nn.Module):

        def __init__(self, R, K, lr, iterations):

                '''
                :param R: The recommendation matrix in which we want to fill in missing values
                :param K: The number of latent factors
                :param lr: The learning rate for the gradient descent
                :param iterations: The number of iterations of gradient descent
                :param zero_mask: The zero mask for the full matrix update
                '''

                super().__init__()

                self.R = R
                self.zero_mask = torch.zeros_like(R)
                self.zero_mask[R!=0] = 1
                self.num_users, self.num_items = R.shape
                self.K = K
                self.lr = lr
                self.iterations = iterations

                # We'll use an exponential function to initialize the values since it's support is in R^+
                exponential_dist_lambda = 5

                # Initialize the P and Q matrices
                self.P = torch.nn.Parameter(FloatTensor(self.num_users, self.K).exponential_(exponential_dist_lambda))
                self.Q = torch.nn.Parameter(FloatTensor(self.num_items, self.K).exponential_(exponential_dist_lambda))

                # Initialize the biases
                self.b_u = torch.nn.Parameter(FloatTensor(self.num_users, 1).exponential_(exponential_dist_lambda))
                self.b_i = torch.nn.Parameter(FloatTensor(self.num_items, 1).exponential_(exponential_dist_lambda))
                self.b = torch.nn.Parameter(FloatTensor([[0.1]]))

                # Criterion and optimizer for the gradient descent
                self.criterion = torch.nn.MSELoss()
                self.optim = torch.optim.Adam(self.parameters(), lr=self.lr)

                # Generate the training data from the existing data points in R
                self.samples = [(i, j, self.R[i,j]) for i in range(self.num_users) for j in range(self.num_items) if R[i,j]&amp;gt;0]
                np.random.shuffle(self.samples)

                # Define entry-wise or full-matrix-wise updates ... Hint: Full matrix converges faster
                self.forward_type = 1

        def forward(self, _u, _i):
                '''
                Elementwise prediction of the data matrix
                :param _u: The user
                :param _i: The item
                :return: \hat{R}_{ui}, the predicted rating through the parameters
                '''

                prediction = self.b + self.b_u[_u]
                prediction += self.b_i[_i]
                prediction += self.P[_u].dot(self.Q[_i])

                return prediction

        def forward_fullmatrix(self):
                '''
                Full matrix prediction where special care is given such that only the data point in the full prediction matrix are used for the loss
                :return: Full R matrix
                '''
                prediction = torch.matmul(self.P, self.Q.t())
                prediction += self.b + self.b_u
                prediction += self.b_i.t()

                # The points we want to predict are 0 in R, so the entries in the predicted matrx \hat{R} at the same position should also be zero
                prediction *= self.zero_mask

                return prediction

        def train(self):
                '''
                Trusty old SGD: Predict \hat{R} -&amp;gt; Compute loss(R, \hat{R}) -&amp;gt; Update matrices and vectors with negative gradient
                :return: Hopefully a converged recommendation matrix! ;-)
                '''
                for e in range(self.iterations):
                        loss_ = 0
                        for (i, j, r) in self.samples:
                                self.zero_grad()

                                if self.forward_type==1:
                                        pred = self.forward_fullmatrix()
                                        loss = self.criterion(pred, R)

                                elif self.forward_type==2:
                                        pred = self.forward(i, j)
                                        loss = self.criterion(pred, r)

                                loss.backward()
                                self.optim.step()
                                loss_+=loss.detach().numpy()

                        if (e+1) % 10 == 0:
                                print(&quot;Iteration: %d ; error = %.4f&quot; % (e+1, loss_))

        def full_matrix(self):
                '''
                Compute the full prediction \hat{R}
                :return:
                '''
                return_ = torch.matmul(self.P, self.Q.t())
                return_ += self.b_u
                return_ += self.b_i.t()
                return_ += self.b

                return return_.detach()

R = np.array([  [5, 3, 0, 1],
                [4, 0, 0, 5],
                [1, 1, 0, 1],
                [1, 0, 0, 1],
                [0, 1, 5, 4],])

R = FloatTensor(R)

mf = NMF(R, K=3, lr=0.005, iterations=300)
mf.train()

print(mf.full_matrix())
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I prefer to encapsulate my code in objects/Python classes.
We initialize the NMF-Algorithm by defining all the relevant parameters and data matrices.
Here we’re using an exponential distribution with a larger $\lambda$ to initialize $P, Q, u, i, b$ such that the their values and small and positive.&lt;/p&gt;

&lt;p&gt;The classic additive NMF algorithm iterates through all existing data points in the data matrix and minimizes the mean squared error loss between the prediction and the true data matrix.
I rewrote the gradient descent to compute the entire matrix in parallel and do one gradient descent step after evaluating the entire data matrix.
In this case special attention has to be paid over which values the loss function is computed.&lt;/p&gt;

&lt;p&gt;In the above example, unknown data points in $R$ were denoted with ‘0’.
After predicting $\hat{R}$, we simply multiply the entries in $\hat{R}$ with 0 such that loss with respect to these unknown entries is zero.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.zero_mask = torch.zeros_like(R)
self.zero_mask[R!=0] = 1

. . .

def forward_fullmatrix(self):
	'''
	Full matrix prediction where special care is given such that only the data point in the full prediction matrix are used for the loss
	:return: Full R matrix
	'''
	prediction = torch.matmul(self.P, self.Q.t())
	prediction += self.b + self.b_u
	prediction += self.b_i.t()

	# The points we want to predict are 0 in R, so the entries in the predicted matrx \hat{R} at the same position should also be zero
	prediction *= self.zero_mask

	return prediction
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After converging on a set of parameters for the given recommendation matrix, we can use the existing parameters $Q, u, i, b$ to predict data on a new partially filled user vector $u^* \in \mathbb{R}^{1 \times k}$.
All we have to do is to use the already existing codebook $Q$ and the biases $u, i, b$ and let the factorization for $p^* \in \mathbb{R}^{1 \times k}$ converge such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u^*_i \approx \sum_{k=0}^K p^*_k Q_{ki} + u_i + i_i + b&lt;/script&gt;</content><summary type="html">What Shall We Recommend?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/NMF_01.png" /></entry><entry><title type="html">Bayesian Neural Networks</title><link href="/blog/BayesianNeuralNetworks/" rel="alternate" type="text/html" title="Bayesian Neural Networks" /><published>2018-06-08T00:00:00+02:00</published><updated>2018-06-08T00:00:00+02:00</updated><id>/blog/BayesianNeuralNetworks</id><content type="html" xml:base="/blog/BayesianNeuralNetworks/"></content><summary type="html">Coming Soon</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="" /></entry><entry><title type="html">Easter Walk Poem</title><link href="/blog/BerlinEasterPoem/" rel="alternate" type="text/html" title="Easter Walk Poem" /><published>2018-04-27T00:00:00+02:00</published><updated>2018-04-27T00:00:00+02:00</updated><id>/blog/BerlinEasterPoem</id><content type="html" xml:base="/blog/BerlinEasterPoem/">&lt;p&gt;The idea to rewrite Goethe’s famous &lt;a href=&quot;http://www.literaturwelt.com/werke/goethe/osterspaziergang.html&quot;&gt;‘Easter Walk Poem’&lt;/a&gt; came while I was sitting on my balcony on the first days of spring.
It’s a custom to recite the poem during my family’s Easter breakfast and when I was back in Berlin after Easter, I started rewriting the poem with respect to Berlin.
This is what this little literary excursion became:&lt;/p&gt;

&lt;p&gt;Berlin-Spaziergang&lt;/p&gt;

&lt;p&gt;Vom Gluten befreit sind Kreuzberg und Mitte &lt;br /&gt;
Durch des Veganers strengen, missbilligenden Blick.&lt;/p&gt;

&lt;p&gt;Der alte Winter, in seiner Schwäche,&lt;br /&gt;
Zog sich nach Spandau zurück.&lt;br /&gt;
Von Schönefeld her sendet er, fliehend, nur&lt;br /&gt;
Ohnmächtiges Unglauben über den BER.&lt;/p&gt;

&lt;p&gt;Am Kotti regen sich die Dealer zum Streben,&lt;br /&gt;
Den Kater wollen sie mit weissem Pulver beleben;&lt;br /&gt;
Am Rosi geraten die Spätis ins Bewegen;&lt;br /&gt;
Rot-Braune Sternis erhalten ihren ersten Segen.&lt;/p&gt;

&lt;p&gt;Doch an richtigen Berlinern fehlts im Revier,&lt;br /&gt;
Berlin nimmt geputzte Hipster dafür.&lt;/p&gt;

&lt;p&gt;Aus dem Brandenburger Tor,&lt;br /&gt;
Dringt ein buntes Gewimmel Richtung Tiergarten hervor.&lt;br /&gt;
Sonntags sonnen Sich Touris im Maurpark gern.&lt;br /&gt;
Sie feiern des Karaokes’ allsonntaglichen Lärm.&lt;/p&gt;

&lt;p&gt;Denn sie sind mit Easyjet für zwei Tage gekommen:&lt;br /&gt;
Von Barcelonas breiten Ramblas,&lt;br /&gt;
Aus New Yorks zu teuren Bars,&lt;br /&gt;
Von Tokyos zu schmalen Gassen,&lt;br /&gt;
Aus Londons engen Häusern,&lt;br /&gt;
Wurden sie alle ins schöne Berlin gelassen.&lt;/p&gt;

&lt;p&gt;Sieh nur, sieh! wie behend alle Menschen,&lt;br /&gt;
In diesem schönen Berlin zusammenleben,&lt;br /&gt;
Selbst von des Berghains fernen Darkrooms,&lt;br /&gt;
Blinken uns wenig Kleider entgegen.&lt;/p&gt;

&lt;p&gt;Wie der Schawarma mit seiner Würze,&lt;br /&gt;
In FHain manch gierigen Mund bewegt,&lt;br /&gt;
Beglückt das Watergate bei Sonnenaufgang,&lt;br /&gt;
Ohren und Augen an der Spree.&lt;/p&gt;

&lt;p&gt;Ich höre schon des Alex’ Getümmel,&lt;br /&gt;
Hier ist des Subkulturs wahrer Himmel,&lt;br /&gt;
Zufrieden jauchzet groß und klein:&lt;br /&gt;
&lt;strong&gt;In Berlin bin ich Mensch, hier darf ich’s sein!&lt;/strong&gt;&lt;/p&gt;</content><summary type="html">Goethe's Easter Walk, the Berlin Version</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="" /></entry><entry><title type="html">Bayesian Optimization</title><link href="/blog/BayesianOptimization/" rel="alternate" type="text/html" title="Bayesian Optimization" /><published>2018-04-24T00:00:00+02:00</published><updated>2018-04-24T00:00:00+02:00</updated><id>/blog/BayesianOptimization</id><content type="html" xml:base="/blog/BayesianOptimization/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;bayesian-optimization&quot;&gt;Bayesian Optimization&lt;/h2&gt;

&lt;p&gt;As stated above, many problem settings in engineering and science can be formulated as optimization problems of a criterion, commonly called an objective function, $\mathcal{F}(x)$ with respect to some argument $x$.
The goal of any optimization is to find the global optimum of such a function $\mathcal{F}(x)$.
For linear or convex optimization problems, this is usually feasible, yet optimization becomes difficult for non-linear objective functions.
Bayesian optimization tries to tackle such non-linear objective functions by searching for a global optimum in a probabilistical manner.&lt;/p&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;p&gt;In computer science, mathematics and operations research, mathematical optimization aims to find the best value $x^* \in \mathcal{X}$ from a set of feasible values $\mathcal{X}$ with respect to an criterion or objective function $\mathcal{F}(x)$.
Optimization problems can be formulated as either maximization or minimization problems of the objective function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathcal{F}(x^* ) = \min_{x \in \mathcal{X}} \mathcal{F}(x) = \max_{x \in \mathcal{X}} -\mathcal{F}(x)
\end{align}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     x^* = \underset{x \in \mathcal{X}}{\operatorname{argmin}} \mathcal{F}(x) = \underset{x \in \mathcal{X}}{\operatorname{argmax}} -\mathcal{F}(x)
\end{align}&lt;/script&gt;

&lt;p&gt;Since $\mathcal{F}(x)$ is often a complicated, non-linear function the solution is searched for in an iterative manner.
Most optimization algorithms evaluate the objective function $\mathcal{F}(x)$ through a set of succesive queries $x_{1:n}=\{ x_i \}_{i=1}^n \subset \mathcal{X}$ such that the information of the previous evaluations guide the next evaluation $x_{n+1}$ through a utility function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     x_{n+1} = \underset{x \in \mathcal{X}}{\operatorname{argmax}} \ \mathcal{U}(x \ | \ x_1, \ldots, x_n)
\end{align}&lt;/script&gt;

&lt;p&gt;The information contained in the past evaluations $x_{1:n}$ is thus leveraged in a way to make the evaluation $x_{n+1}$ as close as possible to the global optimum.
The utility function $\mathcal{U}$ should balance the exploration of the set of feasible optima $\mathcal{X}$ while simultaneously exploiting existing information in $ x_{1:n}$ to find the globally optimal solution $x^* $.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-optimization-with-gaussian-processes&quot;&gt;Bayesian Optimization with Gaussian Processes&lt;/h2&gt;

&lt;p&gt;In Bayesian optimization a Gaussian process is used to compute a probability distribution over the past evaluations $x_{1:n}$, which guides a subsequent sampling process.
The sampling process uses an acquisition function $\Lambda(x \ | \ x_{1:n})$, which is a utility function on the posterior distribution computed by the Gaussian process.
The acquisition function balances both the exploration as well as the exploitation of the unknown objective function $\mathcal{F}(x)$.
The next evaluation is chosen such that it maximizes the acquisition function, i.e.
\begin{align}
     x_{n+1} = \underset{x \in \mathcal{X}}{\operatorname{argmax}} \ \Lambda(x \ | \ x_{1:n})
\end{align}&lt;/p&gt;

&lt;p&gt;By computing posterior distributions over all predictions at once, Gaussian processes have a powerful property which enables them to search for an optimum globally.
The posterior distributions allow Gaussian processes to balance both exploitation and exploration of the set of feasible solutions by incorporating their uncertainty into optimization task.&lt;/p&gt;

&lt;p&gt;The acqusition function $\Lambda(x \ | \ x_{1:n})$ serves as an improvement criterion for the yet unevaluated feasible solutions.
The improvement is computed relative to the optimal solution $x^+ \in x_{1:n}$ in the set of previous evaluations $x_{1:n}$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     x^+ &amp;= \underset{x \in x_{1:n}}{\operatorname{argmax}} \ \mathcal{F}(x)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A popular acquisition functions is the upper/lower confidence bound \cite{ucb}, which scales the mean with respect to the previously best evaluation.
It then considers a multiple of the standard deviation and adds it for maximization problems or subtracts it for minimization problems.
The hyperparameter $\kappa$ is usually selected as a small integer number, which can be intuitively selected due to its close relationship to confidence values of the Gaussian distribution.
Given the mean $\mu(x)$ and covariance function $\sigma(x)$, the upper confidence bound is computed with the hyperparameter $\kappa$ via&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathbb{UCB}[x] = \mu(x) + \kappa \sigma(x) - \mathcal{F}(x^+)
\end{align}&lt;/script&gt;

&lt;p&gt;A different acquisition function is the expected improvement (EI) \cite{mockus1975bayesian} which considers the expected value at a point $x_{n+1}$ above the currently best value $x^+$.
The expected improvement is the most Bayesian acquisition function as it incorporates the posterior in its entirety including the uncertainty.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{EI}[x] &amp;= \int_{x^+}^{\infty} \left( \frac{f(x) - f(x^+)}{\sigma(x)} \right) \mathcal{N}(x | \mu(x), \sigma(x)) df(x) \\
     &amp;= \int_{x^+}^{\infty} z(x) \ \mathcal{N}(x | \mu(x), \sigma(x)) df(x) \\
     &amp;=\sigma(x) \left( z(x)  \Phi \left( z(x) \right) + \mathcal{N}_{0,1} \left( z(x) \right) \right) \\
     %&amp;=\sigma(x) \left( \frac{\mu(x) - f(x^+)}{\sigma(x)}  \Phi \left( \frac{\mu(x) - f(x^+)}{\sigma(x)} \right) + \mathcal{N}_{0,1} \left( \frac{\mu(x) - f(x^+)}{\sigma(x)} \right) \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the term $z(x)$ represents the z-score for a specific value $x$ in the yet unevaluated feasible set solutions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     z(x) &amp;= \frac{f(x) - f(x^+)}{\sigma(x)}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While the UCB acquisition function has a more straightforward interpretation, it suffers from getting stuck in local minima.
This is due to UCB using a fixed integer multiple $\kappa$ of the variance instead of integrating over it.
The EI acquisition utilizes the uncertainty in a fully Bayesian way and is able to explore the feasible set even after having found an optimum.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;BO_EI0.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI1.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI2.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI3.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI4.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI5.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI6.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI7.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI8.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI9.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI10.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI11.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI12.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI13.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;
&lt;img src=&quot;BO_EI14.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">Using Gaussian Processes for Optimization</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/BO_EI4.png" /></entry><entry><title type="html">Gaussian Processes - Extensions</title><link href="/blog/Gaussian-Processes-Extensions/" rel="alternate" type="text/html" title="Gaussian Processes - Extensions" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes-Extensions</id><content type="html" xml:base="/blog/Gaussian-Processes-Extensions/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;In the real world, the observations onto which the Gaussian process is fitted are often influenced and distorted by noise.
This noise is modeled as a independent, identically distributed normal distribution around zero with an error variance $\sigma^2_{\varepsilon}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     y &amp;= f(x) + \varepsilon, \quad \quad \text{i.i.d.} \ \varepsilon \sim \mathcal{N}(0, \sigma^2_{\varepsilon})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix between the respective observations with noise is modified on the diagonal entries.
The linear covariance operator can be applied independently to both the objective function evaluation and the noise, yet the noise variance can only be included for the diagonal entries of the covariance matrix.
This is due to the assumption of independent, identical distributed noise, which is uncorrelated between observations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y,y'] &amp;= k(x,x') + \mathbb{1}_{y=y'}  \mathbb{V}[\varepsilon]\\
     &amp;= k(x,x') + \mathbb{1}_{y=y'}  \sigma^2_{\varepsilon}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Gaussian process without noisy observations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_nonoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noise only on the mean of the posterior distribution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_meannoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noise only on the variance of the posterior distribution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_varnoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gaussian process with noisy observations affecting both the mean and the variance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Noise_bothnoise.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be realized with the addition of the noise’s variance to the diagonal entries of the covariance matrix of the observation kernel matrix $K_{XX}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K} =
     \begin{bmatrix}
          K_{XX}+\sigma^2_{\varepsilon} \cdot I  &amp; K_{XX_*} \\
          K_{X_*X} &amp; K_{X_*X_*}
     \end{bmatrix} =
     \begin{bmatrix}
          k(X, X) + \sigma_{\varepsilon} \cdot I &amp; k(X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $I$ is an identity matrix $I\in \mathbb{R}^{N \times N}$.
While the noise itself decreases the precision with which we can fit the GP to the observations, it has convenient numerical properties.
The Gramian block matrix $K_{XX}$ has to be inverted during the computation of the mean and covariance function.
Due to possible rank defincencies, $K_{XX}$ can become singular which prohibits its inversion.
Rank definencies in the covariance matrix can arise when two observations are numerically almost identical.
Incorporating the noise variance into the covariance matrix can be thus regarded as a regularization of the Gaussian process.
This opens the possibility of different regularization themes as both the mean and variance can be independently regularized with respect to the inverse of $K_{XX}$ in $\mu(y_* )$ and $\Sigma(y_* )$.&lt;/p&gt;

&lt;h2 id=&quot;model-selection&quot;&gt;Model Selection&lt;/h2&gt;

&lt;p&gt;The optimization of hyperparameters in machine learning is a pivotal process which can influence the performance significantly.
In this regard, Bayesian methods offer a substantial advantage over non-Bayesian methods as the optimal hyperparameters can be automatically recovered from the Bayesian model.
For a supervised learning task, the objective is to maximize the likelihood probability of the targets $p(\mathcal{D})$.&lt;/p&gt;

&lt;p&gt;A central aspect of Bayesian methods is the placement of a prior $p(\theta)$ over possible values of $\theta$ which encodes the prior belief what values of $\theta$ are regarded as probable.
Instead of considering a single value for $\theta$ a probability distribution is used that assigns a different weighting to different values of $\theta$.
This is especially important in tasks with small datasets where the likelihood is sensitive to the variability in the data.&lt;/p&gt;

&lt;p&gt;The prior can be marginalized to evaluate its influence on the data likelihood.
The objective is therefore to find suitable distributions for $\theta$ which increase the likelihood of the data, ie.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     p(\mathcal{D}) = \int p(\mathcal{D}, \theta) \ p(\theta) \ d\theta
\end{align}&lt;/script&gt;

&lt;p&gt;In the case of Gaussian processes with the squared exponential kernel, the hyperparameters are $\theta = \{ \alpha, \sigma \}$ for which we seek values that maximize the probability of the data, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \max_{\theta} \ p(\mathcal{D}; \theta)
     &amp;= \max_{\theta} \ p(y, X ; \theta) \\
     &amp;= \max_{\theta} \ p(y, X  | \theta) \ p(\theta) \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |K_{XX}|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          {K_{XX}}^{-1}
          y
     \right]
     \\
     &amp;= \max_{\theta}
     \frac{1}{\sqrt{(2 \pi)^{N} |k(X, X; \theta)|^2}}
     \exp \left[
     -\frac{1}{2}
          y ^T
          k(X, X;\theta)^{-1}
          y
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the parameters $\theta$ determine the Gramian matrix $k(XX;\theta)$.
The maximization of the data likelihood is commonly reformulated as a minimzation of the negative log-likelihood.
Working with the log-probability offers a higher numerical stability with respect to floating-point arithmetic of modern computers.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \min_{\theta}  -\log{p(\mathcal{D};\theta)}
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |K_{XX}|\right] + \frac{1}{2} y^TK_{XX}^{-1}y \\
     &amp;= \min_{\theta} \ \frac{N}{2} \log\left[ 2 \pi \right] + \log\left[ |k(XX;\theta)|\right] + \frac{1}{2} y^Tk(X,X;\theta)^{-1}y
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The optimization of the log-likelihood can be done with regular optimization algorithms such as limited memory BFGS.&lt;/p&gt;

&lt;h2 id=&quot;derivative-information&quot;&gt;Derivative Information&lt;/h2&gt;

&lt;p&gt;Gaussian processes in their traditional definition are described as a Gaussian distribution over possibly infinite observations.
A Gaussian process computes a predictive distribution for $y_*$ such that predictions are close to observations in their vicinity.
We can expand the Gaussian process by including derivative observations into the set of observations which enforces a similarity in the gradients of the predictions with respect to observations in their vicinity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \begin{bmatrix}
          y \\
          \nabla y \\
          y_*
     \end{bmatrix}
     \sim
     \mathcal{N}\left(\ \cdot \ | \mathbf{0}, \mathbf{K}^{\nabla}\right)
\end{align}&lt;/script&gt;

&lt;p&gt;The joint distribution over predictions, derivative observations and observations can be modeled as a Gaussian over all three types of observations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(y_*, \nabla y, y, X_*, X)
&amp;\propto
\exp \left[
-\frac{1}{2}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}^T
\left[
\begin{array}{c c | c}
     K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
     K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
     \hline
     K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
\end{array}
\right]^{-1}
\begin{bmatrix}
     y \\
     \nabla y \\
     y_*
\end{bmatrix}
\right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the expanded covariance matrix which now includes similarity measures between predictions, observations and derivative observations:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbf{K}^{\nabla}
     &amp;=
     \left[
     \begin{array}{c | c}
          K^{\nabla, \nabla \nabla}_{X X} &amp; K^{\nabla}_{X X_*} \\
          \hline
          K^{\nabla}_{X_*X} &amp; K_{X_* X_*}
     \end{array}
     \right]\\
     &amp;=
     \left[
          \begin{array}{c c | c}
               K_{XX} &amp; K^{\nabla}_{XX} &amp; K_{XX_*} \\
               K^{\nabla T}_{XX} &amp; K^{\nabla\nabla}_{XX} &amp; K^{\nabla}_{XX_*} \\
               \hline
               K_{X_*X} &amp; K^{\nabla}_{X_*X} &amp; K_{X_*X_*}
          \end{array}
     \right] \\
     &amp;=
     \left[
     \renewcommand*{\array_*tretch}{1.5}
          \begin{array}{c c | c}
               k_{y,y}(X, X) &amp; k_{y, \nabla y}(X, X) &amp; k_{y,y_*}(X, X_*) \\
               k_{\nabla y, y}(X, X) &amp; k_{ \nabla y, \nabla y}(X, X) &amp; k_{\nabla y, y_*}(X, X_*) \\
               \hline
               k_{y_*, y}(X_*, X) &amp; k_{y_*, \nabla y}(X_*, X) &amp; k_{y_*, y_*}(X_*, X_*)
          \end{array}
     \right] \label{eq:derivobs_kernelmatrix-1}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The posterior distribution including derivative observations can be derived from the joint distribution with the matrix inversion lemma in the same manner as seen above.
The mean and covariance of the posterior distribution with derivative observations can be computed with the expanded kernel matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
     p(y_* | \nabla y, y, X_*, X) &amp;= \mathcal{N} \big( K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} y, K_{X_*X_*} - K^{\nabla}_{X_*X} {K^{\nabla, \nabla \nabla}_{XX}}^{-1} K^{\nabla}_{XX_*} \big)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Gramian block matrices between predictions, observations and derivative observations can be computed with updated kernels with incorporate the derivative observations.
More precisely, the covariance between two any entries in the observation respectively prediction vector are defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= k_{y, y'}(x, x') \\
     \mathbb{C}[y, \nabla y'] &amp;= k_{y, \nabla y'}(x, x') \\
     \mathbb{C}[\nabla y, y'] &amp;= k_{\nabla y, y'}(x, x') \\
     \mathbb{C}[\nabla y, \nabla y'] &amp;= k_{\nabla y, \nabla y'}(x, x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;These updated kernels can be derived in a fairly straightforward manner since the covariance with the zero mean assumption is a linear operator.
In order to expand the Gaussian process with derivative observations we have to take the derivative of the kernel and expand the covariance matrix with the respective entries:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mathbb{C}[y, y'] &amp;= \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= k(x,x') \\
     \mathbb{C}[y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x'} \frac{1}{N} \sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x'} \mathbb{C}[y,y'] \\
          &amp;= \nabla_{x'} k(x,x')\\
     \mathbb{C}[\nabla_{x}y, \nabla_{x'}y'] &amp;= \frac{1}{N} \sum_{i=0}^N \nabla_{x} y_i \cdot \nabla_{x'}y_i' \\
          &amp;= \nabla_{x} \nabla_{x'} \frac{1}{N}\sum_{i=0}^N y_i \cdot y_i' \\
          &amp;= \nabla_{x}\nabla_{x'} \mathbb{C}[y, y'] \\
          &amp;= \nabla_{x}\nabla_{x'} k(x,x')
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;While derivative observations themselves are usually hard to come by for computationally expensive functions $f(x)$, derivative observations are of numerical advantage in cases where observations lie very close to each other.
In these cases the inversion can become unstable or even impossible due to the rank definciency.
Derivative observations pose a useful way to circumvent such rank definciencies for very close observations by combining two observations into one observation and a derivative observation.&lt;/p&gt;

&lt;p&gt;A Gaussian process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;GP_4Obs.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same Gaussian process with derivative observations. The GP is able to fit the true function considerably better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;GP_4Obs_Deriv.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">Extensions to Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_4Obs_Deriv.png" /></entry><entry><title type="html">Gaussian Processes - Basics</title><link href="/blog/Gaussian-Processes/" rel="alternate" type="text/html" title="Gaussian Processes - Basics" /><published>2018-04-23T00:00:00+02:00</published><updated>2018-04-23T00:00:00+02:00</updated><id>/blog/Gaussian-Processes</id><content type="html" xml:base="/blog/Gaussian-Processes/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Many problems in science and engineering can be formulated as a mathematical optimization problem in which an optimal solution is sought, either locally or globally.
The field of global optimization is the application of applied mathematics and numerical analysis towards finding the overall optimal solution in a set of candidate solutions.
Local optimization is considered an easier problem, in which it suffices to find an optimum which is optimal with respect to its immediate vicinity.
Such a local optimum is obviously a suboptimal solution and, while harder to find, global optima are more preferred.&lt;/p&gt;

&lt;p&gt;Generally, optimization problems are formulated as finding the optimal solution which minimizes, respectively maximizes, a criterion, which is commonly referred to as the objective function.
Further constraints on the the set of solutions can be formulated, such that only a subset of solutions are permissible as candidates for the optimum.&lt;/p&gt;

&lt;p&gt;Optimization is commonly done in an iterative manner where the objective function is evaluated for multiple candidate solutions.
Due to the iterative nature, it becomes desirable to evaluate this function as few times as possible over the course of the entire optimization, which becomes even more crucial when the evaluation of the objective function itself is costly.
Therefore, it would be advantageous to infer information about the objective function beyond the evaluations themselves, which only provide punctual information.&lt;/p&gt;

&lt;p&gt;Bayesian inference models provide such advantages since they compute predictive distributions instead of punctual evaluations.
One class of Bayesian inference models are Gaussian processes (GP), which can be applied to model previous evaluations of the objective function as a multi-variate Gaussian distribution.
Given such a Gaussian distribution over the previous evaluations, information can be inferred over all candidate solutions in the feasible set at once.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-processes&quot;&gt;Gaussian Processes&lt;/h2&gt;

&lt;p&gt;In most situations where observations have many small independent components, their distribution tends towards the Gaussian distribution.
Compared to other probability distributions, the Gaussian distribution is tractable and it’s parameters have intuitive meaning.
The theory of the central limit theorem (CLT) makes the Gaussian distribution a versatile distribution which is used in numerous situations in science and engineering.&lt;/p&gt;

&lt;p&gt;A convenient property of the Gaussian distribution for a random variable $X$ is its complete characterization by its mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance $\Sigma$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu &amp;= \mathbb{E}[X] \\
     \Sigma &amp;= \mathbb{E}[(X-\mu)^T(X-\mu)]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Mathematically, a multivariate Gaussian for a vector $x \in \mathbb{R}^d$ is defined by its mean $\mu \in \mathbb{R}^d$ and covariance function $\Sigma \in \mathbb{R}^{d \times d}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
          \mathcal{N}(x | \mu, \Sigma) &amp;=
               \frac{1}{\sqrt{(2 \pi)^d |\Sigma|^2}}
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right] \\
               &amp;\propto
               \exp \left[
               -\frac{1}{2}
               (x-\mu)^T \Sigma^{-1}(x-\mu)
               \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;A useful property of the Gaussian distribution is that its shape is determined by its mean and covariance in the exponential term.
This allows us to omitt the normalization constant and determine the relevant mean and covariance terms from the exponential term.&lt;/p&gt;

&lt;p&gt;Let $y=f(x)$, where $x \in \mathbb{R}^d$ and $y \in \mathbb{R}$ be the function which we want to estimate with a Gaussian Process.
Furthermore, let $\mathcal{D} = (X, y) = \{(x_i, y_i)\}_{i=0}^N$
with $X \in$ $\mathbb{R}^{N \times d}$
and $y \in \mathbb{R}^{N}$,
be our training observations of the function $f$.&lt;/p&gt;

&lt;p&gt;Lastly, let $ \mathcal{D}_* = ( X_* , y_* ) = \{ ( X_{ * j } , y_{ * j } ) \} _{j=0}^{ N_* } $ with $ X_* \in \mathbb{R}^{N_* \times d} $ and $ y_* \in \mathbb{R}^{ N_* } $ ,
be the test observations at which we want to compute the predictive distributions of $ y_* =f( X_* ) $
for the function $ f $.&lt;/p&gt;

&lt;p&gt;A Gaussian process is defined as a stochastic process, such that every finite collection of realizations
$ X = \{ x_i \}_{ i=0 }^N , x_i \in \mathbb{R}^d$ of the random variables
$ X \sim \mathcal{N}( \cdot  |  \mu, \Sigma),  X \in \mathbb{R}^d $
is a multivariate distribution.&lt;/p&gt;

&lt;p&gt;A constraint of Gaussian processes as they are used in machine learning, which can be relaxed in specific cases, is that they are assumed to have a zero mean.
In order to compute a predictive distribution over $ y_* $ we initially construct the joint distribution over the training observations $\mathcal{D} = (X,y) $ and test observations $ \mathcal{D}_* = ( X_* ,y_* ) $:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_*, y, X_* , X) &amp;= \frac{1}{\sqrt{(2 \pi)^{ N+N_* } |K|^2}}
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ XX } &amp; K_{ X X_* } \\
          K_{ X_* X } &amp; K_{ X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{ X X } &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_*}
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;\propto
     \mathcal{N}
     \left(
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix} \middle|
     \mathbf{0}, K
     \right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the covariance matrix of the joint Gaussian distribution is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K=\begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{ X_* X_* }
     \end{bmatrix}
     =
     \begin{bmatrix}
          k( X, X) &amp; k( X, X_*) \\
          k(X_*, X) &amp; k(X_*, X_*)
     \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $ k(x,x’) $ is an kernel function $ k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ that measures the similarity between two vectors $ x, x’ \in \mathcal{X}$.
We can observe from \eqref{eq:covariance1} that the covariance between any two observations in the distribution is determined by the similarity through the kernel function $k(x, x’)$, namely&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     \mathbb{C}[y, y'] = k(x, x')
\end{align}&lt;/script&gt;

&lt;p&gt;An essential component of a GP is the kernel function with which the covariances is computed.
Often the kernels are engineered to incorporate prior knowledge.
A commonly used kernel is the squared exponential kernel&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
     k(x, x' \ ; \ \theta) = \alpha \exp \left[ - \frac{|| x - x'||^2}{2 \sigma^2}\right], \quad \theta = \{ \alpha, \sigma \}
\end{align}&lt;/script&gt;

&lt;p&gt;where $\theta$ corresponds to the hyperparameters of the Gaussian process which can be independently optimized with respect to the observations $(X, y)$.&lt;/p&gt;

&lt;p&gt;Gaussian Processes can be readily extended to multiple dimensions by simply adjusting the kernel to incorporate multiple dimensions.
The individual variances $\sigma_i$ of the dimensions $\mathbb{R}^d$ in the exponential kernel can be independently adjusted, or optimized with the maximization of the marginal probability of the data.
The expanded kernel for multidimensional input is defined as followed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     k(x, x'; \ \theta) &amp;= \alpha \exp \left[ - \frac{1}{2} (x-x') \Sigma^{-1} (x-x')     \right], \quad \theta=\{ \alpha, \Sigma \} \\
     \Sigma &amp;= \text{diag}(\sigma^2_0, \sigma^2_1, \ldots, \sigma^2_d)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The block matrices $k(X,X) \in \mathbb{R}^{N \times N}$
$ k(X, X_* ) \in \mathbb{R}^{N \times N_* }, $
$k( X_* , X ) \in \mathbb{R}^{ N_* \times N }$ and
$k(X_* , X_* ) \in \mathbb{R}^{N_* \times N_* }$ are the Gramian matrices of the training and test observations with respect to the kernel $k(x, x’)$.&lt;/p&gt;

&lt;p&gt;Furthermore both $k(X,X)$ and $k( X_* , X_* )$ are symmetric matrices and $k( X, X_* )$ and $k( X_* ,X)$ are each others mutually transposed.&lt;/p&gt;

&lt;p&gt;Given the joint distribution $ p(y_* , y, X_* , X) $, the aim for modeling the training and test observations with a GP is to derive the posterior distribution $ p( y_*  | y, X_* , X ) $ .
In order to derive the mean and covariance function of the posterior distribution, the block matrix inversion lemma is used to compute the inverse of the covariance matrix.&lt;/p&gt;

&lt;p&gt;For ease of reading and brevity the respective block matrices were replaced by more easily readible variables in the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     K^{-1}&amp;= \begin{bmatrix}
          K_{ X X} &amp; K_{ X X_* } \\
          K_{ X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1} \label{eq:blockmatrixinversionlemma1} \\
     &amp; =\begin{bmatrix}
          A &amp; B \\
          C &amp; D
     \end{bmatrix}^{-1} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\
          -(D-CA^{-1}B)^{-1}CA^{-1} &amp; (D-CA^{-1}B)^{-1}
     \end{bmatrix} \\
     &amp;=\begin{bmatrix}
          A^{-1} + A^{-1}B\Sigma^{-1}CA^{-1} &amp; -A^{-1}B\Sigma^{-1} \\
          -\Sigma^{-1}CA^{-1} &amp; \Sigma^{-1}
     \end{bmatrix} \label{eq:Sigma^-1Identity} \\
     &amp;= \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix} \label{eq:blockmatrixinversionlemma-1} \\
     \Sigma &amp;= D-CA^{-1}B = K_{X_* X_* } - K_{ X_* X}{K_{ X_* X_* }}^{-1}K_{X X_* }
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Instead of computing the inverse of the entire matrix $K$, which can be computationally expensive for large covariance matrices, the precision matrix $K^{-1}$ can be computed block-wise with the block matrix inversion lemma.
Given the precision matrix in block matrix notation, the inner product in the exponential term of the Gaussian distribution can be computed as a sum over the inner products with the independent block matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* , y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          K_{XX} &amp; K_{X X_* } \\
          K_{X_* X} &amp; K_{X_* X_* }
     \end{bmatrix}^{-1}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}^T
     \begin{bmatrix}
          P &amp; Q \\
          R &amp; S
     \end{bmatrix}
     \begin{bmatrix}
          y \\
          y_*
     \end{bmatrix}
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( y^TPy + y^TQ y_* + y_*^TRy + y_* ^TS y_*
     \right)
     \right] \label{eq:jointdist_innersumoverblockmatrices}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since we are only interested in the posterior distribution $p(y_*  | y, X_* , X )$, terms which do not include $ y_* $ can be moved into the normalization term.
The conditional distribution can thus be simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* |  y, X_* , X)
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -y^TQy_* - y_*^TRy + y_*^TS y_*
     \right)
     \right] \\
     &amp;=
     \exp \left[
     -\frac{1}{2}
     \left( -y^TA^{-1}B\Sigma^{-1} y_* -y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}CA^{-1}y + y_*^T\Sigma^{-1}y_*
     \right)
     \right] \\
     &amp;\propto
     \exp \left[
     -\frac{1}{2}
     \left( -2 y_*^T\Sigma^{-1}K_{X_* X}{K_{ X X }}^{-1} y + y_*^T\Sigma^{-1}y_*
     \right)
     \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the matrices $\Sigma$ being a symmetric matrix by construction, and $B$ and $C$ being each other transposed, namely $C^T=B$, which gives rise to the identity:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     (y^TA^{-1}B\Sigma^{-1}y_*)^T
          &amp;= y_*^T(\Sigma^{-1})^TB^T(A^{-1})^Ty \\
          &amp;= y_*^T\Sigma^{-1}CA^{-1}y
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Alternatively one would argue that the result of both inner products yields the same scalar value due to $B=C^T$.
With the derivations above we obtain a posterior distribution $p(y_*  |  y, X_* , X )$ with the mean and covariance function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
     \mu(y_*)       &amp;= K_{ X_* X}{K_{XX}}^{-1}y \\
     \Sigma(y_*)    &amp;= K_{ X_* X_* } - K_{ X_* X}{K_{ X X }}^{-1}K_{ X X_*}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;It should be noted that during plotting only the diagonal entries of the covariance matrix are of interest since the diagonal entries of the covariance matrix denote the variances at the evaluated points.
Given the computation of both the mean and variance of the posterior distribution we obtain a Gaussian distribution:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
     p(y_* | y, X_*, X) &amp;= \mathcal{N} \big( \underbrace{K_{X_* X} {K_{XX}}^{-1} y}_{\mu}, \underbrace{K_{X_* X_*} - K_{X_* X}{K_{X X}}^{-1}K_{X X_*}}_{\Sigma} \big)
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here is an image of a Gaussian Process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ludwigwinkler/BayesianOptimization/gh-pages/docs/GP_2Obs.png&quot; alt=&quot;&quot; height=&quot;75%&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;</content><summary type="html">A Tutorial for Gaussian Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/GP_2Obs.png" /></entry><entry><title type="html">RL in Challenging State Spaces and Sparse Reward Signals</title><link href="/blog/AdvRL/" rel="alternate" type="text/html" title="RL in Challenging State Spaces and Sparse Reward Signals" /><published>2017-11-06T00:00:00+01:00</published><updated>2017-11-06T00:00:00+01:00</updated><id>/blog/AdvRL</id><content type="html" xml:base="/blog/AdvRL/">&lt;head&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &lt;/script&gt;
       &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], [&quot;\\(&quot;,&quot;\\)&quot;] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       &lt;/script&gt;
       &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a talk I gave at the Berlin Machine Learning Meetup on Nov 6, 2017.
It is somewhat more high level than usual since the audience came from a broad range of backgrounds.&lt;/p&gt;

&lt;p&gt;The content of the talk were ‘AlphaGo’, it’s successor ‘AlphaGo Zero’ and ‘Feudal Networks’ all published by DeepMind.
The talk can be downloaded &lt;a href=&quot;BML-AdvRL.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><summary type="html">Talk on AlphaGo and Hierarchical RL</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/AdvRL_Cover.png" /></entry></feed>
