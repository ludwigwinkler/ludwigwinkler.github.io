<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
  Jekyll integration by somiibo.com
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

<title>Doob's h-Transform</title>
<meta name="description" content="">

<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/assets/icon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:4000/assets/icon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:4000/assets/icon/favicon-16x16.png">
<link rel="manifest" href="http://localhost:4000/assets/icon/manifest.json">
<link rel="mask-icon" href="http://localhost:4000/assets/icon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="http://localhost:4000/assets/icon/favicon.ico">
<meta name="msapplication-config" content="http://localhost:4000/assets/icon/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<!-- CSS -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css" />
<noscript><link rel="stylesheet" href="http://localhost:4000/assets/css/noscript.css" /></noscript>

	</head>
	<body class="is-loading">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Header -->
        <header id="header">
          <a href="http://localhost:4000/" class="logo">Ludwig Winkler</a>
        </header>

				<!-- Nav -->
					<nav id="nav">

            <ul class="links">
  <li class=""><a href="http://localhost:4000/">Home</a></li>
  <li class=""><a href="http://localhost:4000/blog/">Machine Learning & Math</a></li>
  <li class=""><a href="http://localhost:4000/globalizedfinance/">Kleptocracy</a></li>
  <li class=""><a href="http://localhost:4000/readinglist/">Reading List</a></li>
  <li class=""><a href="http://localhost:4000/photography/">Photography</a></li>
  <li class=""><a href="http://localhost:4000/about/">About</a></li>
</ul>
<!-- <ul class="links">
  <li class=""><a href="http://localhost:4000/">Home</a></li>
  <li class=" active "><a href="http://localhost:4000/blog/">Machine Learning & Math</a></li>
  <li class=""><a href="http://localhost:4000/readinglist/">Reading List</a></li>
  <li class=""><a href="http://localhost:4000/photography/">Photography</a></li>
  <li class=""><a href="http://localhost:4000/about/">About</a></li>
</ul> -->


						<ul class="icons">
              <li><a href="https://twitter.com/default" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a></li>
              <li><a href="https://github.com/default" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
				<div id="main">
          <section class="post">
    				<header class="major">
      				<span class="date">18 Oct 2024</span>
      				<h1>Doob's h-Transform</h1>
      				<p>Wrong Direction, buddy</p>
      			</header>
      			<div class="image main"><img src="/images/Doob.png" alt=""></div>
      			<p><head>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             displayMath: [['$$','$$']],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<p>Previously, we saw how Girsanov’s theorem is little else than importance sampling for stochastic processes.</p>

<p>This time I stumbled over <em>Doob’s h-transform</em>.
On first reading is an equally strange equation which seemed hard to understand when simply googling for it.
But peeling away the layers like an onion and almost crying an equal amount along the way, there was some simply and intuitive things to be found.</p>

<h3 id="the-purpose">The Purpose</h3>

<p>To get it core idea out of the way: <strong>Doob’s h-transform is a way to condition a stochastic process on a future event.</strong>
The obvious question is how to achieve such a condition.</p>

<p>For that we first observe our favourite equation on this blog, a stochastic differential equation in the form of an Ito drift-diffusion process</p>
<div style="overflow-x: auto;">
$$
\begin{align}
dx_t = \mu(x_t, t) dt + \sigma(x_t, t) dW_t
\end{align}
$$
</div>

<p>Next, we want to posit the probability of a future state $x_T$ at the end of our stochastic process given our current state $x_t$, $p(x_T | x_t)$.
The probability simply states how probable our terminal future state $x_T$ is, given our current state $x_t$.
Just a plain conditional probability.</p>

<p>Instead of going all the way to $t=T$ we can equally go to $t+\Delta t$ and denote the same transition probability $p(x_{t+\Delta t} | x_t)$.
We can then construct the Chapman-Kolmogorov equation to integrate out over the $x_{t+\Delta t}$ to obtain</p>
<div style="overflow-x: auto;">
$$
\begin{align}
p(x_T | x_t) = \int p(x_T | x_{t+\Delta t}) p(x_{t+\Delta t} | x_t) dx_{t+\Delta t}
\end{align}
$$
</div>

<p>which states little else than considering every path from $x_t$ to $x_{t+\Delta t}$ and subsequently from $x_{t+\Delta t}$ to $x_T$.
If we consider every possible $x_{t+\Delta t}$ in between $x_t$ and $x_T$, we simply marginalize $x_{t+\Delta t}$ out.</p>

<p>Rewriting the Chapman-Kolmogorov equation above yields an interesting identity,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
p(x_T | x_t) 
&amp;= \int p(x_T | x_{t+\Delta t}) p(x_{t+\Delta t} | x_t) dx_{t+\Delta t} \\
&amp;= \mathbb{E}_{p(x_{t+\Delta t} | x_t)} \Big[ p(x_T | x_{t+\Delta t}) \Big]  \\
\end{align}
$$
</div>

<p>which is the definition of a martingale.</p>

<p>The martingale equation above tells us that the probability of hitting $x_T$ given $x_t$ is the same as doing a step with the transition kernel $p(x_{t+\Delta t} | x_t)$ and then computing the probability of hitting $x_T$ from state $x_{t+\Delta t}$.
Said more succinctly, our “predictive” power of $p(x_T | x_t)$ stays the same, regardless of doing a step to $x_{t+\Delta t}$.
Obviously this only holds in the expected sense and sure enough there can massive spikes when considering extreme paths.
But on average, we gain precisely no new information.</p>

<p>The notation of the probability $p(x_T , T | x_t, t)$ is just a more instructive way of signalling what $x_t$ will be used for but is nevertheless still a function of just $x_t$.
We can abstract a bit more and write the martingale in terms of a function $f(x_t)$,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
f(x_t) 
&amp;= \int f(x_{t+\Delta t}) p(x_{t+\Delta t} | x_t) dx_{t+\Delta t} \\
&amp;= \mathbb{E}_{p(x_{t+\Delta t} | x_t)} \Big[ f(x_{t+\Delta t}) \Big]  \\
\end{align}
$$
</div>

<p>Below is a figurative representation of a Martingale.
We use a simple function $f(x_t) = x_t + 1$ and if the drift of the SDE is zero, then we have a Martingale property.
Thus from the moment on where we’re only dealing with a pure diffusive process with no drift, we will not be able to make an educated guess on what the future value of $f(x_{t+\Delta t})$ will be.
Naturally, the variance can still increase, but on average, we can’t predict more than we know at $t$.</p>

<p><img src="/images/Martingale.png" alt="Description of the image" style="width: 100%; height: auto;" /></p>

<h3 id="we-will-continue-right-after-these-commercials-on-the-kolmogorov-backward-equation-insert-advertisement-jingle">We will continue right after these commercials on the Kolmogorov Backward Equation… (insert advertisement jingle)</h3>

<h4 id="commercial-1-the-kolmogorov-backward-equation">Commercial 1: The Kolmogorov Backward Equation</h4>

<p>So how exactly does $p(x_T | x_t)$ change if we change $x_t$?
What are the dynamics of $p(x_T | x_t)$?
The probability $p(x_T | x_t)$ is little else than a function that takes in $x_t$ and outputs a scalar value.
We further know that the change in $x_t$ is driven by the stochastic differential equation above.
So, it’s time to use Ito’s lemma once again!
But first we’ll make the following derivation more general by abbreviating $p(x_T, T | x_t, t) = f(x_t, t)$.</p>
<div style="overflow-x: auto;">
$$
\begin{align}
f(x_t, t) = \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t+\Delta t} , t + \Delta t )]
\end{align}
$$
</div>

<p>What happens if $\Delta t$ goes to zero?</p>

<p>With Ito’s lemma we know how to apply a Taylor expansion to a function the input of which is governed by a SDE.
Applying Ito’s lemma to the term in the expectation we obtain</p>
<div style="overflow-x: auto;">
$$
\begin{align}
f(x_t, t) 
&amp;= \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )}
[ f(x_{t+\Delta t} , t + \Delta t )] \\
&amp;= \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t} , t) + \partial_t f(x_{t} , t) dt \\
&amp; \qquad \qquad + \mu(x_t, t) \ \partial_{x_t} f(x_{t} , t) dt + \frac{1}{2} \sigma^2(x_t, t) \ \partial_{x_t}^2 \ f(x_t, t) \ dt \\
&amp; \qquad \qquad + \sigma(x_t, t) \partial_{x_t} f(x_t, t) dW_t
]
\end{align}
$$
</div>

<p>All the terms in the expectation are deterministic except the Brownian motion $dW_t$.
<!-- Fortunately, we know that the expectation $\mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ dW_t]=0$, as this is one of the core properties of Brownian motion. -->
Fortunately, we know that the expectation $\mathbb{E} [ dW_t]=0$, as this is one of the core properties of Brownian motion.
Thus the equation above reduces after applying the expectation to</p>
<div style="overflow-x: auto;">
$$
\begin{align}
f(x_t, t) 
&amp;= \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t+\Delta t} , t + \Delta t )] \\
&amp;= f(x_{t} , t) + \partial_t f(x_{t} , t) dt \\
&amp; \qquad \qquad + \mu(x_t, t) \ \partial_{x_t} f(x_{t} , t) dt + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} f(x_t, t) \ dt \\
\end{align}
$$
</div>

<p>Cancelling the $f(x_t, t)$ and dividing zero by $dt$ on both sides gives us the Kolmogorov backward equation,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
0
&amp;= \partial_t f(x_{t} , t) + \mu(x_t, t) \ \partial_{x_t} f(x_{t} , t) + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} f(x_t, t) \\
&amp;\updownarrow \\
-\partial_t f(x_{t} , t)
&amp;= \mu(x_t, t) \ \partial_{x_t} f(x_{t} , t) + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} f(x_t, t) \\
\end{align}
$$
</div>

<!-- One of our assumptions is that $x_T$ remains fixed during all our mathematical shenanigans and thus we can plug back in our distribution $p(x_T, T | x_t, t)=f(x_t, t)$ where all the partial derivatives apply to the $x_t$ and $t$ components, not the $x_T$ and $T$! -->
<p>One of our assumptions is that $x_T$ remains fixed during all our mathematical shenanigans and thus we can plug back in our distribution $p(x_T, T | x_t, t)=f(x_t, t)$ where all the partial derivatives apply to the $x_t$ and $t$ components, not the $x_T$ and $T$!</p>

<p>Thus we obtain</p>
<div style="overflow-x: auto;">
$$
\begin{align}
-\partial_t p(x_T, T | x_t, t)
&amp;= \mu(x_t, t) \ \partial_{x_t} p(x_T, T | x_t, t) + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} p(x_T, T | x_t, t) \\
\end{align}
$$
</div>

<p>where again $p(x_T,T|x_t,t)$ is purely a function of $x_t$ (as in $f(x_t, t)$) and is notationally written more elaborately as $p(x_T,T |x_t,t)$.</p>

<h4 id="commercial-2-kolmogorov-forward-solves-a-different-problem-than-the-kolmogorov-backward">Commercial 2: Kolmogorov Forward solves a different problem than the Kolmogorov Backward</h4>

<p>I personally asked myself why the derivation of the Kolmogorov Forward (aka Fokker-Planck) was different from the Kolmogorov Backward Equation.</p>

<p><strong>At it’s core, the backward equation focuses on the evolution of expectations of functions of the process</strong>, conditioning on the current state while evolving backward in time. It addresses how a future payoff (or terminal condition) depends on the present state when viewed from the future time.</p>

<p><strong>The forward equation focuses on the time evolution of the distribution, describing how probabilities evolve forward from an initial condition.</strong>
Naturally you can integrate the FPE PDE also backwards in time.
<strong>While their name might be direct opposites, the KBE and KFE solve wholly different problems.</strong>
The resulting forward and backward equations have very similar structure, only differing in terms of the signs and how the derivatives are applied.
Whereas the FPE/KFE models the interaction between the drift, diffusion and the resulting marginal probability distribution, the KBE only a evaluates a function which never interferes with the dynamics.</p>

<h4 id="commercial-3-the-infinitisimal-generator">Commercial 3: The Infinitisimal Generator</h4>

<p>Let us consider again the Martingale property</p>
<div style="overflow-x: auto;">
$$
\begin{align}
f(x_t, t) = \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t+\Delta t} , t + \Delta t )]
\end{align}
$$
</div>

<p>Pulling $f(x_t, t)$ to the right side we get</p>
<div style="overflow-x: auto;">
$$
\begin{align}
&amp; \mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t+\Delta t} , t + \Delta t )] - f(x_t, t) \\ 
=&amp; \Big\{ \partial_t f(x_{t} , t) + \mu(x_t, t) \ \partial_{x_t} f(x_{t} , t) + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} f(x_t, t)\Big\} \ dt \\
=&amp; \mathcal{A}_t f \cdot dt
\end{align}
$$
</div>

<p>Since mathematicians are lazy by nature (and it’s significantly more convenient down the road), we abbreviate the term in the curly braces as $\mathcal{A}_t \ f$.
In this notation, the SDE drift $\mu(x_t, t)$ and diffusion $\sigma(x_t, t)$ is absorbed into the operator $\mathcal{A}_t$.
The equation above states that the difference between the current value $f(x_t, t)$ and the expected future value is equal to the operator $\mathcal{A}_t$ applied to the function itself and scaled with the time difference.</p>

<p>All that is left to do is to pull the $dt$ via a division to the other side and take the limit $\lim {dt \rightarrow 0^+}$ .</p>

<p>We get</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\color{blue}{\mathcal{A}_t} \ f
&amp; =
\color{blue}{\partial_t} f(x_{t} , t) \color{blue}{+ \mu(x_t, t) \ \partial_{x_t}} f(x_{t} , t) \color{blue}{+ \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t}} f(x_t, t)  \\
&amp; = \lim_{\Delta t \rightarrow 0^+} \frac{\mathbb{E}_{p(x_{t+\Delta t} \ , \ t+\Delta t | x_{t} \ , \ t )} [ f(x_{t+\Delta t} , t + \Delta t )] - f(x_t, t)}{dt} \\ 
\end{align} \\
$$
</div>

<p>The three terms above say the following: The expected, infinitisimal change of a function, the input of which is driven by a SDE, can be written as partial differential equation.
The operator $\mathcal{A}_t$ encapsulates all relevant SDE terms and partial derivatives into a single operator.</p>

<h3 id="commercial-break-is-over">Commercial Break is Over</h3>

<p>With a bit of foresight, we will define a new transition kernel $\tilde{p}(x_{t+\Delta t} | x_t)$ for a slightly adjusted Markov process such that our target value of $p(x_T | x_t)$ is included in the transition kernel,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\tilde{p}(x_{t+\Delta t} | x_t) = p(x_{t+\Delta t} | x_t)  \underbrace{\frac{p(x_T | x_{t+\Delta t})}{p(x_T | x_{t})}}_{\text{scaling}}
\end{align}
$$
</div>

<p>Before, we had defined the transition kernel $p(x_{t+\Delta t} | x_t)$ which is, based on the SDE being Markovian, also Markovian.
The SDE above only takes the current state and nothing else to ‘predict’ the next state.</p>

<p>For $\tilde{p}(x_{t+\Delta t} | x_t)$ to be a proper Markovian transition kernel we have to show that $\int \tilde{p}(x_{t+\Delta t} | x_t) dx_{t+\Delta t} = 1$.
We can show that via the Martingale property by simply integrating both sides and obtaining</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\tilde{p}(x_{t+\Delta t} | x_t) &amp;= p(x_{t+\Delta t} | x_t)  \frac{p(x_T | x_{t+\Delta t})}{p(x_T | x_{t})} \\
&amp; \updownarrow \\
\int \tilde{p}(x_{t+\Delta t} | x_t) dx_{t+\Delta t} &amp;=
\int p(x_{t+\Delta t} | x_t)  \frac{p(x_T | x_{t+\Delta t})}{p(x_T | x_{t})} dx_{t+\Delta t} \\
&amp;= \frac{\mathbb{E}_{p(x_{t+\Delta t} | x_t)} \big[ p(x_T | x_{t+\Delta t})\big]}{p(x_T | x_{t})} \qquad \text{} \\
&amp;= \frac{p(x_T | x_{t})}{p(x_T | x_{t})} \\ 
&amp;= 1
\end{align}
$$
</div>

<p>With that proven, we can now define an infinitissimal generator under the transition kernel $\tilde{p}$.
This is a kind expected finite difference between a state $x_t$ and a next state $x_{t+\Delta t}$ where we take the expected value over the slightly modified transition kernel $\tilde{p}$.
We have</p>
<div style="overflow-x: auto;">
$$
\begin{align}
&amp;\lim_{dt \rightarrow 0^+} \frac{ \mathbb{E}_{\tilde{p}(x_{t+\Delta t} | x_t)} \left[ f(x_{t+\Delta t}, t + \Delta t ) \right] - f(x_{t}, t)}{dt} \\
=&amp;\lim_{dt \rightarrow 0^+} \frac{ \mathbb{E}_{p(x_{t+\Delta t} | x_t)} \left[ f(x_{t+\Delta t}, t + \Delta t ) \ \frac{p(x_T | x_{t + \Delta t})}{p(x_T | x_t)}\right]}{dt } - \frac{f(x_{t}, t)}{dt} \\
=&amp;\lim_{dt \rightarrow 0^+} \frac{ \mathbb{E}_{p(x_{t+\Delta t} | x_t)} \left[ f(x_{t+\Delta t}, t + \Delta t ) \ p(x_T | x_{t + \Delta t})\right]}{dt \ p(x_T | x_t)} - \frac{f(x_{t}, t)}{dt} \frac{p(x_T | x_t)}{p(x_T | x_t)} \\ 
=&amp;\lim_{dt \rightarrow 0^+} \frac{1}{p(x_T | x_t)} \frac{ \mathbb{E}_{p(x_{t+\Delta t} | x_t)} \left[ f(x_{t+\Delta t}, t + \Delta t ) \ p(x_T | x_{t + \Delta t})\right] - f(x_{t}, t)p(x_T | x_t)}{dt} \\
=&amp; \frac{1}{p(x_T | x_t)} \mathcal{A}_t ( f p )
\end{align}
$$
</div>

<p>The term above shows that the infinitisimal generator $\mathcal{A}_t$ applied under the transition kernel $\tilde{p}$ is in fact a slightly modified infinitisimal generator itself.</p>

<p>Intriguing …</p>

<p>So let’s unpack the operator $\mathcal{A}_t$ and see what it’s hiding beneath.
In a sense, all we have to do is to apply the differential operators included in $\mathcal{A}_t$ to the product of the two functions $f(x_t, t) p(x_T, T| x_t, t)$.
In order to alleviate the notational burden, we will denote $f(x_t, t)$ without it’s arguments and abbreviate $p(x_T, T | x_t, t)$ by $h$.</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\frac{1}{h} \mathcal{A}_t ( f h ) &amp;= \frac{1}{h} \Big\{ \partial_t [f h] + \mu(x_t, t) \ \partial_{x_t} [f h] + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} [f h] \Big\} \\
\partial_t [f h] &amp;= \partial_t [f] \ h + f \ \partial_t [h] \\
\partial_x [f h] &amp;= \partial_x [f] \ h + f \ \partial_x [h] \\
\end{align}
$$
</div>

<p>So far, we’ve only differentiates once the functions $f: \mathbb{R}^N \rightarrow \mathbb{R}$ and $p: \mathbb{R}^N \rightarrow \mathbb{R}$ with $x \in \mathbb{R}^N$.
Differentiating these functions once gives us vectors.
But differentiating a second time, as will do with $\partial^2_{x_t} [f h]$ will differentiate the vector with respect to the vector valued input.
Therefore, we have to pay special attention to the order of the differentiation in the sense of</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\partial^2_{x} [f h] &amp;= \partial_{x}\Big[ \partial_{x} [f h] \Big] \\
&amp;= \partial_{x}\Big[ \partial_x [f] \ h + f \ \partial_x [h] \Big] \\
&amp;= \partial_{x}\Big[ \partial_x [f] \ h\Big] + \partial_x \Big[f \ \partial_x [h] \Big] \\
&amp;= \partial_{x}^2 f \ h + \partial_x f \ \partial_x h + \partial_x f \ \partial_x h + f \ \partial_x^2 h  \\
&amp;= \partial_{x}^2 f \ h + 2 \partial_x f \ \partial_x h + f \ \partial_x^2 h  \\
\end{align}
$$
</div>

<p>Now, let’s put all these components back together again</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\frac{1}{h} \mathcal{A}_t ( f h ) &amp;= \frac{1}{h} \Big\{\partial_t [f h] + \mu(x_t, t) \ \partial_{x_t} [f h] + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} [f h] \Big\} \\
&amp;= \frac{1}{h} \Big\{ \partial_t [f] \ h + f \ \color{blue}{\partial_t [h]} + \color{blue}{\mu(x_t, t)} \Big(\partial_x f \ h + f \ \color{blue}{\partial_x h}\Big) \\
&amp; \qquad \qquad \ \ \ + \color{blue}{\frac{1}{2} \sigma^2(x_t, t)} \Big( \partial_{x}^2 f \ h + 2 \partial_x f \ \partial_x h + f \ \color{blue}{\partial_x^2 h} \Big) \Big\}
\end{align}
$$
</div>
<p>where we observe that the terms highlighted in blue evaluate precisely to $0$ according to the Kolmogorov backward equation,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\color{blue}{
0 = \partial_t h(x_{t} , t) + \mu(x_t, t) \ \partial_{x_t} h(x_{t} , t) + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} h(x_t, t)
}
\end{align}
$$
</div>

<p>This simplifies our equation to</p>
<div style="overflow-x: auto;">
$$
\begin{align}
\frac{1}{h} \mathcal{A}_t ( f h ) &amp;= \frac{1}{h} \Big\{\partial_t [f h] + \mu(x_t, t) \ \partial_{x_t} [f h] + \frac{1}{2} \sigma^2(x_t, t) \ \partial^2_{x_t} [f h] \Big\} \\
&amp;= \frac{1}{h} \Big\{ \partial_t [f] \ h + \mu(x_t, t) \partial_x f \ h  + \frac{1}{2} \sigma^2(x_t, t) \Big( \partial_{x}^2 f \ h + 2 \partial_x f \ \partial_x h \Big) \Big\} \\
&amp;= \partial_t [f] + \mu(x_t, t) \partial_x f  + \frac{1}{2} \sigma^2(x_t, t) \Big( \partial_{x}^2 f + 2 \partial_x f \ \underbrace{\frac{\partial_x h}{h}}_{\partial_x \log h} \Big) \\
&amp;= \partial_t [f] + \mu(x_t, t) \partial_x f  + \frac{1}{2} \sigma^2(x_t, t) \partial_{x}^2 f + \sigma^2(x_t, t) \ \partial_x f \ \partial_x \log h \\
&amp;= \partial_t [f] + \Big( \underbrace{\mu(x_t, t) + \sigma^2(x_t, t) \ \partial_x \log h}_{\text{modified SDE drift}} \Big) \partial_x f  + \frac{1}{2} \sigma^2(x_t, t) \partial_{x}^2 f
\end{align}
$$
</div>

<p>Remember how the Kolmogorov backward equation derived for a standard Ito SDE $dx_t = \mu(x_t, t) dt \sigma(x_t, t) dW_t$?
We can go in reverse and read off a corresponding SDE that fulfills the particular Kolmogorov backward equation above,</p>
<div style="overflow-x: auto;">
$$
\begin{align}
dx_t = \Big(\mu(x_t, t) + \sigma^2(x_t, t) \ \partial_x \log h(x_t, t) \Big) dt + \sigma(x_t, t) dW_t
\end{align}
$$
</div>

<p><img src="/images/Doob.png" alt="Description of the image" style="width: 100%; height: auto;" /></p>

<p>Interestingly, this looks extremely similar to reverse-time diffusion as it’s used in diffusion based generative modelling in machine learning.</p>
</p>
      		</section>

          <!-- <div class="comments-wrapper">
          <div id="disqus_thread"></div> -->
          <!-- <script>
              /**
               *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
               *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
               */

              var disqus_config = function () {
                  this.page.url = '/blog/Doob/';  /*Replace PAGE_URL with your page's canonical URL variable*/
                  this.page.identifier = '/blog/Doob/'; /*Replace PAGE_IDENTIFIER with your page's unique identifier variable*/
              };

              (function() {  /* dont endit below this line */
                  var d = document, s = d.createElement('script');

                  s.src = 'https://default.disqus.com/embed.js';

                  s.setAttribute('data-timestamp', +new Date());
                  (d.head || d.body).appendChild(s);
              })();
          </script> -->
          <!-- <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript> -->
        <!-- </div>/.comments-wrapper -->


					<!-- Footer -->
						<footer>
              <ul class="actions">
                <li><a href="http://localhost:4000/blog/" class="button">My Blog</a></li>
              </ul>
						</footer>
					</div>

				<!-- Footer -->
        <!-- <footer id="footer">
  <section>
    <form method="POST" action="https://formspree.io/">
      <div class="field">
        <label for="name">Name</label>
        <input type="text" name="name" id="name" />
      </div>
      <div class="field">
        <label for="email">Email</label>
        <input type="text" name="email" id="email" />
      </div>
      <div class="field">
        <label for="message">Message</label>
        <textarea name="message" id="message" rows="3"></textarea>
      </div>
      <ul class="actions">
        <li><input type="submit" value="Send Message" /></li>
      </ul>
    </form>
  </section>
  <section class="split contact">
    <section class="alt">
      <h3>Location</h3>
      <p>Berlin, Germany</p>
    </section>
    <section>
      <h3>Phone</h3>
      <p><a href="tel:"></a></p>
    </section>
    <section>
      <h3>Email</h3>
      <p><a href="mailto:"></a></p>
    </section>
    <section>
      <h3>Social</h3>
      <ul class="icons alt">
        <li><a href="https://twitter.com/ludiXIVwinkler" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a></li>
        <!-- <li><a href="https://facebook.com/default" class="icon fa-facebook" rel="nofollow"><span class="label">Facebook</span></a></li> -->
        <!-- <li><a href="https://instagram.com/default" class="icon fa-instagram" rel="nofollow"><span class="label">Instagram</span></a></li> -->
        <li><a href="https://github.com/ludwigwinkler" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a></li>
      </ul>
    </section>
  </section>
</footer>
<!-- Copyright -->
<div id="copyright">
  <ul>
       <li>&copy; HTML5 UP</li>
       <li>Design by <a href="https://html5up.net" rel="nofollow">HTML5 UP</a></li>
       <li>Jekyll Integration by <a href="https://soundgrail.com">SoundGrail</a></li>
       <li>Theme made by <a href="https://github.com/iwiedenm/jekyll-theme-massively-src"> iwiedenm</a></li>
 </ul>
</div>
 -->

			</div>

      <!-- Scripts -->
  		<!-- <!-- DYN -->
<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
<script src="http://localhost:4000/assets/js/skel.min.js"></script>
<script src="http://localhost:4000/assets/js/util.js"></script>
<script src="http://localhost:4000/assets/js/main.js"></script>
 -->
			<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=default"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'default');
</script>
 -->

	</body>
</html>
